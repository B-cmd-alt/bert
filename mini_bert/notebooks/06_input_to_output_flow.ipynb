{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Input-to-Output Flow in BERT\n",
    "\n",
    "This notebook traces a complete journey from raw text to final predictions in Mini-BERT.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Text â†’ Tokens â†’ IDs (Tokenization)\n",
    "2. IDs â†’ Embeddings (Input representation)\n",
    "3. Embeddings â†’ Hidden states (Transformer processing)\n",
    "4. Hidden states â†’ Logits (Output projection)\n",
    "5. Logits â†’ Predictions (Final processing)\n",
    "6. Shape transformations at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model import MiniBERT\n",
    "from tokenizer import WordPieceTokenizer\n",
    "from mlm import mask_tokens\n",
    "\n",
    "np.random.seed(42)\n",
    "# Set style for better visualizations - handle version compatibility\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-darkgrid') \n    except OSError:\n        plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From Raw Text to Token IDs\n",
    "\n",
    "The first step is converting human-readable text into numbers the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer\ntokenizer = WordPieceTokenizer()\ntokenizer.load_model('../tokenizer_8k.pkl')\n\n# Example text\nraw_text = \"The quick brown fox jumps over the lazy dog.\"\nprint(f\"Raw text: '{raw_text}'\")\nprint(f\"Length: {len(raw_text)} characters\")\n\n# Step 1: Tokenization\ntoken_ids = tokenizer.encode(raw_text)\ntokens = tokenizer.decode(token_ids).split()\n\nprint(f\"\\nAfter tokenization:\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Token IDs: {token_ids}\")\nprint(f\"Number of tokens: {len(token_ids)}\")\n\n# Visualize token mapping\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(tokens)), [len(token) for token in tokens], color='skyblue')\nplt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\nplt.ylabel('Token Length (chars)')\nplt.title('Token Lengths')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.bar(range(len(token_ids)), token_ids, color='lightcoral')\nplt.xticks(range(len(token_ids)), tokens, rotation=45, ha='right')\nplt.ylabel('Token ID')\nplt.title('Token IDs')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Special tokens\nprint(f\"\\nSpecial token IDs:\")\nspecial_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\nfor token in special_tokens:\n    if token in tokenizer.vocab:\n        print(f\"  {token}: {tokenizer.vocab[token]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Token IDs to Embeddings\n",
    "\n",
    "Token IDs become dense vector representations through embedding lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model\nmodel = MiniBERT()\n\n# Prepare input (add batch dimension)\ninput_ids = np.array([token_ids])  # Shape: [1, seq_len]\nbatch_size, seq_len = input_ids.shape\n\nprint(f\"Input shape: {input_ids.shape}\")\nprint(f\"Batch size: {batch_size}, Sequence length: {seq_len}\")\n\n# Step 2a: Token embeddings lookup\ntoken_embeddings = model.params['token_embeddings'][input_ids]  # [batch, seq_len, hidden_size]\nprint(f\"\\nToken embeddings shape: {token_embeddings.shape}\")\n\n# Step 2b: Position embeddings\nposition_embeddings = model.params['position_embeddings'][:seq_len]  # [seq_len, hidden_size]\nprint(f\"Position embeddings shape: {position_embeddings.shape}\")\n\n# Step 2c: Combine embeddings\ninput_embeddings = token_embeddings + position_embeddings  # Broadcasting\nprint(f\"Combined embeddings shape: {input_embeddings.shape}\")\n\n# Visualize embedding dimensions\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Token embeddings heatmap\nim1 = axes[0].imshow(token_embeddings[0], cmap='RdBu_r', aspect='auto')\naxes[0].set_title('Token Embeddings')\naxes[0].set_xlabel('Hidden Dimension')\naxes[0].set_ylabel('Token Position')\naxes[0].set_yticks(range(len(tokens)))\naxes[0].set_yticklabels(tokens)\nplt.colorbar(im1, ax=axes[0])\n\n# Position embeddings heatmap\nim2 = axes[1].imshow(position_embeddings, cmap='RdBu_r', aspect='auto')\naxes[1].set_title('Position Embeddings')\naxes[1].set_xlabel('Hidden Dimension')\naxes[1].set_ylabel('Position')\nplt.colorbar(im2, ax=axes[1])\n\n# Combined embeddings heatmap\nim3 = axes[2].imshow(input_embeddings[0], cmap='RdBu_r', aspect='auto')\naxes[2].set_title('Combined Embeddings')\naxes[2].set_xlabel('Hidden Dimension')\naxes[2].set_ylabel('Token Position')\naxes[2].set_yticks(range(len(tokens)))\naxes[2].set_yticklabels(tokens)\nplt.colorbar(im3, ax=axes[2])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nKey insight: Each token now has a {model.config.hidden_size}-dimensional vector\")\nprint(f\"that combines word meaning (token embedding) + position info\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Through the Transformer Layers\n",
    "\n",
    "Now we trace how information flows through each transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_through_layers(model, input_embeddings, detailed=True):\n",
    "    \"\"\"Trace input through each transformer layer.\"\"\"\n",
    "    \n",
    "    layer_outputs = {'input': input_embeddings[0].copy()}\n",
    "    current_hidden = input_embeddings\n",
    "    \n",
    "    print(\"Tracing through transformer layers...\")\n",
    "    print(f\"Input shape: {current_hidden.shape}\")\n",
    "    \n",
    "    # Through each transformer layer\n",
    "    for layer_idx in range(model.config.num_hidden_layers):\n",
    "        print(f\"\\n--- Layer {layer_idx} ---\")\n",
    "        \n",
    "        # Store input to this layer\n",
    "        layer_input = current_hidden.copy()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        print(f\"  Input to attention: {current_hidden.shape}\")\n",
    "        attn_output, attn_cache = model._multi_head_attention(current_hidden, layer_idx)\n",
    "        print(f\"  Attention output: {attn_output.shape}\")\n",
    "        \n",
    "        # Residual connection + Layer norm\n",
    "        current_hidden = current_hidden + attn_output\n",
    "        print(f\"  After residual: {current_hidden.shape}\")\n",
    "        \n",
    "        gamma = getattr(model, f'ln1_gamma_{layer_idx}')\n",
    "        beta = getattr(model, f'ln1_beta_{layer_idx}')\n",
    "        current_hidden = model._layer_norm(current_hidden, gamma, beta)\n",
    "        print(f\"  After layer norm 1: {current_hidden.shape}\")\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_input = current_hidden.copy()\n",
    "        ffn_output = model._feed_forward(current_hidden, layer_idx)\n",
    "        print(f\"  FFN output: {ffn_output.shape}\")\n",
    "        \n",
    "        # Residual connection + Layer norm\n",
    "        current_hidden = ffn_input + ffn_output\n",
    "        print(f\"  After residual: {current_hidden.shape}\")\n",
    "        \n",
    "        gamma = getattr(model, f'ln2_gamma_{layer_idx}')\n",
    "        beta = getattr(model, f'ln2_beta_{layer_idx}')\n",
    "        current_hidden = model._layer_norm(current_hidden, gamma, beta)\n",
    "        print(f\"  Final output: {current_hidden.shape}\")\n",
    "        \n",
    "        # Store layer output\n",
    "        layer_outputs[f'layer_{layer_idx}'] = current_hidden[0].copy()\n",
    "        \n",
    "        if detailed:\n",
    "            # Show how much the representation changed\n",
    "            input_norm = np.linalg.norm(layer_input[0])\n",
    "            output_norm = np.linalg.norm(current_hidden[0])\n",
    "            change_norm = np.linalg.norm(current_hidden[0] - layer_input[0])\n",
    "            print(f\"  Representation change: {change_norm:.4f} (input norm: {input_norm:.4f}, output norm: {output_norm:.4f})\")\n",
    "    \n",
    "    return layer_outputs, current_hidden\n",
    "\n",
    "# Trace through layers\n",
    "layer_outputs, final_hidden = trace_through_layers(model, input_embeddings)\n",
    "\n",
    "print(f\"\\nFinal hidden states shape: {final_hidden.shape}\")\n",
    "print(f\"Each token now has a context-aware representation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Layer-by-Layer Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how representations evolve through layers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot representations at different layers\n",
    "layers_to_show = ['input', 'layer_0', 'layer_1', 'layer_2']\n",
    "titles = ['Input Embeddings', 'After Layer 0', 'After Layer 1', 'After Layer 2']\n",
    "\n",
    "for idx, (layer_name, title) in enumerate(zip(layers_to_show, titles)):\n",
    "    if layer_name in layer_outputs:\n",
    "        representation = layer_outputs[layer_name]\n",
    "        \n",
    "        im = axes[idx].imshow(representation, cmap='RdBu_r', aspect='auto')\n",
    "        axes[idx].set_title(title)\n",
    "        axes[idx].set_xlabel('Hidden Dimension')\n",
    "        axes[idx].set_ylabel('Token Position')\n",
    "        axes[idx].set_yticks(range(len(tokens)))\n",
    "        axes[idx].set_yticklabels(tokens)\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze representation similarity between layers\n",
    "print(\"\\nLayer-to-layer similarity (cosine similarity):\")\n",
    "for i in range(len(layers_to_show) - 1):\n",
    "    layer1 = layers_to_show[i]\n",
    "    layer2 = layers_to_show[i + 1]\n",
    "    \n",
    "    if layer1 in layer_outputs and layer2 in layer_outputs:\n",
    "        repr1 = layer_outputs[layer1].flatten()\n",
    "        repr2 = layer_outputs[layer2].flatten()\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(repr1, repr2) / (np.linalg.norm(repr1) * np.linalg.norm(repr2))\n",
    "        print(f\"  {layer1} â†’ {layer2}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: From Hidden States to Logits\n",
    "\n",
    "The final hidden states are projected to vocabulary-sized logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply final layer norm\n",
    "final_layer_norm = model._layer_norm(final_hidden, model.final_ln_gamma, model.final_ln_beta)\n",
    "print(f\"After final layer norm: {final_layer_norm.shape}\")\n",
    "\n",
    "# Project to vocabulary size (MLM head)\n",
    "logits = final_layer_norm @ model.mlm_head_weight + model.mlm_head_bias\n",
    "print(f\"\\nFinal logits shape: {logits.shape}\")\n",
    "print(f\"Batch size: {logits.shape[0]}\")\n",
    "print(f\"Sequence length: {logits.shape[1]}\")\n",
    "print(f\"Vocabulary size: {logits.shape[2]}\")\n",
    "\n",
    "# Analyze logits\n",
    "print(f\"\\nLogits statistics:\")\n",
    "print(f\"  Mean: {logits.mean():.4f}\")\n",
    "print(f\"  Std: {logits.std():.4f}\")\n",
    "print(f\"  Min: {logits.min():.4f}\")\n",
    "print(f\"  Max: {logits.max():.4f}\")\n",
    "\n",
    "# Visualize logits for first few tokens\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Show logits for first 5 tokens\n",
    "num_tokens_to_show = min(5, seq_len)\n",
    "num_top_vocab = 20  # Show top 20 vocabulary items\n",
    "\n",
    "for i in range(num_tokens_to_show):\n",
    "    plt.subplot(1, num_tokens_to_show, i + 1)\n",
    "    \n",
    "    token_logits = logits[0, i]  # Logits for token i\n",
    "    \n",
    "    # Get top predictions\n",
    "    top_indices = np.argsort(token_logits)[-num_top_vocab:]\n",
    "    top_logits = token_logits[top_indices]\n",
    "    \n",
    "    # Plot\n",
    "    plt.barh(range(num_top_vocab), top_logits, color='skyblue')\n",
    "    plt.xlabel('Logit Value')\n",
    "    plt.title(f'Token: \"{tokens[i]}\"\\nTop {num_top_vocab} Predictions')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add vocabulary labels (if possible)\n",
    "    if hasattr(tokenizer, 'id_to_token'):\n",
    "        vocab_labels = [tokenizer.id_to_token.get(idx, f'ID_{idx}') for idx in top_indices]\n",
    "        plt.yticks(range(num_top_vocab), vocab_labels, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: From Logits to Predictions\n",
    "\n",
    "Convert logits to probabilities and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = softmax(logits)  # [batch, seq_len, vocab_size]\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Probabilities sum to 1: {probabilities[0, 0].sum():.6f}\")\n",
    "\n",
    "# Get top predictions for each position\n",
    "top_k = 5\n",
    "top_predictions = []\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    pos_probs = probabilities[0, pos]  # Probabilities for this position\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_indices = np.argsort(pos_probs)[-top_k:][::-1]  # Sort descending\n",
    "    top_probs = pos_probs[top_indices]\n",
    "    \n",
    "    # Try to get token strings\n",
    "    try:\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "    except:\n",
    "        top_tokens = [f'ID_{idx}' for idx in top_indices]\n",
    "    \n",
    "    top_predictions.append({\n",
    "        'position': pos,\n",
    "        'input_token': tokens[pos],\n",
    "        'predictions': list(zip(top_tokens, top_probs, top_indices))\n",
    "    })\n",
    "\n",
    "# Display predictions\n",
    "print(\"\\nTop predictions for each position:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pred_info in top_predictions[:5]:  # Show first 5 positions\n",
    "    pos = pred_info['position']\n",
    "    input_token = pred_info['input_token']\n",
    "    \n",
    "    print(f\"\\nPosition {pos}: Input token = '{input_token}'\")\n",
    "    print(\"Top predictions:\")\n",
    "    \n",
    "    for rank, (pred_token, prob, token_id) in enumerate(pred_info['predictions']):\n",
    "        percentage = prob * 100\n",
    "        is_correct = pred_token.strip() == input_token.strip()\n",
    "        marker = \" âœ“\" if is_correct else \"\"\n",
    "        print(f\"  {rank+1}. '{pred_token}' (ID: {token_id}) - {percentage:.2f}%{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Complete Pipeline with MLM\n",
    "\n",
    "Let's see the complete pipeline with masked language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masked version of our input\n",
    "print(\"Original text:\", raw_text)\n",
    "print(\"Original tokens:\", tokens)\n",
    "\n",
    "# Apply masking\n",
    "masked_ids, target_ids, mask_positions = mask_tokens(\n",
    "    np.array([token_ids]), \n",
    "    mask_token_id=tokenizer.vocab['[MASK]'],\n",
    "    vocab_size=model.config.vocab_size,\n",
    "    mask_prob=0.15\n",
    ")\n",
    "\n",
    "# Decode masked sentence\n",
    "masked_tokens = tokenizer.decode(masked_ids[0]).split()\n",
    "print(f\"\\nMasked tokens: {masked_tokens}\")\n",
    "print(f\"Mask positions: {mask_positions[0] if len(mask_positions) > 0 else 'None'}\")\n",
    "print(f\"Target IDs: {target_ids[0] if len(target_ids) > 0 else 'None'}\")\n",
    "\n",
    "if len(mask_positions) > 0:  # If we have masks\n",
    "    # Run complete forward pass with masked input\n",
    "    masked_logits, cache = model.forward(masked_ids)\n",
    "    masked_probabilities = softmax(masked_logits)\n",
    "    \n",
    "    print(f\"\\nMasked input forward pass:\")\n",
    "    print(f\"Input shape: {masked_ids.shape}\")\n",
    "    print(f\"Output logits shape: {masked_logits.shape}\")\n",
    "    \n",
    "    # Analyze predictions at masked positions\n",
    "    print(\"\\nPredictions for masked positions:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, pos in enumerate(mask_positions[0]):\n",
    "        if i < len(target_ids[0]):\n",
    "            true_token_id = target_ids[0][i]\n",
    "            true_token = tokenizer.decode([true_token_id])\n",
    "            \n",
    "            # Get predictions for this masked position\n",
    "            pos_probs = masked_probabilities[0, pos]\n",
    "            top_indices = np.argsort(pos_probs)[-5:][::-1]\n",
    "            top_probs = pos_probs[top_indices]\n",
    "            \n",
    "            print(f\"\\nMasked position {pos}: True token = '{true_token}'\")\n",
    "            \n",
    "            for rank, (pred_id, prob) in enumerate(zip(top_indices, top_probs)):\n",
    "                pred_token = tokenizer.decode([pred_id])\n",
    "                is_correct = pred_id == true_token_id\n",
    "                marker = \" âœ“\" if is_correct else \"\"\n",
    "                print(f\"  {rank+1}. '{pred_token}' - {prob*100:.2f}%{marker}\")\n",
    "            \n",
    "            # Show prediction accuracy\n",
    "            true_token_prob = pos_probs[true_token_id]\n",
    "            true_token_rank = np.sum(pos_probs > true_token_prob) + 1\n",
    "            print(f\"  True token probability: {true_token_prob*100:.2f}% (rank {true_token_rank})\")\n",
    "else:\n",
    "    print(\"\\nNo tokens were masked in this example.\")\n",
    "    print(\"Try running again or increase mask_prob.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Shape Transformation Summary\n",
    "\n",
    "Let's summarize all the shape transformations in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shape_flow_diagram():\n",
    "    \"\"\"Create a visual summary of shape transformations.\"\"\"\n",
    "    \n",
    "    # Define the pipeline stages\n",
    "    stages = [\n",
    "        (\"Raw Text\", f\"'{raw_text[:30]}...'\", f\"{len(raw_text)} chars\"),\n",
    "        (\"Tokens\", f\"{tokens[:3]}...\", f\"{len(tokens)} tokens\"),\n",
    "        (\"Token IDs\", f\"{token_ids[:3]}...\", f\"[{seq_len}]\"),\n",
    "        (\"Batch Input\", \"Add batch dim\", f\"[1, {seq_len}]\"),\n",
    "        (\"Token Embeddings\", \"Lookup\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"+ Position Emb\", \"Add positions\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"Layer 0\", \"Attention + FFN\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"Layer 1\", \"Attention + FFN\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"Layer 2\", \"Attention + FFN\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"Final LayerNorm\", \"Normalize\", f\"[1, {seq_len}, 192]\"),\n",
    "        (\"MLM Projection\", \"Linear layer\", f\"[1, {seq_len}, 8192]\"),\n",
    "        (\"Probabilities\", \"Softmax\", f\"[1, {seq_len}, 8192]\")\n",
    "    ]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    # Position stages\n",
    "    y_positions = range(len(stages))\n",
    "    x_center = 0.5\n",
    "    \n",
    "    # Draw boxes and connections\n",
    "    for i, (stage_name, operation, shape) in enumerate(stages):\n",
    "        y = len(stages) - 1 - i  # Reverse order (top to bottom)\n",
    "        \n",
    "        # Draw box\n",
    "        box_width = 0.8\n",
    "        box_height = 0.6\n",
    "        \n",
    "        # Color coding\n",
    "        if 'Layer' in stage_name:\n",
    "            color = 'lightblue'\n",
    "        elif 'Embedding' in stage_name or 'Position' in stage_name:\n",
    "            color = 'lightgreen'\n",
    "        elif stage_name in ['Raw Text', 'Tokens', 'Token IDs']:\n",
    "            color = 'lightyellow'\n",
    "        else:\n",
    "            color = 'lightcoral'\n",
    "        \n",
    "        # Draw rectangle\n",
    "        rect = plt.Rectangle((x_center - box_width/2, y - box_height/2), \n",
    "                           box_width, box_height, \n",
    "                           facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(x_center, y + 0.1, stage_name, ha='center', va='center', \n",
    "                fontweight='bold', fontsize=10)\n",
    "        ax.text(x_center, y - 0.1, operation, ha='center', va='center', \n",
    "                fontsize=8, style='italic')\n",
    "        ax.text(x_center, y - 0.25, shape, ha='center', va='center', \n",
    "                fontsize=9, fontfamily='monospace')\n",
    "        \n",
    "        # Draw arrow to next stage\n",
    "        if i < len(stages) - 1:\n",
    "            ax.arrow(x_center, y - box_height/2 - 0.05, 0, -0.3, \n",
    "                    head_width=0.03, head_length=0.05, fc='black', ec='black')\n",
    "    \n",
    "    # Set limits and remove axes\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-0.5, len(stages) - 0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.title('Complete BERT Input-to-Output Flow\\nShape Transformations', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the diagram\n",
    "create_shape_flow_diagram()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAPE TRANSFORMATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Stage':<20} {'Input Shape':<15} {'Output Shape':<15} {'Key Operation':<25}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "transformations = [\n",
    "    ('Tokenization', 'Text', f'[{seq_len}]', 'String â†’ Token IDs'),\n",
    "    ('Batching', f'[{seq_len}]', f'[1,{seq_len}]', 'Add batch dimension'),\n",
    "    ('Token Embedding', f'[1,{seq_len}]', f'[1,{seq_len},192]', 'ID â†’ Dense vector'),\n",
    "    ('Position Embedding', f'[1,{seq_len},192]', f'[1,{seq_len},192]', 'Add position info'),\n",
    "    ('Transformer x3', f'[1,{seq_len},192]', f'[1,{seq_len},192]', 'Self-attention + FFN'),\n",
    "    ('MLM Head', f'[1,{seq_len},192]', f'[1,{seq_len},8192]', 'Project to vocab'),\n",
    "    ('Softmax', f'[1,{seq_len},8192]', f'[1,{seq_len},8192]', 'Logits â†’ Probabilities')\n",
    "]\n",
    "\n",
    "for stage, input_shape, output_shape, operation in transformations:\n",
    "    print(f\"{stage:<20} {input_shape:<15} {output_shape:<15} {operation:<25}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"â€¢ Hidden dimension (192) stays constant through transformer layers\")\n",
    "print(\"â€¢ Only the final projection changes the last dimension\")\n",
    "print(\"â€¢ Batch and sequence dimensions are preserved throughout\")\n",
    "print(\"â€¢ Each position gets independent predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Journey\n",
    "\n",
    "### **The Complete Pipeline**\n",
    "1. **Text â†’ Tokens**: \"Hello world\" â†’ [\"Hello\", \"world\"]\n",
    "2. **Tokens â†’ IDs**: [\"Hello\", \"world\"] â†’ [1234, 5678]\n",
    "3. **IDs â†’ Embeddings**: [1234, 5678] â†’ Dense vectors\n",
    "4. **Through Transformers**: Context-aware representations\n",
    "5. **To Vocabulary**: Project back to vocabulary space\n",
    "6. **To Predictions**: Softmax â†’ Probabilities\n",
    "\n",
    "### **Key Shape Transformations**\n",
    "- **[text] â†’ [seq_len] â†’ [1, seq_len] â†’ [1, seq_len, 192] â†’ [1, seq_len, 8192]**\n",
    "\n",
    "### **What Each Component Does**\n",
    "- **Embeddings**: Convert discrete â†’ continuous\n",
    "- **Transformers**: Add context awareness\n",
    "- **Output projection**: Map back to vocabulary\n",
    "- **Softmax**: Convert to probabilities\n",
    "\n",
    "### **Why This Design Works**\n",
    "- **Constant hidden size**: Allows deep stacking\n",
    "- **Residual connections**: Enable gradient flow\n",
    "- **Attention**: Captures long-range dependencies\n",
    "- **Position embeddings**: Preserve order information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Different Texts**: Try the pipeline with different input texts. How do the representations change?\n",
    "\n",
    "2. **Layer Analysis**: Compare representations at different layers. Which layers capture which types of information?\n",
    "\n",
    "3. **Attention Patterns**: Extract and visualize attention weights. What patterns do you see?\n",
    "\n",
    "4. **Vocabulary Analysis**: Which tokens does the model predict most confidently? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n",
    "# Try different texts here:\n",
    "# different_text = \"Your text here\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}