{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately\n",
    "\n",
    "**Rank**: #2 - Revolutionary Impact\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "BERT's Masked Language Modeling (MLM) has a fundamental inefficiency: only 15% of tokens are masked, so the model only learns from a small fraction of the input at each step. This means BERT needs enormous amounts of compute to reach good performance.\n",
    "\n",
    "**The Problem with MLM:**\n",
    "- Only 15% of tokens contribute to the loss\n",
    "- 85% of computation is \"wasted\" on unmasked tokens\n",
    "- Need massive datasets and compute for good results\n",
    "- Small models significantly underperform large ones\n",
    "\n",
    "**ELECTRA's Innovation:**\n",
    "- Learn from **ALL** tokens, not just 15%\n",
    "- Replace MLM with \"Replaced Token Detection\" (RTD)\n",
    "- Use a generator-discriminator setup (like GANs)\n",
    "- 4x more sample efficient than BERT\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Generator-Discriminator Architecture**: How ELECTRA uses two models\n",
    "2. **Replaced Token Detection**: The core task that replaces MLM\n",
    "3. **Sample Efficiency**: Why ELECTRA learns faster\n",
    "4. **Implementation**: Building ELECTRA from scratch\n",
    "5. **Results**: Why small ELECTRA models rival large BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set style for better visualizations\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid') \n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "        \n",
    "print(\"ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately\")\n",
    "print(\"Paper: Clark et al., 2020 - Google Research & Stanford\")\n",
    "print(\"Impact: 4x more efficient than BERT, small models match large BERT performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Core Problem with MLM\n",
    "\n",
    "Let's visualize why BERT's MLM is inefficient and how ELECTRA solves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlm_inefficiency():\n",
    "    \"\"\"\n",
    "    Show the fundamental inefficiency of Masked Language Modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example sentence\n",
    "    sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
    "    n_tokens = len(sentence)\n",
    "    \n",
    "    # MLM: Only 15% of tokens are masked\n",
    "    mask_prob = 0.15\n",
    "    num_masked = int(n_tokens * mask_prob)\n",
    "    \n",
    "    # Randomly select tokens to mask\n",
    "    masked_positions = np.random.choice(n_tokens, num_masked, replace=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Original sentence\n",
    "    colors_original = ['lightblue'] * n_tokens\n",
    "    bars1 = axes[0].bar(range(n_tokens), [1] * n_tokens, color=colors_original)\n",
    "    axes[0].set_title('Original Sentence', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylim(0, 1.5)\n",
    "    axes[0].set_xticks(range(n_tokens))\n",
    "    axes[0].set_xticklabels(sentence, rotation=45)\n",
    "    axes[0].set_ylabel('Token Status')\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, word in enumerate(sentence):\n",
    "        axes[0].text(i, 0.5, word, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. BERT MLM - only some tokens contribute to learning\n",
    "    colors_mlm = ['red' if i in masked_positions else 'lightgray' for i in range(n_tokens)]\n",
    "    bars2 = axes[1].bar(range(n_tokens), [1] * n_tokens, color=colors_mlm)\n",
    "    axes[1].set_title(f'BERT MLM: Only {num_masked}/{n_tokens} tokens ({mask_prob*100:.0f}%) contribute to learning', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylim(0, 1.5)\n",
    "    axes[1].set_xticks(range(n_tokens))\n",
    "    axes[1].set_xticklabels(['[MASK]' if i in masked_positions else word \n",
    "                           for i, word in enumerate(sentence)], rotation=45)\n",
    "    axes[1].set_ylabel('Learning Signal')\n",
    "    \n",
    "    # Add legend\n",
    "    axes[1].bar([], [], color='red', label='Contributes to Loss')\n",
    "    axes[1].bar([], [], color='lightgray', label='No Learning Signal')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 3. ELECTRA RTD - all tokens contribute to learning\n",
    "    colors_electra = ['green'] * n_tokens  # All tokens contribute\n",
    "    bars3 = axes[2].bar(range(n_tokens), [1] * n_tokens, color=colors_electra)\n",
    "    axes[2].set_title(f'ELECTRA RTD: All {n_tokens}/{n_tokens} tokens (100%) contribute to learning!', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylim(0, 1.5)\n",
    "    axes[2].set_xticks(range(n_tokens))\n",
    "    \n",
    "    # Generate some replaced tokens for visualization\n",
    "    replacements = sentence.copy()\n",
    "    replaced_positions = np.random.choice(n_tokens, num_masked, replace=False)\n",
    "    replacement_words = ['cat', 'slow', 'red'][:len(replaced_positions)]\n",
    "    \n",
    "    for i, pos in enumerate(replaced_positions):\n",
    "        if i < len(replacement_words):\n",
    "            replacements[pos] = replacement_words[i]\n",
    "    \n",
    "    axes[2].set_xticklabels(replacements, rotation=45)\n",
    "    axes[2].set_ylabel('Learning Signal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate efficiency\n",
    "    mlm_efficiency = mask_prob\n",
    "    electra_efficiency = 1.0\n",
    "    \n",
    "    print(f\"\\nSAMPLE EFFICIENCY COMPARISON:\")\n",
    "    print(f\"BERT MLM: {mlm_efficiency:.1%} of tokens provide learning signal\")\n",
    "    print(f\"ELECTRA RTD: {electra_efficiency:.1%} of tokens provide learning signal\")\n",
    "    print(f\"\\nELECTRA is {electra_efficiency/mlm_efficiency:.1f}x more sample efficient!\")\n",
    "    \n",
    "    return masked_positions, replaced_positions\n",
    "\n",
    "masked_pos, replaced_pos = visualize_mlm_inefficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ELECTRA's Generator-Discriminator Architecture\n",
    "\n",
    "ELECTRA uses two models working together, similar to GANs but adapted for language."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Understanding Generator vs Discriminator in ELECTRA\n\nELECTRA uses **two separate neural networks** that work together, similar to Generative Adversarial Networks (GANs) but adapted for language understanding.\n\n### ðŸŽ­ **The Generator (The \\\"Faker\\\")**\n\n**What it does:** The generator is like BERT - it tries to predict masked tokens.\n\n**Role:** Creates **plausible but wrong** token replacements to challenge the discriminator.\n\n**Architecture:**\n- **Smaller model** (like BERT-Small: 14M parameters)\n- **Task**: Masked Language Modeling (MLM) - same as BERT\n- **Input**: Sentence with some tokens masked as `[MASK]`\n- **Output**: Predictions for what the masked tokens should be\n\n**Example:**\n```\nOriginal:  \\\"The quick brown fox jumps\\\"\nMasked:    \\\"The [MASK] brown fox jumps\\\"\nGenerator: \\\"The slow brown fox jumps\\\"    # Predicts \\\"slow\\\" for [MASK]\n```\n\n### ðŸ•µï¸ **The Discriminator (The \\\"Detective\\\")**\n\n**What it does:** The discriminator learns to detect which tokens are \\\"real\\\" vs \\\"fake\\\".\n\n**Role:** Learns to spot the generator's replacements, building better language understanding.\n\n**Architecture:**\n- **Larger model** (like BERT-Base: 110M parameters)\n- **Task**: Replaced Token Detection (RTD) - binary classification for each token\n- **Input**: Sentence with some tokens replaced by generator\n- **Output**: For each token, probability it's been replaced (0 = original, 1 = replaced)\n\n**Example:**\n```\nOriginal:      \\\"The quick brown fox jumps\\\"\nAfter Generator: \\\"The slow brown fox jumps\\\"\nDiscriminator:  [0.02, 0.95, 0.01, 0.03, 0.01]  # High probability that \\\"slow\\\" is replaced\n```\n\n### ðŸ”„ **How They Work Together**\n\n**Step 1: Generator Creates Fakes**\n- Take original sentence: `\\\"The quick brown fox\\\"`\n- Mask some tokens: `\\\"The [MASK] brown fox\\\"`\n- Generator predicts: `\\\"The slow brown fox\\\"`\n\n**Step 2: Discriminator Detects Fakes**\n- Gets sentence with replacements: `\\\"The slow brown fox\\\"`\n- For each position, predicts: `[ORIGINAL, REPLACED, ORIGINAL, ORIGINAL]`\n- Learns to distinguish real vs fake tokens\n\n**Step 3: Both Models Improve**\n- **Generator** gets better at creating plausible replacements\n- **Discriminator** gets better at detecting subtle differences\n- This creates an **adversarial training** dynamic\n\n### ðŸŽ¯ **Key Differences from BERT**\n\n| Aspect | BERT (MLM) | ELECTRA Generator | ELECTRA Discriminator |\n|--------|------------|-------------------|----------------------|\n| **Task** | Predict masked tokens | Predict masked tokens | Detect replaced tokens |\n| **Learning Signal** | 15% of tokens | 15% of tokens | **100% of tokens** |\n| **Architecture** | Single model | Smaller model | Larger model |\n| **Final Use** | Use for downstream | Discarded after training | **Used for downstream** |\n\n### ðŸ’¡ **Why This Design is Brilliant**\n\n1. **More Learning Signal**: Discriminator learns from ALL tokens, not just 15%\n2. **Adversarial Training**: Generator creates harder examples over time\n3. **Efficiency**: Small generator can train large discriminator effectively\n4. **Better Representations**: Discriminator learns fine-grained understanding\n\n**The key insight:** Instead of wasting 85% of tokens, ELECTRA learns from every single token position!\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleELECTRA:\\n    \\\"\\\"\\\"\\n    Simplified ELECTRA implementation to demonstrate core concepts\\n    \\n    Two main components:\\n    1. Generator: Small model that predicts masked tokens (like BERT)\\n    2. Discriminator: Larger model that detects which tokens were replaced\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self, vocab_size=8192, hidden_size=192, generator_size=64):\\n        self.vocab_size = vocab_size\\n        self.hidden_size = hidden_size\\n        self.generator_size = generator_size\\n        \\n        print(\\\"\\\\n=== INITIALIZING ELECTRA MODELS ===\\\")\\n        \\n        # ===== GENERATOR (\\\"The Faker\\\") =====\\n        # Smaller model that predicts masked tokens (like BERT MLM)\\n        print(\\\"\\\\nðŸŽ­ Generator (Faker):\\\")\\n        print(f\\\"   - Size: {generator_size} hidden dimensions (smaller model)\\\")\\n        print(f\\\"   - Task: Predict masked tokens (MLM)\\\")\\n        print(f\\\"   - Role: Create plausible but wrong replacements\\\")\\n        \\n        self.generator_embeddings = np.random.randn(vocab_size, generator_size) * 0.02\\n        self.generator_mlm_head = np.random.randn(generator_size, vocab_size) * 0.02\\n        gen_params = self.generator_embeddings.size + self.generator_mlm_head.size\\n        \\n        # ===== DISCRIMINATOR (\\\"The Detective\\\") =====\\n        # Larger model that detects which tokens are replaced\\n        print(\\\"\\\\nðŸ•µï¸ Discriminator (Detective):\\\")\\n        print(f\\\"   - Size: {hidden_size} hidden dimensions (larger model)\\\")\\n        print(f\\\"   - Task: Detect replaced tokens (RTD)\\\")\\n        print(f\\\"   - Role: Learn to spot generator's fakes\\\")\\n        \\n        self.discriminator_embeddings = np.random.randn(vocab_size, hidden_size) * 0.02\\n        self.discriminator_rtd_head = np.random.randn(hidden_size, 1) * 0.02\\n        disc_params = self.discriminator_embeddings.size + self.discriminator_rtd_head.size\\n        \\n        print(f\\\"\\\\nðŸ“Š Parameter Comparison:\\\")\\n        print(f\\\"   Generator parameters: {gen_params:,}\\\")\\n        print(f\\\"   Discriminator parameters: {disc_params:,}\\\")\\n        print(f\\\"   Ratio: {disc_params/gen_params:.1f}x larger discriminator\\\")\\n    \\n    def softmax(self, x):\\n        \\\"\\\"\\\"Compute softmax for probability distributions\\\"\\\"\\\"\\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\\n        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\\n    \\n    def sigmoid(self, x):\\n        \\\"\\\"\\\"Compute sigmoid for binary classification\\\"\\\"\\\"\\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\\n    \\n    def generator_step(self, input_ids, masked_positions):\\n        \\\"\\\"\\\"\\n        ðŸŽ­ GENERATOR STEP: Predict masked tokens (like BERT MLM)\\n        \\n        Input: [\\\"The\\\", \\\"[MASK]\\\", \\\"brown\\\", \\\"fox\\\"]\\n        Output: [\\\"The\\\", \\\"slow\\\", \\\"brown\\\", \\\"fox\\\"]  # Predicts \\\"slow\\\" for [MASK]\\n        \\n        This is essentially the same task as BERT's MLM!\\n        \\\"\\\"\\\"\\n        print(\\\"\\\\nðŸŽ­ GENERATOR STEP (Faker creates replacements):\\\")\\n        \\n        # Simple embedding lookup + linear layer\\n        embeddings = self.generator_embeddings[input_ids]  # [seq_len, generator_size]\\n        print(f\\\"   - Input embeddings shape: {embeddings.shape}\\\")\\n        \\n        # MLM predictions - NO .T transpose (this was the bug!)\\n        logits = embeddings @ self.generator_mlm_head  # [seq_len, vocab_size]\\n        probs = self.softmax(logits)\\n        print(f\\\"   - Output probabilities shape: {probs.shape}\\\")\\n        print(f\\\"   - Generating predictions for {len(masked_positions)} masked positions\\\")\\n        \\n        # Sample predictions for masked positions\\n        generated_tokens = input_ids.copy()\\n        \\n        for pos in masked_positions:\\n            # Sample from the probability distribution\\n            generated_token = np.random.choice(self.vocab_size, p=probs[pos])\\n            generated_tokens[pos] = generated_token\\n            print(f\\\"   - Position {pos}: Generated token ID {generated_token}\\\")\\n        \\n        return generated_tokens, probs, logits\\n    \\n    def discriminator_step(self, corrupted_tokens, original_tokens):\\n        \\\"\\\"\\\"\\n        ðŸ•µï¸ DISCRIMINATOR STEP: Detect which tokens are replaced\\n        \\n        Input: [\\\"The\\\", \\\"slow\\\", \\\"brown\\\", \\\"fox\\\"]  # Some tokens replaced by generator\\n        Output: [0.02, 0.95, 0.01, 0.03]    # Probability each token is replaced\\n        \\n        This is a binary classification task for EVERY token position!\\n        \\\"\\\"\\\"\\n        print(\\\"\\\\nðŸ•µï¸ DISCRIMINATOR STEP (Detective finds fakes):\\\")\\n        \\n        # Embedding lookup\\n        embeddings = self.discriminator_embeddings[corrupted_tokens]  # [seq_len, hidden_size]\\n        print(f\\\"   - Input embeddings shape: {embeddings.shape}\\\")\\n        \\n        # Binary classification for each position\\n        logits = embeddings @ self.discriminator_rtd_head  # [seq_len, 1]\\n        logits = logits.squeeze(-1)  # [seq_len]\\n        print(f\\\"   - Classification logits shape: {logits.shape}\\\")\\n        \\n        # Sigmoid to get probabilities (0 = original, 1 = replaced)\\n        probs = self.sigmoid(logits)\\n        \\n        # True labels: 1 if token was replaced, 0 if original\\n        labels = (corrupted_tokens != original_tokens).astype(float)\\n        num_replaced = int(labels.sum())\\n        print(f\\\"   - Detecting {num_replaced} replaced tokens out of {len(labels)} total\\\")\\n        \\n        return probs, labels, logits\\n    \\n    def train_step(self, input_ids, mask_prob=0.15):\\n        \\\"\\\"\\\"\\n        ðŸ”„ COMPLETE ELECTRA TRAINING STEP\\n        \\n        This shows how Generator and Discriminator work together:\\n        1. Mask some tokens in original sentence\\n        2. Generator predicts what masked tokens should be\\n        3. Replace masked tokens with generator's predictions\\n        4. Discriminator tries to detect which tokens were replaced\\n        5. Both models learn from their respective losses\\n        \\\"\\\"\\\"\\n        print(\\\"\\\\nðŸ”„ ELECTRA TRAINING STEP:\\\")\\n        original_tokens = input_ids.copy()\\n        \\n        # Step 1: Create masked input for generator\\n        masked_input = input_ids.copy()\\n        mask_token_id = self.vocab_size - 1  # [MASK] token\\n        \\n        # Randomly select positions to mask\\n        num_mask = max(1, int(len(input_ids) * mask_prob))\\n        masked_positions = np.random.choice(len(input_ids), num_mask, replace=False)\\n        print(f\\\"   Step 1: Masking {num_mask} tokens at positions {masked_positions}\\\")\\n        \\n        # Mask tokens\\n        for pos in masked_positions:\\n            masked_input[pos] = mask_token_id\\n        \\n        # Step 2: Generator predicts masked tokens\\n        print(f\\\"   Step 2: Generator predicts masked tokens...\\\")\\n        generated_tokens, gen_probs, gen_logits = self.generator_step(masked_input, masked_positions)\\n        \\n        # Step 3: Create corrupted sequence for discriminator\\n        corrupted_tokens = original_tokens.copy()\\n        for pos in masked_positions:\\n            corrupted_tokens[pos] = generated_tokens[pos]\\n        print(f\\\"   Step 3: Created corrupted sequence with generator predictions\\\")\\n        \\n        # Step 4: Discriminator detects replaced tokens\\n        print(f\\\"   Step 4: Discriminator detects replaced tokens...\\\")\\n        disc_probs, disc_labels, disc_logits = self.discriminator_step(corrupted_tokens, original_tokens)\\n        \\n        # Calculate losses\\n        # Generator loss: MLM cross-entropy on masked positions\\n        gen_loss = 0\\n        for pos in masked_positions:\\n            target = original_tokens[pos]\\n            gen_loss += -np.log(gen_probs[pos, target] + 1e-10)\\n        gen_loss /= len(masked_positions)\\n        \\n        # Discriminator loss: Binary cross-entropy on all positions\\n        disc_loss = 0\\n        for i in range(len(disc_labels)):\\n            p = disc_probs[i]\\n            label = disc_labels[i]\\n            disc_loss += -(label * np.log(p + 1e-10) + (1-label) * np.log(1-p + 1e-10))\\n        disc_loss /= len(disc_labels)\\n        \\n        print(f\\\"\\\\nðŸ“Š Training Results:\\\")\\n        print(f\\\"   Generator loss (MLM): {gen_loss:.4f}\\\")\\n        print(f\\\"   Discriminator loss (RTD): {disc_loss:.4f}\\\")\\n        \\n        return {\\n            'generator_loss': gen_loss,\\n            'discriminator_loss': disc_loss,\\n            'original_tokens': original_tokens,\\n            'corrupted_tokens': corrupted_tokens,\\n            'discriminator_predictions': disc_probs,\\n            'discriminator_labels': disc_labels,\\n            'masked_positions': masked_positions\\n        }\\n\\n# Demonstrate ELECTRA with detailed explanations\\nprint(\\\"=\\\" * 70)\\nprint(\\\"ELECTRA DEMONSTRATION: Generator vs Discriminator\\\")\\nprint(\\\"=\\\" * 70)\\n\\nelectra = SimpleELECTRA()\\n\\n# Example input\\nvocab = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', '[MASK]', '[PAD]']\\nsentence_ids = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # \\\"the quick brown fox jumps over lazy dog\\\"\\n\\nprint(f\\\"\\\\nðŸ“ Input sentence:\\\")\\nprint(f\\\"   Token IDs: {sentence_ids}\\\")\\nprint(f\\\"   Words: {[vocab[i] for i in sentence_ids]}\\\")\\n\\n# Run training step\\nresult = electra.train_step(sentence_ids)\\n\\n# Show detailed results\\nprint(f\\\"\\\\nðŸ” DETAILED RESULTS:\\\")\\nprint(f\\\"\\\\nðŸ“ Masked positions: {result['masked_positions']}\\\")\\nprint(f\\\"\\\\nðŸ“– Token-by-token breakdown:\\\")\\nfor i in range(len(sentence_ids)):\\n    orig_word = vocab[result['original_tokens'][i] % len(vocab)]\\n    corr_word = vocab[result['corrupted_tokens'][i] % len(vocab)]\\n    pred = result['discriminator_predictions'][i]\\n    label = result['discriminator_labels'][i]\\n    status = \\\"REPLACED\\\" if label == 1 else \\\"ORIGINAL\\\"\\n    \\n    marker = \\\"ðŸŽ¯\\\" if label == 1 else \\\"âœ…\\\"\\n    print(f\\\"   {marker} Position {i}: '{orig_word}' -> '{corr_word}' | Prob: {pred:.3f} | {status}\\\")\\n\\nprint(f\\\"\\\\nðŸŽ¯ Key Insight: Discriminator learns from ALL {len(sentence_ids)} tokens, not just {len(result['masked_positions'])} masked ones!\\\")\\nprint(f\\\"   This is why ELECTRA is {len(sentence_ids)/len(result['masked_positions']):.1f}x more sample efficient than BERT!\\\")\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Why ELECTRA is More Sample Efficient\n",
    "\n",
    "Let's analyze mathematically why ELECTRA learns faster than BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_efficiency():\n",
    "    \"\"\"\n",
    "    Analyze why ELECTRA is more sample efficient than BERT\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    sequence_length = 128\n",
    "    mask_probability = 0.15\n",
    "    num_examples = 1000\n",
    "    \n",
    "    # Calculate learning signals per example\n",
    "    bert_signals_per_example = sequence_length * mask_probability\n",
    "    electra_signals_per_example = sequence_length  # All tokens\n",
    "    \n",
    "    # Total learning signals\n",
    "    bert_total_signals = bert_signals_per_example * num_examples\n",
    "    electra_total_signals = electra_signals_per_example * num_examples\n",
    "    \n",
    "    print(\"SAMPLE EFFICIENCY ANALYSIS:\")\n",
    "    print(f\"\\nSequence length: {sequence_length} tokens\")\n",
    "    print(f\"Number of examples: {num_examples:,}\")\n",
    "    print(f\"\\nBERT MLM:\")\n",
    "    print(f\"  Mask probability: {mask_probability:.1%}\")\n",
    "    print(f\"  Learning signals per example: {bert_signals_per_example:.1f}\")\n",
    "    print(f\"  Total learning signals: {bert_total_signals:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nELECTRA RTD:\")\n",
    "    print(f\"  All tokens contribute: 100%\")\n",
    "    print(f\"  Learning signals per example: {electra_signals_per_example:.1f}\")\n",
    "    print(f\"  Total learning signals: {electra_total_signals:,.0f}\")\n",
    "    \n",
    "    efficiency_ratio = electra_total_signals / bert_total_signals\n",
    "    print(f\"\\nEfficiency ratio: {efficiency_ratio:.1f}x\")\n",
    "    \n",
    "    # Simulate learning curves\n",
    "    steps = np.linspace(0, num_examples, 100)\n",
    "    \n",
    "    # BERT learning curve (slower due to fewer signals)\n",
    "    bert_performance = 1 - np.exp(-steps * bert_signals_per_example / 10000)\n",
    "    \n",
    "    # ELECTRA learning curve (faster due to more signals)\n",
    "    electra_performance = 1 - np.exp(-steps * electra_signals_per_example / 10000)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sample efficiency comparison\n",
    "    methods = ['BERT MLM', 'ELECTRA RTD']\n",
    "    signals = [bert_signals_per_example, electra_signals_per_example]\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    \n",
    "    bars = axes[0].bar(methods, signals, color=colors, alpha=0.8)\n",
    "    axes[0].set_ylabel('Learning Signals per Example')\n",
    "    axes[0].set_title('Sample Efficiency Comparison')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, signals):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Learning curves\n",
    "    axes[1].plot(steps, bert_performance, 'r-', linewidth=3, label='BERT MLM', alpha=0.8)\n",
    "    axes[1].plot(steps, electra_performance, 'b-', linewidth=3, label='ELECTRA RTD', alpha=0.8)\n",
    "    axes[1].set_xlabel('Training Examples')\n",
    "    axes[1].set_ylabel('Performance')\n",
    "    axes[1].set_title('Simulated Learning Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    axes[1].annotate('ELECTRA reaches high\\nperformance faster', \n",
    "                    xy=(300, 0.8), xytext=(500, 0.6),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "                    fontsize=12, color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return efficiency_ratio\n",
    "\n",
    "efficiency_gain = analyze_sample_efficiency()\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WHY THIS MATTERS:\")\n",
    "print(f\"\\n1. Training Speed: ELECTRA needs {1/efficiency_gain:.1f}x less data\")\n",
    "print(f\"2. Compute Cost: {efficiency_gain:.1f}x reduction in training time\")\n",
    "print(f\"3. Model Size: Small ELECTRA can match large BERT\")\n",
    "print(f\"4. Accessibility: Enables BERT-quality on modest hardware\")\n",
    "print(f\"\\n5. Mathematical Intuition:\")\n",
    "print(f\"   - BERT: Loss only on {15}% of tokens\")\n",
    "print(f\"   - ELECTRA: Loss on {100}% of tokens\")\n",
    "print(f\"   - Result: {100/15:.1f}x more learning signal per example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Replaced Token Detection Task\n",
    "\n",
    "Let's dive deeper into how RTD works and why it's effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rtd_task():\n",
    "    \"\"\"\n",
    "    Demonstrate the Replaced Token Detection task in detail\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example sentences with different types of replacements\n",
    "    examples = [\n",
    "        {\n",
    "            'original': 'The quick brown fox jumps over the lazy dog'.split(),\n",
    "            'corrupted': 'The fast brown fox jumps over the lazy dog'.split(),\n",
    "            'explanation': 'Semantic replacement: quick -> fast'\n",
    "        },\n",
    "        {\n",
    "            'original': 'Machine learning algorithms require large datasets'.split(),\n",
    "            'corrupted': 'Machine learning algorithms require purple datasets'.split(),\n",
    "            'explanation': 'Nonsensical replacement: large -> purple'\n",
    "        },\n",
    "        {\n",
    "            'original': 'The cat sat on the comfortable mat'.split(),\n",
    "            'corrupted': 'The cat sat on the comfortable cat'.split(),\n",
    "            'explanation': 'Repetition replacement: mat -> cat'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"REPLACED TOKEN DETECTION (RTD) EXAMPLES:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(14, 4*len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ex_idx, example in enumerate(examples):\n",
    "        original = example['original']\n",
    "        corrupted = example['corrupted']\n",
    "        explanation = example['explanation']\n",
    "        \n",
    "        print(f\"\\nExample {ex_idx + 1}: {explanation}\")\n",
    "        print(f\"Original:  {' '.join(original)}\")\n",
    "        print(f\"Corrupted: {' '.join(corrupted)}\")\n",
    "        \n",
    "        # Find differences\n",
    "        labels = []\n",
    "        for i, (orig, corr) in enumerate(zip(original, corrupted)):\n",
    "            if orig != corr:\n",
    "                labels.append(1)  # Replaced\n",
    "                print(f\"Position {i}: '{orig}' -> '{corr}' [REPLACED]\")\n",
    "            else:\n",
    "                labels.append(0)  # Original\n",
    "        \n",
    "        # Visualize\n",
    "        colors = ['red' if label == 1 else 'lightblue' for label in labels]\n",
    "        bars = axes[ex_idx].bar(range(len(corrupted)), [1] * len(corrupted), color=colors)\n",
    "        axes[ex_idx].set_title(f'Example {ex_idx + 1}: {explanation}', fontweight='bold')\n",
    "        axes[ex_idx].set_ylim(0, 1.5)\n",
    "        axes[ex_idx].set_xticks(range(len(corrupted)))\n",
    "        axes[ex_idx].set_xticklabels(corrupted, rotation=45)\n",
    "        axes[ex_idx].set_ylabel('Token Status')\n",
    "        \n",
    "        # Add token labels on bars\n",
    "        for i, (word, label) in enumerate(zip(corrupted, labels)):\n",
    "            axes[ex_idx].text(i, 0.5, word, ha='center', va='center', \n",
    "                             fontweight='bold', color='white' if label == 1 else 'black')\n",
    "        \n",
    "        # Add legend for first plot\n",
    "        if ex_idx == 0:\n",
    "            axes[ex_idx].bar([], [], color='red', label='Replaced Token')\n",
    "            axes[ex_idx].bar([], [], color='lightblue', label='Original Token')\n",
    "            axes[ex_idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RTD Task Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RTD TASK CHARACTERISTICS:\")\n",
    "    print(\"\\n1. Binary Classification: Each token is either ORIGINAL or REPLACED\")\n",
    "    print(\"2. Contextual Understanding: Model must use context to detect anomalies\")\n",
    "    print(\"3. All Positions Matter: Every token contributes to the loss\")\n",
    "    print(\"4. Generator Quality: Better generator makes task harder (good!)\")\n",
    "    \n",
    "    print(\"\\nWHY RTD WORKS BETTER THAN MLM:\")\n",
    "    print(\"+ Dense learning signal (100% vs 15%)\")\n",
    "    print(\"+ Contextual reasoning required\")\n",
    "    print(\"+ Adversarial training effect\")\n",
    "    print(\"+ Encourages better representation learning\")\n",
    "    \n",
    "demonstrate_rtd_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: ELECTRA's Revolutionary Impact\n",
    "\n",
    "### **Why ELECTRA Ranks #2**\n",
    "\n",
    "1. **Efficiency Revolution**: 4x more sample efficient than BERT\n",
    "2. **Accessibility**: Small models achieve large model performance\n",
    "3. **Paradigm Shift**: From generative to discriminative pre-training\n",
    "4. **Practical Impact**: Enabled deployment in resource-constrained environments\n",
    "\n",
    "### **Core Innovation Comparison**\n",
    "\n",
    "| Aspect | BERT MLM | ELECTRA RTD |\n",
    "|--------|----------|-------------|\n",
    "| **Learning Signal** | 15% of tokens | 100% of tokens |\n",
    "| **Task** | Generate masked tokens | Detect replaced tokens |\n",
    "| **Architecture** | Single model | Generator + Discriminator |\n",
    "| **Sample Efficiency** | 1x | 4x |\n",
    "| **Small Model Performance** | Poor | Excellent |\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "1. **Dense Learning Signal**: Every token contributes to learning\n",
    "2. **Adversarial Training**: Generator-discriminator setup creates challenging examples\n",
    "3. **Contextual Understanding**: Model must understand context to detect replacements\n",
    "4. **Computational Efficiency**: More learning per compute unit\n",
    "\n",
    "**ELECTRA proved that smarter training objectives can dramatically improve efficiency while maintaining quality.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}