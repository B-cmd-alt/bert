{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Optimizations: Robustly Optimized BERT Pretraining\n",
    "\n",
    "**Rank**: #1 - Revolutionary Impact\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "When BERT was first published in 2018, it set new state-of-the-art results across many NLP tasks. However, researchers at Facebook AI noticed that BERT might not have been trained optimally. The original BERT paper left many hyperparameters and training choices unexplored.\n",
    "\n",
    "**The Problem**: BERT's training was:\n",
    "- Undertrained (only 1M steps vs RoBERTa's 500K)\n",
    "- Using static masking (same masks every epoch)\n",
    "- Training with Next Sentence Prediction (NSP) task\n",
    "- Using smaller batch sizes\n",
    "- Not utilizing the full potential of the model\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Dynamic Masking**: Why changing masks improves learning\n",
    "2. **Removing NSP**: Why this task hurts performance\n",
    "3. **Large Batch Training**: The benefits of bigger batches\n",
    "4. **Training Duration**: Why longer training helps\n",
    "5. **Implementation**: How to implement these optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set style for better visualizations\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid') \n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "        \n",
    "print(\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\")\n",
    "print(\"Paper: Liu et al., 2019 - Facebook AI Research\")\n",
    "print(\"Impact: Became the foundation for most modern BERT variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Original Paper Context\n",
    "\n",
    "### Paper Details\n",
    "- **Title**: \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "- **Authors**: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n",
    "- **Institution**: Facebook AI Research\n",
    "- **Published**: July 2019\n",
    "- **arXiv**: https://arxiv.org/abs/1907.11692\n",
    "\n",
    "### Key Contributions\n",
    "1. **Training longer** with bigger batches over more data\n",
    "2. **Removing NSP task** (Next Sentence Prediction)\n",
    "3. **Dynamic masking** instead of static masking\n",
    "4. **Text encoding** with larger byte-pair encoding vocabulary\n",
    "\n",
    "### Impact on the Field\n",
    "RoBERTa's optimizations became standard practice:\n",
    "- **DeBERTa** (2020): Built directly on RoBERTa principles\n",
    "- **ELECTRA** (2020): Used RoBERTa training setup\n",
    "- **SpanBERT** (2019): Adopted dynamic masking\n",
    "- **ALBERT** (2019): Used longer training and larger batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dynamic vs Static Masking\n",
    "\n",
    "**Static Masking (Original BERT)**: Mask tokens once during preprocessing, same masks every epoch\n",
    "**Dynamic Masking (RoBERTa)**: Generate new masks for each training example every time it's seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_masking_demo(text_tokens, mask_prob=0.15, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Demonstrate static masking - same masks every epoch\n",
    "    \"\"\"\n",
    "    # Generate masks once (static)\n",
    "    masks = np.random.rand(len(text_tokens)) < mask_prob\n",
    "    \n",
    "    print(\"STATIC MASKING (Original BERT):\")\n",
    "    print(f\"Original: {text_tokens}\")\n",
    "    \n",
    "    # Same masks every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        masked_tokens = text_tokens.copy()\n",
    "        masked_tokens[masks] = '[MASK]'\n",
    "        print(f\"Epoch {epoch+1}: {masked_tokens}\")\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def dynamic_masking_demo(text_tokens, mask_prob=0.15, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Demonstrate dynamic masking - different masks each epoch\n",
    "    \"\"\"\n",
    "    print(\"\\nDYNAMIC MASKING (RoBERTa):\")\n",
    "    print(f\"Original: {text_tokens}\")\n",
    "    \n",
    "    all_masks = []\n",
    "    # Generate new masks every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        masks = np.random.rand(len(text_tokens)) < mask_prob\n",
    "        masked_tokens = text_tokens.copy()\n",
    "        masked_tokens[masks] = '[MASK]'\n",
    "        print(f\"Epoch {epoch+1}: {masked_tokens}\")\n",
    "        all_masks.append(masks)\n",
    "    \n",
    "    return all_masks\n",
    "\n",
    "# Example sentence\n",
    "sentence = np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog'])\n",
    "\n",
    "# Demonstrate both approaches\n",
    "static_masks = static_masking_demo(sentence)\n",
    "dynamic_masks = dynamic_masking_demo(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masking patterns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Static masking visualization\n",
    "static_pattern = np.tile(static_masks, (5, 1))\n",
    "ax1.imshow(static_pattern, cmap='Reds', aspect='auto')\n",
    "ax1.set_title('Static Masking\\n(Same pattern every epoch)')\n",
    "ax1.set_ylabel('Epoch')\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_xticks(range(len(sentence)))\n",
    "ax1.set_xticklabels(sentence, rotation=45)\n",
    "ax1.set_yticks(range(5))\n",
    "ax1.set_yticklabels([f'Epoch {i+1}' for i in range(5)])\n",
    "\n",
    "# Dynamic masking visualization\n",
    "dynamic_pattern = np.array(dynamic_masks)\n",
    "ax2.imshow(dynamic_pattern, cmap='Blues', aspect='auto')\n",
    "ax2.set_title('Dynamic Masking\\n(Different pattern each epoch)')\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_xticks(range(len(sentence)))\n",
    "ax2.set_xticklabels(sentence, rotation=45)\n",
    "ax2.set_yticks(range(5))\n",
    "ax2.set_yticklabels([f'Epoch {i+1}' for i in range(5)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate diversity metrics\n",
    "static_diversity = 1  # Always the same\n",
    "dynamic_diversity = np.mean([np.sum(mask1 != mask2) for i, mask1 in enumerate(dynamic_masks) \n",
    "                           for j, mask2 in enumerate(dynamic_masks) if i != j])\n",
    "\n",
    "print(f\"\\nMasking Diversity:\")\n",
    "print(f\"Static masking: Always identical patterns\")\n",
    "print(f\"Dynamic masking: Average {dynamic_diversity:.1f} different positions between epochs\")\n",
    "print(f\"\\nBenefit: Dynamic masking provides {len(sentence)} × more training diversity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Removing Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT was trained with two objectives:\n",
    "1. **Masked Language Modeling (MLM)**: Predict masked tokens\n",
    "2. **Next Sentence Prediction (NSP)**: Predict if sentence B follows sentence A\n",
    "\n",
    "RoBERTa showed that NSP actually **hurts** performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate NSP task problems\ndef analyze_nsp_problems():\n    \"\"\"\n    Demonstrate why NSP is problematic\n    \"\"\"\n    \n    # Example sentences\n    sentences = [\n        \"The cat sat on the mat.\",\n        \"It was a sunny day outside.\",\n        \"Machine learning is fascinating.\",\n        \"The weather forecast predicts rain.\",\n        \"Python is a programming language.\"\n    ]\n    \n    print(\"PROBLEMS WITH NEXT SENTENCE PREDICTION (NSP):\")\n    print(\"\\n1. Too Easy - Topic Classification Rather Than Coherence:\")\n    \n    # Show how NSP becomes topic classification\n    pairs = [\n        (sentences[0], sentences[1], \"Not Next\", \"Different topics: cats vs weather\"),\n        (sentences[2], sentences[4], \"Not Next\", \"Different topics: ML vs programming\"),\n        (sentences[0], \"The mat was comfortable for sleeping.\", \"Next\", \"Same topic: cats and mats\")\n    ]\n    \n    for sent_a, sent_b, label, reason in pairs:\n        print(f\"\\n  Sentence A: '{sent_a}'\")\n        print(f\"  Sentence B: '{sent_b}'\")\n        print(f\"  Label: {label} - {reason}\")\n    \n    print(\"\\n2. Artificial Task - Not Realistic:\")\n    print(\"   - Real documents don't have random sentence pairs\")\n    print(\"   - Model learns topic classification, not coherence\")\n    print(\"   - Takes away training time from more useful MLM\")\n    \n    print(\"\\n3. RoBERTa's Solution:\")\n    print(\"   - Remove NSP completely\")\n    print(\"   - Use FULL SENTENCES (not sentence pairs)\")\n    print(\"   - Pack multiple sentences up to max_length\")\n    print(\"   - Focus 100% effort on MLM learning\")\n\nanalyze_nsp_problems()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate RoBERTa's full sentence approach\n",
    "def roberta_sentence_packing(sentences, max_length=50):\n",
    "    \"\"\"\n",
    "    Show how RoBERTa packs sentences instead of using sentence pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize_simple(sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    print(\"RoBERTa's FULL SENTENCE APPROACH:\")\n",
    "    print(\"\\nInstead of [CLS] Sentence A [SEP] Sentence B [SEP]\")\n",
    "    print(\"Use: [CLS] Sent1 Sent2 Sent3... [SEP] (up to max_length)\")\n",
    "    \n",
    "    # Pack sentences\n",
    "    packed_examples = []\n",
    "    current_example = ['[CLS]']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize_simple(sentence)\n",
    "        \n",
    "        # Check if adding this sentence exceeds max_length\n",
    "        if len(current_example) + len(tokens) + 1 <= max_length:  # +1 for [SEP]\n",
    "            current_example.extend(tokens)\n",
    "        else:\n",
    "            # Finish current example and start new one\n",
    "            current_example.append('[SEP]')\n",
    "            packed_examples.append(current_example.copy())\n",
    "            current_example = ['[CLS]'] + tokens\n",
    "    \n",
    "    # Add final example\n",
    "    if len(current_example) > 1:\n",
    "        current_example.append('[SEP]')\n",
    "        packed_examples.append(current_example)\n",
    "    \n",
    "    # Show results\n",
    "    for i, example in enumerate(packed_examples):\n",
    "        print(f\"\\nExample {i+1} (length {len(example)}):\")\n",
    "        print(\" \".join(example))\n",
    "    \n",
    "    return packed_examples\n",
    "\n",
    "# Example documents\n",
    "long_sentences = [\n",
    "    \"The cat sat on the mat because it was comfortable.\",\n",
    "    \"It was a sunny day outside with clear blue skies.\",\n",
    "    \"The weather forecast predicted rain for tomorrow.\",\n",
    "    \"Machine learning algorithms require large amounts of training data.\",\n",
    "    \"Python is a versatile programming language used in data science.\",\n",
    "    \"Deep learning models can learn complex patterns from examples.\"\n",
    "]\n",
    "\n",
    "packed = roberta_sentence_packing(long_sentences)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENEFITS OF FULL SENTENCE APPROACH:\")\n",
    "print(\"✓ More natural text understanding\")\n",
    "print(\"✓ No artificial sentence pairing\")\n",
    "print(\"✓ Better use of sequence length\")\n",
    "print(\"✓ Focuses learning on language modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Large Batch Training\n",
    "\n",
    "RoBERTa showed that larger batch sizes significantly improve performance. Let's understand why mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_batch_size_effects(small_batch=32, large_batch=8192, num_updates=1000):\n",
    "    \"\"\"\n",
    "    Simulate the effect of different batch sizes on gradient estimates\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate \"true\" gradient (what we want to estimate)\n",
    "    true_gradient = np.array([1.0, -0.5, 0.3, -0.2, 0.8])\n",
    "    \n",
    "    # Function to simulate noisy gradient estimate\n",
    "    def get_gradient_estimate(batch_size, true_grad, noise_scale=0.3):\n",
    "        # Larger batches have less noise (Central Limit Theorem)\n",
    "        noise = np.random.normal(0, noise_scale / np.sqrt(batch_size), len(true_grad))\n",
    "        return true_grad + noise\n",
    "    \n",
    "    # Collect gradient estimates\n",
    "    small_batch_gradients = []\n",
    "    large_batch_gradients = []\n",
    "    \n",
    "    for _ in range(num_updates):\n",
    "        small_grad = get_gradient_estimate(small_batch, true_gradient)\n",
    "        large_grad = get_gradient_estimate(large_batch, true_gradient)\n",
    "        \n",
    "        small_batch_gradients.append(small_grad)\n",
    "        large_batch_gradients.append(large_grad)\n",
    "    \n",
    "    small_batch_gradients = np.array(small_batch_gradients)\n",
    "    large_batch_gradients = np.array(large_batch_gradients)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"BATCH SIZE EFFECTS ON GRADIENT ESTIMATION:\")\n",
    "    print(f\"\\nTrue gradient: {true_gradient}\")\n",
    "    print(f\"\\nSmall batch (size {small_batch}):\")\n",
    "    print(f\"  Mean estimate: {small_batch_gradients.mean(axis=0)}\")\n",
    "    print(f\"  Std deviation: {small_batch_gradients.std(axis=0)}\")\n",
    "    \n",
    "    print(f\"\\nLarge batch (size {large_batch}):\")\n",
    "    print(f\"  Mean estimate: {large_batch_gradients.mean(axis=0)}\")\n",
    "    print(f\"  Std deviation: {large_batch_gradients.std(axis=0)}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot first gradient component over time\n",
    "    axes[0].plot(small_batch_gradients[:100, 0], alpha=0.7, label=f'Small batch ({small_batch})', color='red')\n",
    "    axes[0].plot(large_batch_gradients[:100, 0], alpha=0.7, label=f'Large batch ({large_batch})', color='blue')\n",
    "    axes[0].axhline(y=true_gradient[0], color='green', linestyle='--', label='True gradient')\n",
    "    axes[0].set_title('Gradient Component 1 Over Time')\n",
    "    axes[0].set_xlabel('Update Step')\n",
    "    axes[0].set_ylabel('Gradient Value')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of gradient estimates\n",
    "    axes[1].hist(small_batch_gradients[:, 0], bins=30, alpha=0.5, label=f'Small batch ({small_batch})', color='red', density=True)\n",
    "    axes[1].hist(large_batch_gradients[:, 0], bins=30, alpha=0.5, label=f'Large batch ({large_batch})', color='blue', density=True)\n",
    "    axes[1].axvline(x=true_gradient[0], color='green', linestyle='--', label='True gradient')\n",
    "    axes[1].set_title('Distribution of Gradient Estimates')\n",
    "    axes[1].set_xlabel('Gradient Value')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return small_batch_gradients, large_batch_gradients\n",
    "\n",
    "small_grads, large_grads = simulate_batch_size_effects()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WHY LARGE BATCHES WORK BETTER:\")\n",
    "print(\"1. Better gradient estimates (less noise)\")\n",
    "print(\"2. More stable training (less oscillation)\")\n",
    "print(\"3. Can use larger learning rates\")\n",
    "print(\"4. Better convergence to optimal solution\")\n",
    "print(\"\\nRoBERTa used batches of 8K vs BERT's 256!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Duration and Learning Rate\n",
    "\n",
    "RoBERTa trained much longer than BERT and used different learning rate schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_training_schedules():\n",
    "    \"\"\"\n",
    "    Compare BERT vs RoBERTa training schedules\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training configurations\n",
    "    bert_steps = 1000000\n",
    "    roberta_steps = 500000\n",
    "    \n",
    "    bert_batch_size = 256\n",
    "    roberta_batch_size = 8192\n",
    "    \n",
    "    # Calculate effective training (tokens seen)\n",
    "    sequence_length = 512\n",
    "    \n",
    "    bert_tokens = bert_steps * bert_batch_size * sequence_length\n",
    "    roberta_tokens = roberta_steps * roberta_batch_size * sequence_length\n",
    "    \n",
    "    print(\"TRAINING COMPARISON:\")\n",
    "    print(f\"\\nBERT:\")\n",
    "    print(f\"  Steps: {bert_steps:,}\")\n",
    "    print(f\"  Batch size: {bert_batch_size}\")\n",
    "    print(f\"  Total tokens: {bert_tokens:,} ({bert_tokens/1e9:.1f}B)\")\n",
    "    \n",
    "    print(f\"\\nRoBERTa:\")\n",
    "    print(f\"  Steps: {roberta_steps:,}\")\n",
    "    print(f\"  Batch size: {roberta_batch_size}\")\n",
    "    print(f\"  Total tokens: {roberta_tokens:,} ({roberta_tokens/1e9:.1f}B)\")\n",
    "    \n",
    "    print(f\"\\nRoBERTa saw {roberta_tokens/bert_tokens:.1f}x more tokens!\")\n",
    "    \n",
    "    # Simulate learning curves\n",
    "    def learning_curve(total_steps, batch_size, warmup_steps=10000):\n",
    "        steps = np.linspace(0, total_steps, 1000)\n",
    "        \n",
    "        # Warmup + polynomial decay\n",
    "        lr_schedule = np.zeros_like(steps)\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            if step < warmup_steps:\n",
    "                lr_schedule[i] = step / warmup_steps\n",
    "            else:\n",
    "                decay = 1 - (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "                lr_schedule[i] = max(0, decay)\n",
    "        \n",
    "        # Simulate loss (inversely related to training progress)\n",
    "        # Larger batches allow better convergence\n",
    "        batch_factor = np.log(batch_size / 256) * 0.1 + 1\n",
    "        progress = np.cumsum(lr_schedule) / np.sum(lr_schedule)\n",
    "        loss = 4.0 * np.exp(-3 * progress * batch_factor) + 0.5\n",
    "        \n",
    "        return steps, lr_schedule, loss\n",
    "    \n",
    "    # Generate curves\n",
    "    bert_steps_arr, bert_lr, bert_loss = learning_curve(bert_steps, bert_batch_size)\n",
    "    roberta_steps_arr, roberta_lr, roberta_loss = learning_curve(roberta_steps, roberta_batch_size)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Learning rate schedules\n",
    "    ax1.plot(bert_steps_arr/1000, bert_lr, label='BERT', color='red', linewidth=2)\n",
    "    ax1.plot(roberta_steps_arr/1000, roberta_lr, label='RoBERTa', color='blue', linewidth=2)\n",
    "    ax1.set_xlabel('Training Steps (thousands)')\n",
    "    ax1.set_ylabel('Learning Rate')\n",
    "    ax1.set_title('Learning Rate Schedules')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax2.plot(bert_steps_arr/1000, bert_loss, label='BERT', color='red', linewidth=2)\n",
    "    ax2.plot(roberta_steps_arr/1000, roberta_loss, label='RoBERTa', color='blue', linewidth=2)\n",
    "    ax2.set_xlabel('Training Steps (thousands)')\n",
    "    ax2.set_ylabel('Training Loss')\n",
    "    ax2.set_title('Training Loss Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal losses: BERT={bert_loss[-1]:.3f}, RoBERTa={roberta_loss[-1]:.3f}\")\n",
    "    print(f\"Improvement: {(bert_loss[-1] - roberta_loss[-1])/bert_loss[-1]*100:.1f}%\")\n",
    "\ncompare_training_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Implementation - RoBERTa Training Loop\n",
    "\n",
    "Let's implement a simplified version of RoBERTa's training improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaTrainer:\n",
    "    \"\"\"\n",
    "    Simplified RoBERTa trainer implementing key optimizations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=8192, max_length=512, mask_prob=0.15):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token_id = vocab_size - 1  # [MASK]\n",
    "        \n",
    "    def dynamic_masking(self, input_ids):\n",
    "        \"\"\"\n",
    "        Apply dynamic masking (new masks each time)\n",
    "        \"\"\"\n",
    "        masked_ids = input_ids.copy()\n",
    "        labels = input_ids.copy()\n",
    "        \n",
    "        # Generate random mask for each example\n",
    "        for i in range(len(input_ids)):\n",
    "            # Randomly select positions to mask\n",
    "            mask_positions = np.random.rand(len(input_ids[i])) < self.mask_prob\n",
    "            \n",
    "            # Skip special tokens (first and last)\n",
    "            mask_positions[0] = False  # [CLS]\n",
    "            mask_positions[-1] = False  # [SEP]\n",
    "            \n",
    "            # Apply masking strategy (80% MASK, 10% random, 10% original)\n",
    "            mask_indices = np.where(mask_positions)[0]\n",
    "            \n",
    "            for pos in mask_indices:\n",
    "                rand = np.random.random()\n",
    "                if rand < 0.8:\n",
    "                    masked_ids[i][pos] = self.mask_token_id  # 80% MASK\n",
    "                elif rand < 0.9:\n",
    "                    masked_ids[i][pos] = np.random.randint(0, self.vocab_size)  # 10% random\n",
    "                # 10% keep original (no change needed)\n",
    "            \n",
    "            # Only compute loss on masked positions\n",
    "            labels[i][~mask_positions] = -100  # Ignore in loss\n",
    "        \n",
    "        return masked_ids, labels\n",
    "    \n",
    "    def pack_sentences(self, sentences, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Pack multiple sentences into examples (RoBERTa approach)\n",
    "        \"\"\"\n",
    "        if tokenizer is None:\n",
    "            # Simple whitespace tokenizer\n",
    "            tokenizer = lambda x: x.split()\n",
    "        \n",
    "        CLS_TOKEN = 0\n",
    "        SEP_TOKEN = 1\n",
    "        \n",
    "        packed_examples = []\n",
    "        current_tokens = [CLS_TOKEN]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer(sentence)\n",
    "            # Convert to IDs (simplified)\n",
    "            token_ids = [hash(token) % (self.vocab_size - 100) + 100 for token in tokens]\n",
    "            \n",
    "            if len(current_tokens) + len(token_ids) + 1 <= self.max_length:\n",
    "                current_tokens.extend(token_ids)\n",
    "            else:\n",
    "                # Finish current example\n",
    "                current_tokens.append(SEP_TOKEN)\n",
    "                # Pad to max_length\n",
    "                while len(current_tokens) < self.max_length:\n",
    "                    current_tokens.append(0)  # PAD\n",
    "                \n",
    "                packed_examples.append(current_tokens)\n",
    "                current_tokens = [CLS_TOKEN] + token_ids\n",
    "        \n",
    "        # Add final example\n",
    "        if len(current_tokens) > 1:\n",
    "            current_tokens.append(SEP_TOKEN)\n",
    "            while len(current_tokens) < self.max_length:\n",
    "                current_tokens.append(0)\n",
    "            packed_examples.append(current_tokens)\n",
    "        \n",
    "        return np.array(packed_examples)\n",
    "    \n",
    "    def train_step(self, input_ids):\n",
    "        \"\"\"\n",
    "        Single training step with RoBERTa improvements\n",
    "        \"\"\"\n",
    "        # 1. Apply dynamic masking\n",
    "        masked_ids, labels = self.dynamic_masking(input_ids)\n",
    "        \n",
    "        # 2. Forward pass (simplified)\n",
    "        # In real implementation, this would be the model forward pass\n",
    "        logits = np.random.randn(len(masked_ids), self.max_length, self.vocab_size)\n",
    "        \n",
    "        # 3. Compute MLM loss (only on masked positions)\n",
    "        loss = 0\n",
    "        num_masked = 0\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] != -100:  # Masked position\n",
    "                    # Cross-entropy loss (simplified)\n",
    "                    target = labels[i][j]\n",
    "                    pred_logits = logits[i][j]\n",
    "                    # Softmax + cross-entropy\n",
    "                    exp_logits = np.exp(pred_logits - np.max(pred_logits))\n",
    "                    probs = exp_logits / np.sum(exp_logits)\n",
    "                    loss += -np.log(probs[target] + 1e-10)\n",
    "                    num_masked += 1\n",
    "        \n",
    "        loss = loss / num_masked if num_masked > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'num_masked': num_masked,\n",
    "            'masked_ids': masked_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Demonstrate RoBERTa trainer\n",
    "trainer = RoBERTaTrainer()\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning models learn patterns from data\",\n",
    "    \"Natural language processing enables computers to understand text\",\n",
    "    \"Transformers revolutionized the field of artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Pack sentences (RoBERTa style)\n",
    "packed_examples = trainer.pack_sentences(sentences)\n",
    "print(f\"Packed {len(sentences)} sentences into {len(packed_examples)} examples\")\n",
    "print(f\"Example shape: {packed_examples.shape}\")\n",
    "\n",
    "# Show dynamic masking in action\n",
    "print(\"\\nDynamic Masking Examples:\")\n",
    "for epoch in range(3):\n",
    "    result = trainer.train_step(packed_examples[:1])  # Just first example\n",
    "    masked_positions = np.sum(result['labels'][0] != -100)\n",
    "    print(f\"Epoch {epoch+1}: {masked_positions} tokens masked, loss = {result['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Results and Impact Analysis\n",
    "\n",
    "Let's analyze the empirical results that made RoBERTa so influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_roberta_results():\n",
    "    \"\"\"\n",
    "    Analyze and visualize RoBERTa's performance improvements\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated results based on the original paper\n",
    "    tasks = ['MNLI', 'QQP', 'QNLI', 'SST-2', 'CoLA', 'STS-B', 'MRPC', 'RTE']\n",
    "    \n",
    "    # Performance scores (simplified/approximated from paper)\n",
    "    bert_scores = [84.6, 71.2, 90.5, 93.5, 52.1, 85.8, 88.9, 66.4]\n",
    "    roberta_scores = [87.6, 72.1, 92.8, 94.8, 63.6, 86.5, 90.2, 78.7]\n",
    "    \n",
    "    improvements = [(r - b) for b, r in zip(bert_scores, roberta_scores)]\n",
    "    \n",
    "    print(\"RoBERTa PERFORMANCE IMPROVEMENTS:\")\n",
    "    print(\"\\nTask\\t\\tBERT\\tRoBERTa\\tImprovement\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for task, bert, roberta, imp in zip(tasks, bert_scores, roberta_scores, improvements):\n",
    "        print(f\"{task:8s}\\t{bert:5.1f}\\t{roberta:7.1f}\\t+{imp:5.1f}\")\n",
    "    \n",
    "    avg_improvement = np.mean(improvements)\n",
    "    print(f\"\\nAverage improvement: +{avg_improvement:.1f} points\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Performance comparison\n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, bert_scores, width, label='BERT', color='lightcoral', alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, roberta_scores, width, label='RoBERTa', color='skyblue', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Tasks')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('BERT vs RoBERTa Performance')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(tasks, rotation=45)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1 + bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Improvement breakdown\n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    bars = ax2.bar(tasks, improvements, color=colors, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.set_xlabel('Tasks')\n",
    "    ax2.set_ylabel('Improvement (points)')\n",
    "    ax2.set_title('RoBERTa Improvements by Task')\n",
    "    ax2.set_xticklabels(tasks, rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + (0.2 if height > 0 else -0.5),\n",
    "                f'+{imp:.1f}', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "improvements = analyze_roberta_results()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS FROM ROBERTA:\")\n",
    "print(\"\\n1. Training Recipe Matters More Than Architecture:\")\n",
    "print(\"   - Same architecture as BERT\")\n",
    "print(\"   - All improvements from training optimizations\")\n",
    "print(\"\\n2. Established New Training Best Practices:\")\n",
    "print(\"   - Dynamic masking became standard\")\n",
    "print(\"   - NSP removal adopted by most models\")\n",
    "print(\"   - Large batch training became norm\")\n",
    "print(\"\\n3. Influenced Entire Field:\")\n",
    "print(\"   - DeBERTa, ELECTRA, SpanBERT all use RoBERTa training\")\n",
    "print(\"   - Showed importance of hyperparameter tuning\")\n",
    "print(\"   - Computational efficiency became focus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: RoBERTa's Revolutionary Impact\n",
    "\n",
    "### **Why RoBERTa Ranks #1**\n",
    "\n",
    "1. **Foundational Impact**: Established training practices still used today\n",
    "2. **Simplicity**: No architectural changes, just better training\n",
    "3. **Universal Adoption**: Every major transformer model uses RoBERTa principles\n",
    "4. **Paradigm Shift**: Showed training recipe > architecture novelty\n",
    "\n",
    "### **Key Contributions**\n",
    "\n",
    "| Technique | Original BERT | RoBERTa | Impact |\n",
    "|-----------|---------------|---------|--------|\n",
    "| **Masking** | Static | Dynamic | +2-3 points across tasks |\n",
    "| **NSP Task** | Used | Removed | +1-2 points improvement |\n",
    "| **Batch Size** | 256 | 8,192 | Better convergence |\n",
    "| **Training** | 1M steps | 500K steps | 10x more tokens seen |\n",
    "\n",
    "### **Legacy and Influence**\n",
    "\n",
    "**Direct Descendants:**\n",
    "- **DeBERTa**: Built on RoBERTa + disentangled attention\n",
    "- **ELECTRA**: Used RoBERTa training setup\n",
    "- **SpanBERT**: Adopted dynamic masking\n",
    "\n",
    "**Principles Established:**\n",
    "1. **Dynamic masking** is superior to static\n",
    "2. **NSP task is harmful** for most applications  \n",
    "3. **Large batches** improve convergence\n",
    "4. **Training duration** matters more than model size\n",
    "5. **Hyperparameter tuning** can rival architectural innovations\n",
    "\n",
    "### **Practical Takeaways**\n",
    "\n",
    "For anyone training transformer models:\n",
    "- ✅ Use dynamic masking\n",
    "- ✅ Skip sentence-pair tasks unless needed\n",
    "- ✅ Use largest batch size possible\n",
    "- ✅ Train longer with more data\n",
    "- ✅ Focus on data quality and training recipe\n",
    "\n",
    "**RoBERTa proved that sometimes the best innovation is doing the basics better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Dynamic Masking Experiment**: Implement static vs dynamic masking on the same dataset. Measure convergence speed.\n",
    "\n",
    "2. **Batch Size Analysis**: Train a small model with different batch sizes (32, 128, 512). Plot loss curves.\n",
    "\n",
    "3. **NSP Ablation**: Create a simple sentence pair classification task. Compare with and without NSP pre-training.\n",
    "\n",
    "4. **Sentence Packing**: Implement efficient sentence packing for different document lengths. Measure utilization.\n",
    "\n",
    "5. **Learning Rate Scaling**: Experiment with learning rate scaling for different batch sizes using the √batch_size rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n",
    "# Try implementing the exercises above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}