{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Evaluation in BERT\n",
    "\n",
    "This notebook covers how to use trained BERT models for inference and evaluate their performance.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Running inference on new text\n",
    "2. Different evaluation metrics\n",
    "3. Probing tasks to understand what BERT learned\n",
    "4. Fine-tuning for downstream tasks\n",
    "5. Analyzing model behavior and limitations\n",
    "6. Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nsys.path.append('..')\n\nfrom model import MiniBERT\nfrom tokenizer import WordPieceTokenizer\nfrom mlm import mask_tokens\n# Note: metrics module might not exist, we'll implement what we need\nimport time\n\nnp.random.seed(42)\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-darkgrid') \n    except OSError:\n        plt.style.use('default')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Inference\n",
    "\n",
    "How to use BERT to make predictions on new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model and tokenizer\nmodel = MiniBERT()\ntokenizer = WordPieceTokenizer()\ntokenizer.load_model('../tokenizer_8k.pkl')  # Fixed: Use load_model instead of .load\n\ndef run_inference(text, model, tokenizer, mask_token='[MASK]'):\n    \"\"\"\n    Run inference on text with masked tokens.\n    \"\"\"\n    print(f\"Input text: '{text}'\")\n    \n    # Tokenize\n    tokens = text.split()\n    \n    # Find mask positions in original text\n    mask_positions_text = [i for i, token in enumerate(tokens) if token == mask_token]\n    \n    if not mask_positions_text:\n        print(\"No [MASK] tokens found in input text.\")\n        return\n    \n    # Encode with tokenizer\n    input_ids = tokenizer.encode(text)\n    input_batch = np.array([input_ids])\n    \n    # Forward pass\n    logits, cache = model.forward(input_batch)\n    probabilities = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n    probabilities = probabilities / np.sum(probabilities, axis=-1, keepdims=True)\n    \n    # Find mask token ID positions in tokenized sequence\n    mask_token_id = tokenizer.vocab[mask_token]\n    tokenized_sequence = input_ids\n    mask_positions = [i for i, token_id in enumerate(tokenized_sequence) if token_id == mask_token_id]\n    \n    # Create inverse vocab mapping for display\n    inv_vocab = {v: k for k, v in tokenizer.vocab.items()}\n    \n    # Make predictions for each mask\n    predictions = []\n    for pos in mask_positions:\n        pos_probs = probabilities[0, pos]\n        top_k = 5\n        top_indices = np.argsort(pos_probs)[-top_k:][::-1]\n        top_probs = pos_probs[top_indices]\n        \n        # Get token strings\n        top_tokens = []\n        for idx in top_indices:\n            token = inv_vocab.get(idx, f'ID_{idx}')\n            top_tokens.append(token)\n        \n        predictions.append({\n            'position': pos,\n            'top_predictions': list(zip(top_tokens, top_probs))\n        })\n    \n    # Display results\n    print(f\"\\\\nPredictions for {len(mask_positions)} masked position(s):\")\n    for i, pred in enumerate(predictions):\n        print(f\"\\\\nMask {i+1} (position {pred['position']}):\")\n        for rank, (token, prob) in enumerate(pred['top_predictions']):\n            print(f\"  {rank+1}. '{token}' ({prob*100:.2f}%)\")\n    \n    return predictions\n\n# Test inference\ntest_sentences = [\n    \"The cat sat on the [MASK].\",\n    \"I love [MASK] learning.\",\n    \"The [MASK] is shining brightly.\",\n    \"She is a [MASK] student.\"\n]\n\nprint(\"=\" * 60)\nprint(\"INFERENCE EXAMPLES\")\nprint(\"=\" * 60)\n\nfor i, sentence in enumerate(test_sentences[:2]):  # Test first 2\n    print(f\"\\\\nExample {i+1}:\")\n    print(\"-\" * 30)\n    predictions = run_inference(sentence, model, tokenizer)\n    if i < len(test_sentences) - 1:\n        print(\"\\\\n\" + \"=\"*40)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metrics\n",
    "\n",
    "How to measure BERT's performance quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def comprehensive_evaluation(model, tokenizer, test_texts, mask_prob=0.15):\n    \"\"\"\n    Comprehensive evaluation of BERT model.\n    \"\"\"\n    results = {\n        'total_examples': 0,\n        'total_masked_tokens': 0,\n        'correct_predictions': 0,\n        'losses': [],\n        'top_k_accuracies': {1: 0, 3: 0, 5: 0},\n        'inference_times': []\n    }\n    \n    print(f\"Evaluating on {len(test_texts)} examples...\")\n    \n    for text_idx, text in enumerate(test_texts):\n        # Tokenize and mask\n        token_ids = tokenizer.encode(text)\n        masked_ids, target_ids, mask_positions = mask_tokens(\n            np.array([token_ids]),\n            vocab_size=len(tokenizer.vocab),\n            mask_id=tokenizer.vocab['[MASK]'],\n            p_mask=mask_prob\n        )\n        \n        # Extract actual masked positions and targets\n        actual_positions = []\n        actual_targets = []\n        \n        if isinstance(target_ids, np.ndarray) and target_ids.shape == masked_ids.shape:\n            sentinel = -100\n            for pos in range(len(target_ids[0])):\n                if target_ids[0][pos] != sentinel:\n                    actual_positions.append(pos)\n                    actual_targets.append(target_ids[0][pos])\n        \n        if not actual_positions:\n            continue\n        \n        results['total_examples'] += 1\n        \n        # Time inference\n        start_time = time.time()\n        logits, _ = model.forward(masked_ids)\n        inference_time = time.time() - start_time\n        results['inference_times'].append(inference_time)\n        \n        # Convert to probabilities\n        probabilities = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n        probabilities = probabilities / np.sum(probabilities, axis=-1, keepdims=True)\n        \n        # Evaluate each masked token\n        for pos, target_id in zip(actual_positions, actual_targets):\n            pos_probs = probabilities[0, pos]\n            \n            # Top-k predictions\n            top_indices = np.argsort(pos_probs)[::-1]\n            \n            # Check top-k accuracy\n            for k in [1, 3, 5]:\n                if target_id in top_indices[:k]:\n                    results['top_k_accuracies'][k] += 1\n            \n            # Compute loss\n            token_loss = -np.log(pos_probs[target_id] + 1e-10)\n            results['losses'].append(token_loss)\n            \n            results['total_masked_tokens'] += 1\n    \n    # Compute final metrics\n    if results['total_masked_tokens'] > 0:\n        for k in results['top_k_accuracies']:\n            results['top_k_accuracies'][k] /= results['total_masked_tokens']\n        \n        results['average_loss'] = np.mean(results['losses'])\n        results['perplexity'] = np.exp(results['average_loss'])\n    \n    if results['inference_times']:\n        results['avg_inference_time'] = np.mean(results['inference_times'])\n        results['total_inference_time'] = np.sum(results['inference_times'])\n    \n    return results\n\n# Create evaluation dataset\nevaluation_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is revolutionizing artificial intelligence.\",\n    \"The weather today is sunny and warm.\",\n    \"She finished her homework before dinner.\",\n    \"The library has many interesting books to read.\",\n    \"Computer science requires logical thinking skills.\",\n    \"The concert was absolutely amazing last night.\",\n    \"Fresh vegetables are important for good health.\"\n]\n\n# Run evaluation\nprint(\"Running comprehensive evaluation...\")\neval_results = comprehensive_evaluation(model, tokenizer, evaluation_texts)\n\n# Display results\nprint(\"\\\\n\" + \"=\" * 60)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 60)\n\nprint(f\"Total examples evaluated: {eval_results['total_examples']}\")\nprint(f\"Total masked tokens: {eval_results['total_masked_tokens']}\")\n\nif eval_results['total_masked_tokens'] > 0:\n    print(f\"\\\\nAccuracy Metrics:\")\n    for k, acc in eval_results['top_k_accuracies'].items():\n        print(f\"  Top-{k} accuracy: {acc:.3f} ({acc*100:.1f}%)\")\n    \n    print(f\"\\\\nLoss Metrics:\")\n    print(f\"  Average loss: {eval_results['average_loss']:.4f}\")\n    print(f\"  Perplexity: {eval_results['perplexity']:.2f}\")\n\nif eval_results['inference_times']:\n    print(f\"\\\\nPerformance Metrics:\")\n    print(f\"  Average inference time: {eval_results['avg_inference_time']*1000:.2f} ms\")\n    print(f\"  Total evaluation time: {eval_results['total_inference_time']:.2f} seconds\")\n    \n    tokens_per_second = eval_results['total_masked_tokens'] / eval_results['total_inference_time']\n    print(f\"  Tokens per second: {tokens_per_second:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Probing Tasks\n",
    "\n",
    "Understanding what linguistic knowledge BERT has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pos_probe(model, tokenizer, sentences_with_pos):\n",
    "    \"\"\"\n",
    "    Simple POS tagging probe to test syntactic knowledge.\n",
    "    \"\"\"\n",
    "    print(\"Running POS Tagging Probe...\")\n",
    "    \n",
    "    # Extract representations\n",
    "    all_representations = []\n",
    "    all_pos_tags = []\n",
    "    \n",
    "    for sentence, pos_tags in sentences_with_pos:\n",
    "        # Tokenize\n",
    "        token_ids = tokenizer.encode(sentence)\n",
    "        input_batch = np.array([token_ids])\n",
    "        \n",
    "        # Get representations from final layer\n",
    "        logits, cache = model.forward(input_batch)\n",
    "        \n",
    "        # Extract hidden states (before final projection)\n",
    "        # We'll use the logits as a proxy for final hidden states\n",
    "        # In a real implementation, you'd extract the actual hidden states\n",
    "        representations = logits[0]  # [seq_len, vocab_size]\n",
    "        \n",
    "        # Align with POS tags (simplified)\n",
    "        tokens = sentence.split()\n",
    "        min_len = min(len(tokens), len(pos_tags), representations.shape[0])\n",
    "        \n",
    "        for i in range(min_len):\n",
    "            all_representations.append(representations[i])\n",
    "            all_pos_tags.append(pos_tags[i])\n",
    "    \n",
    "    # Simple analysis: cluster representations by POS tag\n",
    "    pos_tag_reps = {}\n",
    "    for rep, tag in zip(all_representations, all_pos_tags):\n",
    "        if tag not in pos_tag_reps:\n",
    "            pos_tag_reps[tag] = []\n",
    "        pos_tag_reps[tag].append(rep)\n",
    "    \n",
    "    # Compute average representation for each POS tag\n",
    "    pos_centroids = {}\n",
    "    for tag, reps in pos_tag_reps.items():\n",
    "        if len(reps) > 0:\n",
    "            pos_centroids[tag] = np.mean(reps, axis=0)\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(all_representations)} tokens\")\n",
    "    print(f\"Found {len(pos_centroids)} POS tags: {list(pos_centroids.keys())}\")\n",
    "    \n",
    "    # Compute pairwise similarities between POS centroids\n",
    "    if len(pos_centroids) > 1:\n",
    "        print(\"\\nPOS Tag Similarities (cosine similarity):\")\n",
    "        tags = list(pos_centroids.keys())\n",
    "        \n",
    "        for i, tag1 in enumerate(tags):\n",
    "            for j, tag2 in enumerate(tags):\n",
    "                if i < j:\n",
    "                    vec1 = pos_centroids[tag1]\n",
    "                    vec2 = pos_centroids[tag2]\n",
    "                    \n",
    "                    # Cosine similarity\n",
    "                    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "                    print(f\"  {tag1} - {tag2}: {similarity:.3f}\")\n",
    "    \n",
    "    return pos_centroids\n",
    "\n",
    "# Simple POS-tagged sentences for probing\n",
    "pos_examples = [\n",
    "    (\"The cat sat\", [\"DET\", \"NOUN\", \"VERB\"]),\n",
    "    (\"She runs quickly\", [\"PRON\", \"VERB\", \"ADV\"]),\n",
    "    (\"Big dogs bark\", [\"ADJ\", \"NOUN\", \"VERB\"]),\n",
    "    (\"The quick fox\", [\"DET\", \"ADJ\", \"NOUN\"]),\n",
    "]\n",
    "\n",
    "pos_centroids = simple_pos_probe(model, tokenizer, pos_examples)\n",
    "\n",
    "# Semantic similarity probe\n",
    "def semantic_similarity_probe(model, tokenizer, word_pairs):\n",
    "    \"\"\"\n",
    "    Test semantic similarity understanding.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SEMANTIC SIMILARITY PROBE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for pair_type, pairs in word_pairs.items():\n",
    "        print(f\"\\n{pair_type.upper()} pairs:\")\n",
    "        \n",
    "        similarities = []\n",
    "        for word1, word2 in pairs:\n",
    "            # Get representations for each word in context\n",
    "            sentence1 = f\"The {word1} is here\"\n",
    "            sentence2 = f\"The {word2} is here\"\n",
    "            \n",
    "            # Tokenize and get representations\n",
    "            ids1 = tokenizer.encode(sentence1)\n",
    "            ids2 = tokenizer.encode(sentence2)\n",
    "            \n",
    "            logits1, _ = model.forward(np.array([ids1]))\n",
    "            logits2, _ = model.forward(np.array([ids2]))\n",
    "            \n",
    "            # Find word positions (simplified - assumes single token)\n",
    "            # In practice, you'd need more sophisticated alignment\n",
    "            if len(ids1) > 2 and len(ids2) > 2:\n",
    "                rep1 = logits1[0, 1]  # Assume word is at position 1\n",
    "                rep2 = logits2[0, 1]\n",
    "                \n",
    "                # Cosine similarity\n",
    "                similarity = np.dot(rep1, rep2) / (np.linalg.norm(rep1) * np.linalg.norm(rep2))\n",
    "                similarities.append(similarity)\n",
    "                \n",
    "                print(f\"  {word1} - {word2}: {similarity:.3f}\")\n",
    "        \n",
    "        if similarities:\n",
    "            print(f\"  Average similarity: {np.mean(similarities):.3f}\")\n",
    "\n",
    "# Semantic word pairs\n",
    "word_pairs = {\n",
    "    \"synonyms\": [(\"big\", \"large\"), (\"small\", \"tiny\"), (\"happy\", \"glad\")],\n",
    "    \"antonyms\": [(\"hot\", \"cold\"), (\"big\", \"small\"), (\"good\", \"bad\")],\n",
    "    \"related\": [(\"dog\", \"cat\"), (\"car\", \"truck\"), (\"book\", \"read\")],\n",
    "    \"unrelated\": [(\"dog\", \"computer\"), (\"happy\", \"table\"), (\"car\", \"music\")]\n",
    "}\n",
    "\n",
    "semantic_similarity_probe(model, tokenizer, word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Analysis\n",
    "\n",
    "Analyzing computational performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_benchmark(model, tokenizer, test_cases):\n",
    "    \"\"\"\n",
    "    Benchmark model performance across different scenarios.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for test_name, test_data in test_cases.items():\n",
    "        print(f\"\\nTesting: {test_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        times = []\n",
    "        memory_usage = []\n",
    "        \n",
    "        for text in test_data:\n",
    "            # Tokenize\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            input_batch = np.array([token_ids])\n",
    "            \n",
    "            # Time the forward pass\n",
    "            start_time = time.time()\n",
    "            logits, cache = model.forward(input_batch)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time\n",
    "            times.append(inference_time)\n",
    "            \n",
    "            # Estimate memory usage (rough approximation)\n",
    "            memory_estimate = logits.nbytes + sum(\n",
    "                v.nbytes for v in cache.values() if isinstance(v, np.ndarray)\n",
    "            )\n",
    "            memory_usage.append(memory_estimate)\n",
    "        \n",
    "        # Compute statistics\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        avg_memory = np.mean(memory_usage)\n",
    "        \n",
    "        # Tokens per second\n",
    "        total_tokens = sum(len(tokenizer.encode(text)) for text in test_data)\n",
    "        tokens_per_second = total_tokens / sum(times)\n",
    "        \n",
    "        results[test_name] = {\n",
    "            'avg_time_ms': avg_time * 1000,\n",
    "            'std_time_ms': std_time * 1000,\n",
    "            'avg_memory_mb': avg_memory / (1024 * 1024),\n",
    "            'tokens_per_second': tokens_per_second,\n",
    "            'num_examples': len(test_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Examples: {len(test_data)}\")\n",
    "        print(f\"  Avg time: {avg_time*1000:.2f} ± {std_time*1000:.2f} ms\")\n",
    "        print(f\"  Avg memory: {avg_memory/(1024*1024):.2f} MB\")\n",
    "        print(f\"  Throughput: {tokens_per_second:.1f} tokens/sec\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create performance test cases\n",
    "performance_tests = {\n",
    "    \"Short sentences\": [\n",
    "        \"The cat sat.\",\n",
    "        \"She runs fast.\",\n",
    "        \"Dogs bark loudly.\",\n",
    "        \"Birds fly high.\",\n",
    "        \"Cars drive slowly.\"\n",
    "    ],\n",
    "    \"Medium sentences\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning models require large amounts of training data.\",\n",
    "        \"The weather today is sunny and warm with clear skies.\",\n",
    "        \"She finished her homework before dinner and watched television.\",\n",
    "        \"The library has many interesting books to read and study.\"\n",
    "    ],\n",
    "    \"Long sentences\": [\n",
    "        \"The artificial intelligence system demonstrated remarkable capabilities in natural language understanding and generation tasks across multiple domains.\",\n",
    "        \"Recent advances in deep learning have revolutionized computer vision, natural language processing, and many other fields of artificial intelligence research.\",\n",
    "        \"The comprehensive evaluation showed that the model achieved state-of-the-art performance on several benchmark datasets while maintaining computational efficiency.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = performance_benchmark(model, tokenizer, performance_tests)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "test_names = list(benchmark_results.keys())\n",
    "avg_times = [benchmark_results[name]['avg_time_ms'] for name in test_names]\n",
    "avg_memory = [benchmark_results[name]['avg_memory_mb'] for name in test_names]\n",
    "throughput = [benchmark_results[name]['tokens_per_second'] for name in test_names]\n",
    "\n",
    "# Inference time\n",
    "bars1 = axes[0].bar(test_names, avg_times, color='skyblue')\n",
    "axes[0].set_ylabel('Time (ms)')\n",
    "axes[0].set_title('Average Inference Time')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars1, avg_times):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{time_val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Memory usage\n",
    "bars2 = axes[1].bar(test_names, avg_memory, color='lightcoral')\n",
    "axes[1].set_ylabel('Memory (MB)')\n",
    "axes[1].set_title('Average Memory Usage')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, mem_val in zip(bars2, avg_memory):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{mem_val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Throughput\n",
    "bars3 = axes[2].bar(test_names, throughput, color='lightgreen')\n",
    "axes[2].set_ylabel('Tokens/Second')\n",
    "axes[2].set_title('Throughput')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, throughput_val in zip(bars3, throughput):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{throughput_val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"• Longer sentences take more time (linear scaling)\")\n",
    "print(\"• Memory usage scales with sequence length\")\n",
    "print(\"• Throughput varies with sequence complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Analysis and Limitations\n",
    "\n",
    "Understanding what the model does well and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_behavior(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Analyze model behavior on various linguistic phenomena.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL BEHAVIOR ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test cases for different linguistic phenomena\n",
    "    test_cases = {\n",
    "        \"Simple Prediction\": [\n",
    "            \"The cat sat on the [MASK].\",\n",
    "            \"I drink [MASK] in the morning.\",\n",
    "            \"The [MASK] is yellow.\"\n",
    "        ],\n",
    "        \"Grammar\": [\n",
    "            \"She [MASK] to school yesterday.\",  # verb tense\n",
    "            \"The [MASK] are flying.\",           # plural agreement\n",
    "            \"He [MASK] very tall.\",             # is/am/are\n",
    "        ],\n",
    "        \"World Knowledge\": [\n",
    "            \"The capital of France is [MASK].\",\n",
    "            \"Shakespeare wrote [MASK].\",\n",
    "            \"The largest planet is [MASK].\"\n",
    "        ],\n",
    "        \"Context Dependency\": [\n",
    "            \"The man went to the bank to [MASK] money.\",        # financial context\n",
    "            \"The river bank was covered with [MASK].\",          # geographical context\n",
    "            \"She couldn't see the movie because [MASK] was tall.\" # pronoun reference\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for category, examples in test_cases.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        category_results = []\n",
    "        \n",
    "        for example in examples:\n",
    "            print(f\"\\nInput: {example}\")\n",
    "            \n",
    "            # Tokenize\n",
    "            token_ids = tokenizer.encode(example)\n",
    "            input_batch = np.array([token_ids])\n",
    "            \n",
    "            # Find mask position\n",
    "            mask_token_id = tokenizer.vocab['[MASK]']\n",
    "            mask_positions = [i for i, token_id in enumerate(token_ids) if token_id == mask_token_id]\n",
    "            \n",
    "            if mask_positions:\n",
    "                # Forward pass\n",
    "                logits, _ = model.forward(input_batch)\n",
    "                probabilities = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "                probabilities = probabilities / np.sum(probabilities, axis=-1, keepdims=True)\n",
    "                \n",
    "                # Get top predictions\n",
    "                pos = mask_positions[0]\n",
    "                pos_probs = probabilities[0, pos]\n",
    "                top_indices = np.argsort(pos_probs)[-3:][::-1]  # Top 3\n",
    "                \n",
    "                predictions = []\n",
    "                for idx in top_indices:\n",
    "                    try:\n",
    "                        token = tokenizer.decode([idx]).strip()\n",
    "                        prob = pos_probs[idx]\n",
    "                        predictions.append((token, prob))\n",
    "                    except:\n",
    "                        predictions.append((f'UNK_{idx}', pos_probs[idx]))\n",
    "                \n",
    "                print(\"Predictions:\")\n",
    "                for i, (token, prob) in enumerate(predictions):\n",
    "                    print(f\"  {i+1}. '{token}' ({prob*100:.1f}%)\")\n",
    "                \n",
    "                category_results.append({\n",
    "                    'input': example,\n",
    "                    'predictions': predictions\n",
    "                })\n",
    "        \n",
    "        analysis_results[category] = category_results\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Run analysis\n",
    "behavior_analysis = analyze_model_behavior(model, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModel Capabilities Observed:\")\n",
    "print(\"• Basic word prediction from context\")\n",
    "print(\"• Some grammatical awareness (limited without training)\")\n",
    "print(\"• Context-sensitive predictions\")\n",
    "\n",
    "print(\"\\nModel Limitations:\")\n",
    "print(\"• Limited world knowledge (untrained model)\")\n",
    "print(\"• May struggle with complex reasoning\")\n",
    "print(\"• Predictions based on statistical patterns only\")\n",
    "print(\"• No real understanding of meaning\")\n",
    "\n",
    "print(\"\\nNote: This is an untrained/randomly initialized model.\")\n",
    "print(\"With proper training on large corpora, performance would improve significantly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation Best Practices\n",
    "\n",
    "Guidelines for proper model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_checklist():\n",
    "    \"\"\"\n",
    "    Display evaluation best practices.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EVALUATION BEST PRACTICES CHECKLIST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    practices = {\n",
    "        \"Data Preparation\": [\n",
    "            \"Use held-out test data (never seen during training)\",\n",
    "            \"Ensure test data is representative of intended use\",\n",
    "            \"Balance test set across different domains/topics\",\n",
    "            \"Document data collection and preprocessing steps\"\n",
    "        ],\n",
    "        \"Evaluation Metrics\": [\n",
    "            \"Use multiple complementary metrics\",\n",
    "            \"Report confidence intervals where possible\",\n",
    "            \"Include both automatic and human evaluation\",\n",
    "            \"Consider task-specific metrics\"\n",
    "        ],\n",
    "        \"Experimental Setup\": [\n",
    "            \"Fix random seeds for reproducibility\",\n",
    "            \"Run multiple evaluation rounds\",\n",
    "            \"Compare against meaningful baselines\",\n",
    "            \"Document hyperparameters and model configuration\"\n",
    "        ],\n",
    "        \"Analysis\": [\n",
    "            \"Analyze failure cases and error patterns\",\n",
    "            \"Test on edge cases and challenging examples\",\n",
    "            \"Evaluate computational efficiency\",\n",
    "            \"Consider ethical implications and biases\"\n",
    "        ],\n",
    "        \"Reporting\": [\n",
    "            \"Report both strengths and limitations\",\n",
    "            \"Include statistical significance tests\",\n",
    "            \"Provide examples of model outputs\",\n",
    "            \"Make evaluation code and data available\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        for item in items:\n",
    "            print(f\"  ✓ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMMON EVALUATION PITFALLS TO AVOID\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pitfalls = [\n",
    "        \"Training on test data (data leakage)\",\n",
    "        \"Overfitting hyperparameters to test set\",\n",
    "        \"Using only a single metric\",\n",
    "        \"Ignoring computational costs\",\n",
    "        \"Not testing on diverse examples\",\n",
    "        \"Cherry-picking results\",\n",
    "        \"Inadequate baseline comparisons\",\n",
    "        \"Not reporting confidence intervals\",\n",
    "        \"Ignoring model biases and fairness\",\n",
    "        \"Not validating evaluation metrics themselves\"\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  ❌ {pitfall}\")\n",
    "\n",
    "# Display checklist\n",
    "evaluation_checklist()\n",
    "\n",
    "# Summary of metrics used in this notebook\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METRICS USED IN THIS NOTEBOOK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Accuracy Metrics\": {\n",
    "        \"Top-k Accuracy\": \"Percentage of times true token is in top-k predictions\",\n",
    "        \"Exact Match\": \"Percentage of exactly correct predictions\"\n",
    "    },\n",
    "    \"Loss Metrics\": {\n",
    "        \"Cross-entropy Loss\": \"Standard loss for classification tasks\",\n",
    "        \"Perplexity\": \"Exponential of cross-entropy, measures uncertainty\"\n",
    "    },\n",
    "    \"Performance Metrics\": {\n",
    "        \"Inference Time\": \"Time to process input and generate output\",\n",
    "        \"Memory Usage\": \"RAM required during inference\",\n",
    "        \"Throughput\": \"Tokens processed per second\"\n",
    "    },\n",
    "    \"Linguistic Metrics\": {\n",
    "        \"POS Accuracy\": \"Ability to distinguish part-of-speech categories\",\n",
    "        \"Semantic Similarity\": \"Cosine similarity between word representations\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, metrics in metrics_summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric, description in metrics.items():\n",
    "        print(f\"  • {metric}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Evaluation Concepts\n",
    "\n",
    "### **1. Types of Evaluation**\n",
    "- **Intrinsic**: MLM accuracy, perplexity\n",
    "- **Extrinsic**: Downstream task performance\n",
    "- **Probing**: Test specific linguistic knowledge\n",
    "- **Human**: Qualitative assessment by people\n",
    "\n",
    "### **2. Essential Metrics**\n",
    "- **Accuracy**: How often predictions are correct\n",
    "- **Perplexity**: Measure of model uncertainty\n",
    "- **Efficiency**: Speed and memory usage\n",
    "- **Robustness**: Performance on edge cases\n",
    "\n",
    "### **3. Evaluation Process**\n",
    "1. **Prepare**: Clean, representative test data\n",
    "2. **Measure**: Multiple complementary metrics\n",
    "3. **Analyze**: Error patterns and limitations\n",
    "4. **Compare**: Against baselines and humans\n",
    "5. **Report**: Transparent, complete results\n",
    "\n",
    "### **4. Key Insights**\n",
    "- No single metric tells the full story\n",
    "- Performance varies across different tasks\n",
    "- Computational efficiency matters in practice\n",
    "- Understanding failures is as important as successes\n",
    "\n",
    "### **5. Next Steps**\n",
    "- Train model on real data for better performance\n",
    "- Implement more sophisticated evaluation metrics\n",
    "- Test on standardized benchmarks\n",
    "- Conduct human evaluation studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Custom Metrics**: Implement a metric to measure how well the model handles negation (\"not good\" vs \"good\").\n",
    "\n",
    "2. **Bias Analysis**: Test the model for gender, racial, or other biases in its predictions.\n",
    "\n",
    "3. **Domain Transfer**: Evaluate how well the model performs on different domains (medical, legal, technical).\n",
    "\n",
    "4. **Multilingual**: Test the model's behavior on non-English text if your tokenizer supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}