{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO): Simplified Alignment Without Reward Models\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "Direct Preference Optimization (DPO) is a groundbreaking approach that revolutionized how we align language models with human preferences. It eliminates the need for explicit reward models in RLHF, directly optimizing the policy using preference data.\n",
    "\n",
    "**Key Innovation**: Treats the alignment problem as a classification task on preference pairs, bypassing the complex RLHF pipeline.\n",
    "\n",
    "**Impact**: Widespread adoption for alignment, simplifying the training process while achieving comparable or better results than RLHF.\n",
    "\n",
    "## üìö Background & Motivation\n",
    "\n",
    "### The RLHF Complexity Problem\n",
    "Traditional RLHF requires:\n",
    "1. **Supervised Fine-tuning (SFT)** on demonstrations\n",
    "2. **Reward Model training** on preference data  \n",
    "3. **Reinforcement Learning** with the reward model\n",
    "4. **Complex optimization** with PPO/TRPO\n",
    "\n",
    "### The DPO Solution\n",
    "- **Direct optimization** on preference data\n",
    "- **No reward model** needed\n",
    "- **Simpler training** with standard supervised learning\n",
    "- **Theoretical grounding** via Bandit Problem formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, List\n",
    "import math\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundation\n",
    "\n",
    "### DPO Core Mathematics\n",
    "\n",
    "DPO is based on the insight that the optimal policy œÄ* for RLHF has a closed form:\n",
    "\n",
    "**œÄ*(y|x) = 1/Z(x) √ó œÄ_ref(y|x) √ó exp(r*(x,y) / Œ≤)**\n",
    "\n",
    "Where:\n",
    "- **œÄ_ref**: Reference model (SFT model)\n",
    "- **r***: Optimal reward function\n",
    "- **Œ≤**: Temperature parameter\n",
    "- **Z(x)**: Partition function\n",
    "\n",
    "### Key Insight: Reparameterization\n",
    "\n",
    "We can solve for the reward function:\n",
    "\n",
    "**r*(x,y) = Œ≤ √ó log(œÄ*(y|x) / œÄ_ref(y|x)) + Œ≤ √ó log Z(x)**\n",
    "\n",
    "### DPO Loss Function\n",
    "\n",
    "For preference pairs (y_w, y_l) where y_w ‚âª y_l:\n",
    "\n",
    "**L_DPO = -log œÉ(Œ≤ √ó log(œÄ_Œ∏(y_w|x) / œÄ_ref(y_w|x)) - Œ≤ √ó log(œÄ_Œ∏(y_l|x) / œÄ_ref(y_l|x)))**\n",
    "\n",
    "Where œÉ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Direct Preference Optimization Loss.\n",
    "    \n",
    "    Implements the DPO loss function that directly optimizes preferences\n",
    "    without requiring a separate reward model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.1, label_smoothing: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.Tensor,\n",
    "        policy_rejected_logps: torch.Tensor, \n",
    "        reference_chosen_logps: torch.Tensor,\n",
    "        reference_rejected_logps: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute DPO loss.\n",
    "        \n",
    "        Args:\n",
    "            policy_chosen_logps: Log probabilities of chosen responses under policy\n",
    "            policy_rejected_logps: Log probabilities of rejected responses under policy\n",
    "            reference_chosen_logps: Log probabilities of chosen responses under reference\n",
    "            reference_rejected_logps: Log probabilities of rejected responses under reference\n",
    "        \n",
    "        Returns:\n",
    "            loss: DPO loss\n",
    "            chosen_rewards: Implicit rewards for chosen responses\n",
    "            rejected_rewards: Implicit rewards for rejected responses\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute implicit rewards\n",
    "        policy_chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "        policy_rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "        \n",
    "        # DPO loss: -log sigmoid(beta * (log(pi_chosen/pi_ref_chosen) - log(pi_rejected/pi_ref_rejected)))\n",
    "        logits = policy_chosen_rewards - policy_rejected_rewards\n",
    "        \n",
    "        if self.label_smoothing == 0.0:\n",
    "            loss = -F.logsigmoid(logits)\n",
    "        else:\n",
    "            # Label smoothing: mix with uniform distribution\n",
    "            loss = -F.logsigmoid(logits) * (1 - self.label_smoothing) - F.logsigmoid(-logits) * self.label_smoothing\n",
    "        \n",
    "        return loss.mean(), policy_chosen_rewards.mean(), policy_rejected_rewards.mean()\n",
    "    \n",
    "    def get_implicit_reward(self, policy_logps: torch.Tensor, reference_logps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute implicit reward from log probabilities.\n",
    "        \"\"\"\n",
    "        return self.beta * (policy_logps - reference_logps)\n",
    "\n",
    "\n",
    "def compute_log_probabilities(model, input_ids, attention_mask, labels):\n",
    "    \"\"\"\n",
    "    Compute log probabilities for a sequence under a model.\n",
    "    \n",
    "    This is a simplified version - in practice, you'd use the actual model's\n",
    "    forward pass and compute log probabilities for the labels.\n",
    "    \"\"\"\n",
    "    # Simplified simulation of log probability computation\n",
    "    batch_size, seq_len = labels.shape\n",
    "    \n",
    "    # Simulate model logits (in practice, this comes from model.forward())\n",
    "    vocab_size = 32000\n",
    "    logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Gather log probabilities for the actual labels\n",
    "    gathered_log_probs = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Mask out padding tokens and sum\n",
    "    mask = (labels != -100).float()  # Assuming -100 is the ignore index\n",
    "    sequence_log_prob = (gathered_log_probs * mask).sum(dim=-1)\n",
    "    \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "# Test DPO loss computation\n",
    "def test_dpo_loss():\n",
    "    \"\"\"\n",
    "    Test the DPO loss computation with synthetic data.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing DPO Loss Computation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create synthetic preference data\n",
    "    batch_size = 8\n",
    "    \n",
    "    # Simulate log probabilities\n",
    "    # Policy model gives higher probability to chosen responses\n",
    "    policy_chosen_logps = torch.randn(batch_size) + 1.0  # Higher values\n",
    "    policy_rejected_logps = torch.randn(batch_size) - 1.0  # Lower values\n",
    "    \n",
    "    # Reference model is neutral\n",
    "    reference_chosen_logps = torch.randn(batch_size)\n",
    "    reference_rejected_logps = torch.randn(batch_size)\n",
    "    \n",
    "    # Test different beta values\n",
    "    betas = [0.01, 0.1, 0.5, 1.0]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for beta in betas:\n",
    "        dpo_loss_fn = DPOLoss(beta=beta)\n",
    "        \n",
    "        loss, chosen_rewards, rejected_rewards = dpo_loss_fn(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps\n",
    "        )\n",
    "        \n",
    "        reward_margin = chosen_rewards - rejected_rewards\n",
    "        \n",
    "        results.append({\n",
    "            'beta': beta,\n",
    "            'loss': loss.item(),\n",
    "            'chosen_reward': chosen_rewards.item(),\n",
    "            'rejected_reward': rejected_rewards.item(),\n",
    "            'reward_margin': reward_margin.item()\n",
    "        })\n",
    "        \n",
    "        print(f\"Beta {beta:4.2f}: Loss={loss:.4f}, Margin={reward_margin:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "dpo_test_results = test_dpo_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è DPO Training Implementation\n",
    "\n",
    "Let's implement a complete DPO training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for preference pairs.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - prompt: Input text\n",
    "    - chosen: Preferred response\n",
    "    - rejected: Less preferred response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preferences: List[Dict]):\n",
    "        self.preferences = preferences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preferences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.preferences[idx]\n",
    "\n",
    "\n",
    "class DPOTrainer:\n",
    "    \"\"\"\n",
    "    DPO Trainer for preference optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        tokenizer,\n",
    "        beta: float = 0.1,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        self.policy_model = policy_model\n",
    "        self.reference_model = reference_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.beta = beta\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.reference_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dpo_loss = DPOLoss(beta=beta)\n",
    "    \n",
    "    def compute_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Compute DPO loss for a batch of preference pairs.\n",
    "        \"\"\"\n",
    "        # Extract prompts and responses\n",
    "        prompts = batch['prompt']\n",
    "        chosen_responses = batch['chosen']\n",
    "        rejected_responses = batch['rejected']\n",
    "        \n",
    "        # Simulate tokenization and model forward pass\n",
    "        batch_size = len(prompts)\n",
    "        \n",
    "        # In practice, you would:\n",
    "        # 1. Tokenize prompts + responses\n",
    "        # 2. Run forward pass through both models\n",
    "        # 3. Compute log probabilities for the response tokens\n",
    "        \n",
    "        # Simulate log probabilities\n",
    "        policy_chosen_logps = torch.randn(batch_size)\n",
    "        policy_rejected_logps = torch.randn(batch_size)\n",
    "        reference_chosen_logps = torch.randn(batch_size)\n",
    "        reference_rejected_logps = torch.randn(batch_size)\n",
    "        \n",
    "        # Compute DPO loss\n",
    "        loss, chosen_rewards, rejected_rewards = self.dpo_loss(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'chosen_rewards': chosen_rewards,\n",
    "            'rejected_rewards': rejected_rewards,\n",
    "            'reward_margin': chosen_rewards - rejected_rewards,\n",
    "            'policy_chosen_logps': policy_chosen_logps.mean(),\n",
    "            'policy_rejected_logps': policy_rejected_logps.mean()\n",
    "        }\n",
    "    \n",
    "    def train_step(self, batch, optimizer):\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_dict = self.compute_loss(batch)\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "# Simulate DPO training\n",
    "def simulate_dpo_training():\n",
    "    \"\"\"\n",
    "    Simulate a DPO training process with synthetic data.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Simulating DPO Training\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create synthetic preference data\n",
    "    preferences = []\n",
    "    for i in range(100):\n",
    "        preferences.append({\n",
    "            'prompt': f\"Question {i}: What is the capital of France?\",\n",
    "            'chosen': \"The capital of France is Paris.\",\n",
    "            'rejected': \"I think it might be Lyon or something.\"\n",
    "        })\n",
    "    \n",
    "    dataset = PreferenceDataset(preferences)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Simulate models (in practice, these would be actual transformer models)\n",
    "    class MockModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(10, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    policy_model = MockModel()\n",
    "    reference_model = MockModel()\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = DPOTrainer(\n",
    "        policy_model=policy_model,\n",
    "        reference_model=reference_model,\n",
    "        tokenizer=None,  # Would be actual tokenizer\n",
    "        beta=0.1\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    training_stats = []\n",
    "    num_epochs = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_margins = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            loss_dict = trainer.train_step(batch, optimizer)\n",
    "            \n",
    "            epoch_losses.append(loss_dict['loss'].item())\n",
    "            epoch_margins.append(loss_dict['reward_margin'].item())\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_margin = np.mean(epoch_margins)\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_loss,\n",
    "            'reward_margin': avg_margin\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Margin={avg_margin:.4f}\")\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "# Run simulation\n",
    "training_stats = simulate_dpo_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä DPO vs RLHF Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dpo_vs_rlhf():\n",
    "    \"\"\"\n",
    "    Compare DPO and RLHF approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Comparison metrics\n",
    "    metrics = {\n",
    "        'Training Steps': {'RLHF': 3, 'DPO': 1},\n",
    "        'Models Required': {'RLHF': 4, 'DPO': 2},  # Policy, Reference, Reward, Critic vs Policy, Reference\n",
    "        'Hyperparameters': {'RLHF': 15, 'DPO': 3},  # Approximate complexity\n",
    "        'Training Stability': {'RLHF': 6, 'DPO': 9},  # Subjective score 1-10\n",
    "        'Implementation Complexity': {'RLHF': 8, 'DPO': 3},  # Subjective score 1-10\n",
    "        'Memory Usage': {'RLHF': 8, 'DPO': 5},  # Relative scale\n",
    "        'Training Time': {'RLHF': 10, 'DPO': 4},  # Relative scale\n",
    "    }\n",
    "    \n",
    "    # Theoretical analysis of beta parameter\n",
    "    def analyze_beta_parameter():\n",
    "        betas = np.logspace(-3, 0, 20)  # 0.001 to 1.0\n",
    "        \n",
    "        # Simulate how different betas affect the reward margin\n",
    "        logp_diff = 2.0  # Policy gives 2.0 higher log prob to chosen vs rejected\n",
    "        reward_margins = betas * logp_diff\n",
    "        \n",
    "        # Simulate preference probability (how often model prefers chosen)\n",
    "        preference_probs = torch.sigmoid(torch.tensor(reward_margins)).numpy()\n",
    "        \n",
    "        return betas, reward_margins, preference_probs\n",
    "    \n",
    "    betas, margins, prefs = analyze_beta_parameter()\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Training pipeline comparison\n",
    "    ax1 = plt.subplot(2, 4, 1)\n",
    "    methods = list(metrics.keys())\n",
    "    rlhf_values = [metrics[m]['RLHF'] for m in methods]\n",
    "    dpo_values = [metrics[m]['DPO'] for m in methods]\n",
    "    \n",
    "    x_pos = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, rlhf_values, width, label='RLHF', alpha=0.8, color='red')\n",
    "    bars2 = ax1.bar(x_pos + width/2, dpo_values, width, label='DPO', alpha=0.8, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Metrics')\n",
    "    ax1.set_ylabel('Complexity/Resource Score')\n",
    "    ax1.set_title('RLHF vs DPO Comparison')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. Beta parameter analysis\n",
    "    ax2 = plt.subplot(2, 4, 2)\n",
    "    ax2.semilogx(betas, margins, 'o-', linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Beta Parameter')\n",
    "    ax2.set_ylabel('Reward Margin')\n",
    "    ax2.set_title('Beta vs Reward Margin')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Preference probability\n",
    "    ax3 = plt.subplot(2, 4, 3)\n",
    "    ax3.semilogx(betas, prefs, 'o-', color='green', linewidth=2, markersize=6)\n",
    "    ax3.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random')\n",
    "    ax3.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='Strong Preference')\n",
    "    ax3.set_xlabel('Beta Parameter')\n",
    "    ax3.set_ylabel('Preference Probability')\n",
    "    ax3.set_title('Beta vs Preference Strength')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    # 4. Training dynamics simulation\n",
    "    ax4 = plt.subplot(2, 4, 4)\n",
    "    epochs = np.arange(1, 21)\n",
    "    \n",
    "    # Simulate RLHF vs DPO training curves\n",
    "    rlhf_curve = 1.0 - 0.8 * np.exp(-epochs/8) + 0.1 * np.sin(epochs) * np.exp(-epochs/10)\n",
    "    dpo_curve = 1.0 - 0.9 * np.exp(-epochs/5)\n",
    "    \n",
    "    ax4.plot(epochs, rlhf_curve, 'o-', label='RLHF', linewidth=2, alpha=0.8)\n",
    "    ax4.plot(epochs, dpo_curve, 's-', label='DPO', linewidth=2, alpha=0.8)\n",
    "    ax4.set_xlabel('Training Epochs')\n",
    "    ax4.set_ylabel('Alignment Score')\n",
    "    ax4.set_title('Training Dynamics')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Loss landscape visualization\n",
    "    ax5 = plt.subplot(2, 4, 5)\n",
    "    \n",
    "    # Simulate loss vs log probability ratio\n",
    "    logp_ratios = np.linspace(-3, 3, 100)\n",
    "    dpo_losses = -np.log(1 / (1 + np.exp(-logp_ratios)))\n",
    "    \n",
    "    ax5.plot(logp_ratios, dpo_losses, linewidth=3)\n",
    "    ax5.set_xlabel('Log P(chosen) - Log P(rejected)')\n",
    "    ax5.set_ylabel('DPO Loss')\n",
    "    ax5.set_title('DPO Loss Landscape')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Equal Preference')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # 6. Computational complexity\n",
    "    ax6 = plt.subplot(2, 4, 6)\n",
    "    \n",
    "    stages = ['SFT', 'Reward\\nModel', 'RL\\nTraining', 'Total']\n",
    "    rlhf_times = [1, 1, 3, 5]  # Relative time units\n",
    "    dpo_times = [1, 0, 1, 2]   # DPO skips reward model, simpler RL\n",
    "    \n",
    "    x_pos = np.arange(len(stages))\n",
    "    \n",
    "    bars1 = ax6.bar(x_pos - width/2, rlhf_times, width, label='RLHF', alpha=0.8, color='red')\n",
    "    bars2 = ax6.bar(x_pos + width/2, dpo_times, width, label='DPO', alpha=0.8, color='blue')\n",
    "    \n",
    "    ax6.set_xlabel('Training Stage')\n",
    "    ax6.set_ylabel('Relative Time')\n",
    "    ax6.set_title('Training Time Breakdown')\n",
    "    ax6.set_xticks(x_pos)\n",
    "    ax6.set_xticklabels(stages)\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 7. Performance comparison\n",
    "    ax7 = plt.subplot(2, 4, 7)\n",
    "    \n",
    "    tasks = ['Helpfulness', 'Harmlessness', 'Honesty', 'Overall']\n",
    "    rlhf_scores = [85, 92, 78, 85]\n",
    "    dpo_scores = [87, 89, 82, 86]  # DPO often matches or exceeds RLHF\n",
    "    \n",
    "    x_pos = np.arange(len(tasks))\n",
    "    \n",
    "    bars1 = ax7.bar(x_pos - width/2, rlhf_scores, width, label='RLHF', alpha=0.8, color='red')\n",
    "    bars2 = ax7.bar(x_pos + width/2, dpo_scores, width, label='DPO', alpha=0.8, color='blue')\n",
    "    \n",
    "    ax7.set_xlabel('Evaluation Metric')\n",
    "    ax7.set_ylabel('Score')\n",
    "    ax7.set_title('Performance Comparison')\n",
    "    ax7.set_xticks(x_pos)\n",
    "    ax7.set_xticklabels(tasks)\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "    ax7.set_ylim(70, 95)\n",
    "    \n",
    "    # 8. Memory usage over time\n",
    "    ax8 = plt.subplot(2, 4, 8)\n",
    "    \n",
    "    time_steps = np.arange(1, 11)\n",
    "    rlhf_memory = [2, 4, 6, 8, 8, 8, 8, 8, 8, 8]  # Ramps up, stays high\n",
    "    dpo_memory = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3]   # Stays constant\n",
    "    \n",
    "    ax8.plot(time_steps, rlhf_memory, 'o-', label='RLHF', linewidth=2, markersize=6)\n",
    "    ax8.plot(time_steps, dpo_memory, 's-', label='DPO', linewidth=2, markersize=6)\n",
    "    ax8.set_xlabel('Training Progress')\n",
    "    ax8.set_ylabel('Memory Usage (GB)')\n",
    "    ax8.set_title('Memory Usage During Training')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, betas, margins, prefs\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_dpo_vs_rlhf()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä DPO vs RLHF Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"DPO Advantages:\")\n",
    "print(\"  ‚úÖ Simpler training pipeline (1 step vs 3)\")\n",
    "print(\"  ‚úÖ No reward model needed\")\n",
    "print(\"  ‚úÖ More stable training\")\n",
    "print(\"  ‚úÖ Lower memory requirements\")\n",
    "print(\"  ‚úÖ Faster training time\")\n",
    "print(\"  ‚úÖ Easier to implement\")\n",
    "print(\"\\nRLHF Advantages:\")\n",
    "print(\"  ‚úÖ More interpretable rewards\")\n",
    "print(\"  ‚úÖ Can handle complex preferences\")\n",
    "print(\"  ‚úÖ Established track record\")\n",
    "print(\"\\nWhen to use DPO:\")\n",
    "print(\"  ‚Ä¢ Simple preference tasks\")\n",
    "print(\"  ‚Ä¢ Limited computational resources\")\n",
    "print(\"  ‚Ä¢ Quick iteration cycles\")\n",
    "print(\"  ‚Ä¢ Stable training requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced DPO Variants\n",
    "\n",
    "Several improvements and variants of DPO have been developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustDPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Robust DPO (R-DPO) that handles distribution shift.\n",
    "    \n",
    "    Addresses the sensitivity of standard DPO to preference distribution changes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.1, eta: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.eta = eta  # Robustness parameter\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.Tensor,\n",
    "        policy_rejected_logps: torch.Tensor,\n",
    "        reference_chosen_logps: torch.Tensor,\n",
    "        reference_rejected_logps: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute Robust DPO loss.\n",
    "        \"\"\"\n",
    "        # Standard DPO terms\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "        \n",
    "        # Robust regularization term\n",
    "        logits = chosen_rewards - rejected_rewards\n",
    "        \n",
    "        # Add robustness penalty\n",
    "        robust_penalty = self.eta * (chosen_rewards.pow(2) + rejected_rewards.pow(2))\n",
    "        \n",
    "        # Combined loss\n",
    "        dpo_loss = -F.logsigmoid(logits)\n",
    "        total_loss = dpo_loss + robust_penalty\n",
    "        \n",
    "        return total_loss.mean(), logits.mean()\n",
    "\n",
    "\n",
    "class KLDPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    KL-regularized DPO that maintains closer alignment with reference model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.1, kl_weight: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.kl_weight = kl_weight\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.Tensor,\n",
    "        policy_rejected_logps: torch.Tensor,\n",
    "        reference_chosen_logps: torch.Tensor,\n",
    "        reference_rejected_logps: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute KL-regularized DPO loss.\n",
    "        \"\"\"\n",
    "        # Standard DPO computation\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "        \n",
    "        logits = chosen_rewards - rejected_rewards\n",
    "        dpo_loss = -F.logsigmoid(logits)\n",
    "        \n",
    "        # KL divergence penalty\n",
    "        kl_chosen = policy_chosen_logps - reference_chosen_logps\n",
    "        kl_rejected = policy_rejected_logps - reference_rejected_logps\n",
    "        kl_penalty = self.kl_weight * (kl_chosen.pow(2) + kl_rejected.pow(2))\n",
    "        \n",
    "        total_loss = dpo_loss + kl_penalty\n",
    "        \n",
    "        return total_loss.mean(), dpo_loss.mean(), kl_penalty.mean()\n",
    "\n",
    "\n",
    "def compare_dpo_variants():\n",
    "    \"\"\"\n",
    "    Compare different DPO variants.\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Comparing DPO Variants\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create test data\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Simulate preference data with some noise\n",
    "    policy_chosen_logps = torch.randn(batch_size) + 0.5\n",
    "    policy_rejected_logps = torch.randn(batch_size) - 0.5\n",
    "    reference_chosen_logps = torch.randn(batch_size)\n",
    "    reference_rejected_logps = torch.randn(batch_size)\n",
    "    \n",
    "    # Initialize different loss functions\n",
    "    standard_dpo = DPOLoss(beta=0.1)\n",
    "    robust_dpo = RobustDPOLoss(beta=0.1, eta=0.05)\n",
    "    kl_dpo = KLDPOLoss(beta=0.1, kl_weight=0.01)\n",
    "    \n",
    "    # Compute losses\n",
    "    standard_loss, standard_chosen, standard_rejected = standard_dpo(\n",
    "        policy_chosen_logps, policy_rejected_logps,\n",
    "        reference_chosen_logps, reference_rejected_logps\n",
    "    )\n",
    "    \n",
    "    robust_loss, robust_logits = robust_dpo(\n",
    "        policy_chosen_logps, policy_rejected_logps,\n",
    "        reference_chosen_logps, reference_rejected_logps\n",
    "    )\n",
    "    \n",
    "    kl_total_loss, kl_dpo_loss, kl_penalty = kl_dpo(\n",
    "        policy_chosen_logps, policy_rejected_logps,\n",
    "        reference_chosen_logps, reference_rejected_logps\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    results = {\n",
    "        'Standard DPO': {\n",
    "            'loss': standard_loss.item(),\n",
    "            'reward_margin': (standard_chosen - standard_rejected).item(),\n",
    "            'properties': 'Simple, stable, widely used'\n",
    "        },\n",
    "        'Robust DPO': {\n",
    "            'loss': robust_loss.item(),\n",
    "            'reward_margin': robust_logits.item(),\n",
    "            'properties': 'Handles distribution shift, more robust'\n",
    "        },\n",
    "        'KL-DPO': {\n",
    "            'loss': kl_total_loss.item(),\n",
    "            'dpo_component': kl_dpo_loss.item(),\n",
    "            'kl_component': kl_penalty.item(),\n",
    "            'properties': 'Maintains reference alignment, conservative'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nVariant Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    for variant, metrics in results.items():\n",
    "        print(f\"\\n{variant}:\")\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare variants\n",
    "variant_results = compare_dpo_variants()\n",
    "\n",
    "# Visualize beta sensitivity\n",
    "def analyze_beta_sensitivity():\n",
    "    \"\"\"\n",
    "    Analyze how different DPO variants respond to beta changes.\n",
    "    \"\"\"\n",
    "    betas = np.logspace(-2, 0, 15)  # 0.01 to 1.0\n",
    "    \n",
    "    standard_losses = []\n",
    "    robust_losses = []\n",
    "    kl_losses = []\n",
    "    \n",
    "    # Fixed test data\n",
    "    policy_chosen = torch.tensor([1.0])\n",
    "    policy_rejected = torch.tensor([-1.0])\n",
    "    ref_chosen = torch.tensor([0.0])\n",
    "    ref_rejected = torch.tensor([0.0])\n",
    "    \n",
    "    for beta in betas:\n",
    "        # Standard DPO\n",
    "        standard_dpo = DPOLoss(beta=beta)\n",
    "        loss_std, _, _ = standard_dpo(policy_chosen, policy_rejected, ref_chosen, ref_rejected)\n",
    "        standard_losses.append(loss_std.item())\n",
    "        \n",
    "        # Robust DPO\n",
    "        robust_dpo = RobustDPOLoss(beta=beta, eta=0.05)\n",
    "        loss_rob, _ = robust_dpo(policy_chosen, policy_rejected, ref_chosen, ref_rejected)\n",
    "        robust_losses.append(loss_rob.item())\n",
    "        \n",
    "        # KL-DPO\n",
    "        kl_dpo = KLDPOLoss(beta=beta, kl_weight=0.01)\n",
    "        loss_kl, _, _ = kl_dpo(policy_chosen, policy_rejected, ref_chosen, ref_rejected)\n",
    "        kl_losses.append(loss_kl.item())\n",
    "    \n",
    "    # Plot sensitivity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(betas, standard_losses, 'o-', label='Standard DPO', linewidth=2)\n",
    "    plt.semilogx(betas, robust_losses, 's-', label='Robust DPO', linewidth=2)\n",
    "    plt.semilogx(betas, kl_losses, '^-', label='KL-DPO', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Beta Parameter')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('DPO Variant Sensitivity to Beta')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return betas, standard_losses, robust_losses, kl_losses\n",
    "\n",
    "# Analyze sensitivity\n",
    "sensitivity_results = analyze_beta_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practical Exercises\n",
    "\n",
    "### Exercise 1: Implement Your Own DPO Loss\n",
    "Create a custom DPO loss with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_custom_dpo_loss():\n",
    "    \"\"\"\n",
    "    Exercise: Implement a custom DPO loss function.\n",
    "    \n",
    "    Your task: Implement a DPO loss that includes:\n",
    "    1. Standard DPO loss\n",
    "    2. Length normalization\n",
    "    3. Confidence weighting\n",
    "    4. Temperature scaling\n",
    "    \"\"\"\n",
    "    print(\"üß™ Exercise: Custom DPO Loss Implementation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    class CustomDPOLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Custom DPO loss with additional features.\n",
    "        \n",
    "        TODO: Complete the implementation with the features described above.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(\n",
    "            self, \n",
    "            beta: float = 0.1,\n",
    "            temperature: float = 1.0,\n",
    "            length_normalization: bool = True,\n",
    "            confidence_weighting: bool = True\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.beta = beta\n",
    "            self.temperature = temperature\n",
    "            self.length_normalization = length_normalization\n",
    "            self.confidence_weighting = confidence_weighting\n",
    "        \n",
    "        def forward(\n",
    "            self,\n",
    "            policy_chosen_logps: torch.Tensor,\n",
    "            policy_rejected_logps: torch.Tensor,\n",
    "            reference_chosen_logps: torch.Tensor,\n",
    "            reference_rejected_logps: torch.Tensor,\n",
    "            chosen_lengths: torch.Tensor = None,\n",
    "            rejected_lengths: torch.Tensor = None,\n",
    "            confidence_scores: torch.Tensor = None\n",
    "        ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "            \"\"\"\n",
    "            Compute custom DPO loss.\n",
    "            \n",
    "            TODO: Implement the following features:\n",
    "            \n",
    "            1. Length normalization: Divide log probabilities by sequence length\n",
    "            2. Confidence weighting: Weight loss by confidence scores\n",
    "            3. Temperature scaling: Scale logits by temperature\n",
    "            4. Return detailed metrics\n",
    "            \"\"\"\n",
    "            \n",
    "            # 1. Length normalization\n",
    "            if self.length_normalization and chosen_lengths is not None:\n",
    "                policy_chosen_logps = policy_chosen_logps / chosen_lengths\n",
    "                policy_rejected_logps = policy_rejected_logps / rejected_lengths\n",
    "                reference_chosen_logps = reference_chosen_logps / chosen_lengths\n",
    "                reference_rejected_logps = reference_rejected_logps / rejected_lengths\n",
    "            \n",
    "            # 2. Compute rewards\n",
    "            chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "            rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "            \n",
    "            # 3. Temperature scaling\n",
    "            logits = (chosen_rewards - rejected_rewards) / self.temperature\n",
    "            \n",
    "            # 4. Base DPO loss\n",
    "            dpo_loss = -F.logsigmoid(logits)\n",
    "            \n",
    "            # 5. Confidence weighting\n",
    "            if self.confidence_weighting and confidence_scores is not None:\n",
    "                dpo_loss = dpo_loss * confidence_scores\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = {\n",
    "                'loss': dpo_loss.mean(),\n",
    "                'chosen_rewards': chosen_rewards.mean(),\n",
    "                'rejected_rewards': rejected_rewards.mean(),\n",
    "                'reward_margin': (chosen_rewards - rejected_rewards).mean(),\n",
    "                'accuracy': (logits > 0).float().mean()\n",
    "            }\n",
    "            \n",
    "            return dpo_loss.mean(), metrics\n",
    "    \n",
    "    # Test the custom loss\n",
    "    batch_size = 8\n",
    "    \n",
    "    # Create test data\n",
    "    policy_chosen_logps = torch.randn(batch_size) + 1.0\n",
    "    policy_rejected_logps = torch.randn(batch_size) - 1.0\n",
    "    reference_chosen_logps = torch.randn(batch_size)\n",
    "    reference_rejected_logps = torch.randn(batch_size)\n",
    "    \n",
    "    # Additional features\n",
    "    chosen_lengths = torch.randint(10, 50, (batch_size,)).float()\n",
    "    rejected_lengths = torch.randint(10, 50, (batch_size,)).float()\n",
    "    confidence_scores = torch.rand(batch_size)  # Random confidence [0, 1]\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        {'name': 'Standard', 'length_normalization': False, 'confidence_weighting': False},\n",
    "        {'name': 'Length Normalized', 'length_normalization': True, 'confidence_weighting': False},\n",
    "        {'name': 'Confidence Weighted', 'length_normalization': False, 'confidence_weighting': True},\n",
    "        {'name': 'Full Custom', 'length_normalization': True, 'confidence_weighting': True}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        custom_loss = CustomDPOLoss(\n",
    "            beta=0.1,\n",
    "            temperature=1.0,\n",
    "            length_normalization=config['length_normalization'],\n",
    "            confidence_weighting=config['confidence_weighting']\n",
    "        )\n",
    "        \n",
    "        loss, metrics = custom_loss(\n",
    "            policy_chosen_logps, policy_rejected_logps,\n",
    "            reference_chosen_logps, reference_rejected_logps,\n",
    "            chosen_lengths, rejected_lengths, confidence_scores\n",
    "        )\n",
    "        \n",
    "        result = {'config': config['name'], **{k: v.item() for k, v in metrics.items()}}\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n{config['name']} Configuration:\")\n",
    "        print(f\"  Loss: {loss.item():.4f}\")\n",
    "        print(f\"  Reward Margin: {metrics['reward_margin'].item():.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy'].item():.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the exercise\n",
    "exercise_results = exercise_custom_dpo_loss()\n",
    "\n",
    "# Visualize results\n",
    "if exercise_results:\n",
    "    configs = [r['config'] for r in exercise_results]\n",
    "    losses = [r['loss'] for r in exercise_results]\n",
    "    margins = [r['reward_margin'] for r in exercise_results]\n",
    "    accuracies = [r['accuracy'] for r in exercise_results]\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Loss comparison\n",
    "    ax1.bar(configs, losses, alpha=0.7, color='skyblue')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss by Configuration')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Reward margin comparison\n",
    "    ax2.bar(configs, margins, alpha=0.7, color='lightgreen')\n",
    "    ax2.set_ylabel('Reward Margin')\n",
    "    ax2.set_title('Reward Margin by Configuration')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ax3.bar(configs, accuracies, alpha=0.7, color='orange')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Accuracy by Configuration')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Takeaways\n",
    "\n",
    "### DPO Advantages:\n",
    "1. **Simplicity**: Single-stage training vs multi-stage RLHF\n",
    "2. **Stability**: More stable than RL-based approaches\n",
    "3. **Efficiency**: Faster training and lower memory usage\n",
    "4. **Performance**: Often matches or exceeds RLHF results\n",
    "5. **Implementation**: Easier to implement and debug\n",
    "\n",
    "### Core Insights:\n",
    "1. **Reward Reparameterization**: Clever mathematical insight eliminates reward model\n",
    "2. **Classification Framing**: Treats preferences as binary classification\n",
    "3. **Direct Optimization**: Optimizes policy directly on preference data\n",
    "4. **Theoretical Foundation**: Grounded in bandit problem theory\n",
    "\n",
    "### Best Practices:\n",
    "1. **Beta Selection**: Start with Œ≤ = 0.1, adjust based on preference strength\n",
    "2. **Data Quality**: High-quality preference data is crucial\n",
    "3. **Reference Model**: Use well-trained SFT model as reference\n",
    "4. **Evaluation**: Monitor both loss and implicit reward margins\n",
    "5. **Variants**: Consider robust variants for challenging scenarios\n",
    "\n",
    "### When to Use DPO:\n",
    "- **Resource Constraints**: When computational resources are limited\n",
    "- **Quick Iteration**: When you need fast experimentation cycles\n",
    "- **Stable Training**: When training stability is important\n",
    "- **Simple Preferences**: When preference data is straightforward\n",
    "\n",
    "### Limitations:\n",
    "1. **Preference Complexity**: May struggle with complex multi-dimensional preferences\n",
    "2. **Distribution Shift**: Sensitive to changes in preference distribution\n",
    "3. **Reward Interpretability**: Implicit rewards are less interpretable\n",
    "4. **Fine-grained Control**: Less control over specific reward components\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Try Variants**: Experiment with Robust DPO, KL-DPO, and other variants\n",
    "2. **Real Data**: Apply DPO to actual preference datasets\n",
    "3. **Integration**: Combine with other techniques like LoRA for efficiency\n",
    "4. **Evaluation**: Develop better metrics for preference alignment\n",
    "5. **Research**: Explore recent improvements like IPO, ORPO, and others\n",
    "\n",
    "**DPO has revolutionized alignment by showing that complex RLHF pipelines aren't always necessary. Its simplicity and effectiveness make it an essential tool for modern language model alignment!** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}