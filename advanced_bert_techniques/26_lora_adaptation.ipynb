{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA: Low-Rank Adaptation of Large Language Models\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a revolutionary parameter-efficient fine-tuning technique that has transformed how we adapt large language models. Instead of updating all model parameters, LoRA injects trainable low-rank matrices into transformer layers while keeping the original weights frozen.\n",
    "\n",
    "**Key Innovation**: Decomposes weight updates into low-rank matrices A and B, reducing trainable parameters by up to 10,000√ó while maintaining performance.\n",
    "\n",
    "**Impact**: Universal adoption in HuggingFace PEFT library, enabling fine-tuning of massive models on consumer hardware.\n",
    "\n",
    "## üìö Background & Motivation\n",
    "\n",
    "### The Problem\n",
    "- Full fine-tuning requires updating all model parameters\n",
    "- Memory requirements scale linearly with model size\n",
    "- Storing separate copies for each task becomes prohibitive\n",
    "- Training large models requires expensive hardware\n",
    "\n",
    "### The LoRA Solution\n",
    "- Hypothesis: Weight updates during adaptation have low \"intrinsic rank\"\n",
    "- Decompose weight updates ŒîW into low-rank matrices: ŒîW = BA\n",
    "- Only train A and B matrices, freeze original weights\n",
    "- Merge weights during inference: W_new = W_original + BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundation\n",
    "\n",
    "### Core LoRA Mathematics\n",
    "\n",
    "For a pre-trained weight matrix W‚ÇÄ ‚àà ‚Ñù·µàÀ£·µè, LoRA represents the weight update as:\n",
    "\n",
    "**W = W‚ÇÄ + ŒîW = W‚ÇÄ + BA**\n",
    "\n",
    "Where:\n",
    "- **B ‚àà ‚Ñù·µàÀ£ ≥**: Down-projection matrix (trainable)\n",
    "- **A ‚àà ‚Ñù ≥À£·µè**: Up-projection matrix (trainable)  \n",
    "- **r**: Rank (much smaller than d, k)\n",
    "- **W‚ÇÄ**: Original frozen weights\n",
    "\n",
    "### Parameter Reduction\n",
    "- **Original parameters**: d √ó k\n",
    "- **LoRA parameters**: r √ó (d + k)\n",
    "- **Reduction ratio**: (d √ó k) / (r √ó (d + k))\n",
    "\n",
    "For typical values (d=4096, k=4096, r=16):\n",
    "- Original: 16,777,216 parameters\n",
    "- LoRA: 131,072 parameters  \n",
    "- **Reduction: 128√ó**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) layer implementation.\n",
    "    \n",
    "    This layer adds trainable low-rank matrices to a frozen linear layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 16,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Store original layer (frozen)\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.requires_grad_(False)\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize LoRA weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        print(f\"‚úÖ LoRA layer created:\")\n",
    "        print(f\"   Original params: {in_features * out_features:,}\")\n",
    "        print(f\"   LoRA params: {rank * (in_features + out_features):,}\")\n",
    "        print(f\"   Reduction: {(in_features * out_features) / (rank * (in_features + out_features)):.1f}x\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original forward pass (frozen)\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA forward pass\n",
    "        lora_output = self.lora_B(self.lora_A(self.dropout(x)))\n",
    "        \n",
    "        # Combine with scaling\n",
    "        return original_output + lora_output * self.scaling\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into original layer for inference.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Compute LoRA weight update\n",
    "            lora_weight = self.lora_B.weight @ self.lora_A.weight\n",
    "            \n",
    "            # Add to original weights\n",
    "            self.original_layer.weight.add_(lora_weight * self.scaling)\n",
    "            \n",
    "            # Zero out LoRA weights\n",
    "            self.lora_A.weight.zero_()\n",
    "            self.lora_B.weight.zero_()\n",
    "    \n",
    "    def get_parameter_count(self):\n",
    "        \"\"\"Get parameter counts for analysis.\"\"\"\n",
    "        original_params = sum(p.numel() for p in self.original_layer.parameters())\n",
    "        lora_params = sum(p.numel() for p in [self.lora_A.weight, self.lora_B.weight])\n",
    "        return {\n",
    "            'original': original_params,\n",
    "            'lora': lora_params,\n",
    "            'total': original_params + lora_params,\n",
    "            'reduction': original_params / lora_params\n",
    "        }\n",
    "\n",
    "# Test the LoRA layer\n",
    "original_linear = nn.Linear(1024, 1024)\n",
    "lora_layer = LoRALayer(original_linear, rank=16, alpha=16)\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(32, 1024)\n",
    "output = lora_layer(x)\n",
    "print(f\"\\nüîÑ Forward pass successful: {output.shape}\")\n",
    "\n",
    "# Analyze parameters\n",
    "params = lora_layer.get_parameter_count()\n",
    "print(f\"\\nüìä Parameter Analysis:\")\n",
    "for key, value in params.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {key}: {value:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è LoRA Attention Implementation\n",
    "\n",
    "The most common application of LoRA is in attention layers, where we apply it to the query, key, value, and output projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with LoRA adaptation.\n",
    "    \n",
    "    Applies LoRA to query, key, value, and output projections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        lora_rank: int = 16,\n",
    "        lora_alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Original attention layers (frozen)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Apply LoRA to projections\n",
    "        self.q_lora = LoRALayer(self.q_proj, lora_rank, lora_alpha, dropout)\n",
    "        self.k_lora = LoRALayer(self.k_proj, lora_rank, lora_alpha, dropout)\n",
    "        self.v_lora = LoRALayer(self.v_proj, lora_rank, lora_alpha, dropout)\n",
    "        self.out_lora = LoRALayer(self.out_proj, lora_rank, lora_alpha, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Apply LoRA-enhanced projections\n",
    "        q = self.q_lora(x)\n",
    "        k = self.k_lora(x)\n",
    "        v = self.v_lora(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and apply output projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, embed_dim\n",
    "        )\n",
    "        \n",
    "        output = self.out_lora(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Get count of trainable vs total parameters.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total': total_params,\n",
    "            'trainable': trainable_params,\n",
    "            'frozen': total_params - trainable_params,\n",
    "            'efficiency': trainable_params / total_params\n",
    "        }\n",
    "\n",
    "# Test LoRA attention\n",
    "lora_attention = LoRAMultiHeadAttention(\n",
    "    embed_dim=768,\n",
    "    num_heads=12,\n",
    "    lora_rank=16\n",
    ")\n",
    "\n",
    "# Test with sample input\n",
    "x = torch.randn(4, 128, 768)  # batch_size, seq_len, embed_dim\n",
    "output, weights = lora_attention(x)\n",
    "\n",
    "print(f\"\\nüîÑ LoRA Attention Test:\")\n",
    "print(f\"   Input shape: {x.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Attention weights shape: {weights.shape}\")\n",
    "\n",
    "# Analyze parameter efficiency\n",
    "param_stats = lora_attention.get_trainable_parameters()\n",
    "print(f\"\\nüìä Parameter Efficiency:\")\n",
    "for key, value in param_stats.items():\n",
    "    if key == 'efficiency':\n",
    "        print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä LoRA Rank Analysis\n",
    "\n",
    "The rank r is a crucial hyperparameter that controls the trade-off between parameter efficiency and model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lora_rank_efficiency():\n",
    "    \"\"\"\n",
    "    Analyze the trade-off between rank and parameter efficiency.\n",
    "    \"\"\"\n",
    "    embed_dim = 768\n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    \n",
    "    results = {\n",
    "        'rank': [],\n",
    "        'lora_params': [],\n",
    "        'total_params': [],\n",
    "        'efficiency': [],\n",
    "        'reduction_ratio': []\n",
    "    }\n",
    "    \n",
    "    # Original linear layer\n",
    "    original_params = embed_dim * embed_dim\n",
    "    \n",
    "    for rank in ranks:\n",
    "        # LoRA parameters: r * (d + k)\n",
    "        lora_params = rank * (embed_dim + embed_dim)\n",
    "        total_params = original_params + lora_params\n",
    "        efficiency = lora_params / total_params\n",
    "        reduction_ratio = original_params / lora_params\n",
    "        \n",
    "        results['rank'].append(rank)\n",
    "        results['lora_params'].append(lora_params)\n",
    "        results['total_params'].append(total_params)\n",
    "        results['efficiency'].append(efficiency)\n",
    "        results['reduction_ratio'].append(reduction_ratio)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze rank efficiency\n",
    "rank_analysis = analyze_lora_rank_efficiency()\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Parameter count vs rank\n",
    "ax1.plot(rank_analysis['rank'], rank_analysis['lora_params'], 'o-', label='LoRA params', linewidth=2)\n",
    "ax1.axhline(y=768*768, color='r', linestyle='--', label='Original params')\n",
    "ax1.set_xlabel('LoRA Rank')\n",
    "ax1.set_ylabel('Parameters')\n",
    "ax1.set_title('Parameter Count vs LoRA Rank')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# 2. Reduction ratio vs rank\n",
    "ax2.plot(rank_analysis['rank'], rank_analysis['reduction_ratio'], 'o-', color='green', linewidth=2)\n",
    "ax2.set_xlabel('LoRA Rank')\n",
    "ax2.set_ylabel('Parameter Reduction Ratio')\n",
    "ax2.set_title('Parameter Reduction vs LoRA Rank')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# 3. Efficiency percentage\n",
    "efficiency_pct = [e * 100 for e in rank_analysis['efficiency']]\n",
    "bars = ax3.bar(range(len(rank_analysis['rank'])), efficiency_pct, color='orange', alpha=0.7)\n",
    "ax3.set_xlabel('LoRA Rank')\n",
    "ax3.set_ylabel('Trainable Parameters (%)')\n",
    "ax3.set_title('Training Efficiency by Rank')\n",
    "ax3.set_xticks(range(len(rank_analysis['rank'])))\n",
    "ax3.set_xticklabels(rank_analysis['rank'])\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height:.2f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Memory usage comparison\n",
    "rank_subset = [1, 4, 16, 64]\n",
    "memory_original = [768*768*4] * len(rank_subset)  # 4 bytes per float32\n",
    "memory_lora = [rank * (768 + 768) * 4 for rank in rank_subset]\n",
    "\n",
    "x_pos = np.arange(len(rank_subset))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x_pos - width/2, [m/1e6 for m in memory_original], width, \n",
    "        label='Original', color='red', alpha=0.7)\n",
    "ax4.bar(x_pos + width/2, [m/1e6 for m in memory_lora], width, \n",
    "        label='LoRA', color='blue', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('LoRA Rank')\n",
    "ax4.set_ylabel('Memory Usage (MB)')\n",
    "ax4.set_title('Memory Usage Comparison')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(rank_subset)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nüìä LoRA Rank Analysis Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<6} {'LoRA Params':<12} {'Reduction':<12} {'Efficiency':<12} {'Memory (MB)':<12}\")\n",
    "print(\"=\" * 80)\n",
    "for i, rank in enumerate(rank_analysis['rank']):\n",
    "    if rank in [1, 4, 8, 16, 32, 64]:\n",
    "        lora_params = rank_analysis['lora_params'][i]\n",
    "        reduction = rank_analysis['reduction_ratio'][i]\n",
    "        efficiency = rank_analysis['efficiency'][i] * 100\n",
    "        memory_mb = lora_params * 4 / 1e6  # 4 bytes per float32\n",
    "        print(f\"{rank:<6} {lora_params:<12,} {reduction:<12.1f}x {efficiency:<12.2f}% {memory_mb:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practical LoRA Training Example\n",
    "\n",
    "Let's implement a complete example showing how to use LoRA for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified BERT-like model for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30000, embed_dim=768, num_heads=12, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(512, embed_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=embed_dim * 4,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(embed_dim, 2)  # Binary classification\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_len = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        pos_embeds = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeds + pos_embeds\n",
    "        \n",
    "        # Transformer layers\n",
    "        hidden_states = embeddings\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Classification (use [CLS] token)\n",
    "        cls_hidden = hidden_states[:, 0]  # First token\n",
    "        logits = self.classifier(cls_hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def apply_lora_to_model(model, lora_rank=16, lora_alpha=16):\n",
    "    \"\"\"\n",
    "    Apply LoRA to all linear layers in the model.\n",
    "    \"\"\"\n",
    "    lora_layers = []\n",
    "    \n",
    "    def apply_lora_recursive(module, name=\"\"):\n",
    "        for child_name, child in module.named_children():\n",
    "            full_name = f\"{name}.{child_name}\" if name else child_name\n",
    "            \n",
    "            if isinstance(child, nn.Linear):\n",
    "                # Skip embedding and final classifier layers\n",
    "                if 'embeddings' not in full_name and 'classifier' not in full_name:\n",
    "                    # Replace with LoRA layer\n",
    "                    lora_layer = LoRALayer(child, lora_rank, lora_alpha)\n",
    "                    setattr(module, child_name, lora_layer)\n",
    "                    lora_layers.append((full_name, lora_layer))\n",
    "                    print(f\"‚úÖ Applied LoRA to: {full_name}\")\n",
    "            else:\n",
    "                apply_lora_recursive(child, full_name)\n",
    "    \n",
    "    apply_lora_recursive(model)\n",
    "    return lora_layers\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count trainable and total parameters.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'trainable': trainable_params,\n",
    "        'frozen': total_params - trainable_params,\n",
    "        'efficiency': trainable_params / total_params if total_params > 0 else 0\n",
    "    }\n",
    "\n",
    "# Create and analyze model\n",
    "print(\"ü§ñ Creating SimpleBERT model...\")\n",
    "model = SimpleBERT(vocab_size=1000, embed_dim=256, num_heads=8, num_layers=4)\n",
    "\n",
    "# Count parameters before LoRA\n",
    "params_before = count_parameters(model)\n",
    "print(f\"\\nüìä Parameters before LoRA:\")\n",
    "for key, value in params_before.items():\n",
    "    if key == 'efficiency':\n",
    "        print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")\n",
    "\n",
    "# Apply LoRA\n",
    "print(f\"\\nüîß Applying LoRA (rank=16)...\")\n",
    "lora_layers = apply_lora_to_model(model, lora_rank=16, lora_alpha=16)\n",
    "\n",
    "# Count parameters after LoRA\n",
    "params_after = count_parameters(model)\n",
    "print(f\"\\nüìä Parameters after LoRA:\")\n",
    "for key, value in params_after.items():\n",
    "    if key == 'efficiency':\n",
    "        print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")\n",
    "\n",
    "# Test the model\n",
    "batch_size, seq_len = 4, 128\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "attention_mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    \n",
    "print(f\"\\nüîÑ Model test successful:\")\n",
    "print(f\"   Input shape: {input_ids.shape}\")\n",
    "print(f\"   Output shape: {logits.shape}\")\n",
    "print(f\"   LoRA layers applied: {len(lora_layers)}\")\n",
    "\n",
    "# Calculate efficiency improvement\n",
    "reduction_factor = params_before['total'] / params_after['trainable']\n",
    "print(f\"\\nüöÄ LoRA Efficiency:\")\n",
    "print(f\"   Parameter reduction: {reduction_factor:.1f}x\")\n",
    "print(f\"   Training efficiency: {params_after['efficiency']*100:.2f}% parameters trainable\")\n",
    "print(f\"   Memory savings: ~{(1 - params_after['efficiency']) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced LoRA Techniques\n",
    "\n",
    "### 1. Adaptive Rank Selection\n",
    "Different layers may benefit from different ranks based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_importance():\n",
    "    \"\"\"\n",
    "    Simulate layer importance analysis for adaptive rank selection.\n",
    "    \"\"\"\n",
    "    # Simulated importance scores (in practice, computed from gradients/activations)\n",
    "    layer_names = [\n",
    "        'layers.0.self_attn.q_proj',\n",
    "        'layers.0.self_attn.k_proj', \n",
    "        'layers.0.self_attn.v_proj',\n",
    "        'layers.0.self_attn.out_proj',\n",
    "        'layers.0.linear1',\n",
    "        'layers.0.linear2',\n",
    "        'layers.1.self_attn.q_proj',\n",
    "        'layers.1.self_attn.k_proj',\n",
    "        'layers.1.self_attn.v_proj',\n",
    "        'layers.1.self_attn.out_proj',\n",
    "        'layers.1.linear1',\n",
    "        'layers.1.linear2',\n",
    "    ]\n",
    "    \n",
    "    # Simulate importance scores (higher = more important)\n",
    "    np.random.seed(42)\n",
    "    importance_scores = np.random.beta(2, 5, len(layer_names))  # Skewed towards lower values\n",
    "    \n",
    "    # Assign ranks based on importance\n",
    "    def assign_adaptive_rank(importance):\n",
    "        if importance > 0.7:\n",
    "            return 64\n",
    "        elif importance > 0.5:\n",
    "            return 32\n",
    "        elif importance > 0.3:\n",
    "            return 16\n",
    "        elif importance > 0.1:\n",
    "            return 8\n",
    "        else:\n",
    "            return 4\n",
    "    \n",
    "    adaptive_ranks = [assign_adaptive_rank(score) for score in importance_scores]\n",
    "    \n",
    "    # Compare with fixed rank\n",
    "    fixed_rank = 16\n",
    "    fixed_params = len(layer_names) * fixed_rank * (256 + 256)  # Assume 256-dim layers\n",
    "    adaptive_params = sum(rank * (256 + 256) for rank in adaptive_ranks)\n",
    "    \n",
    "    print(\"üß† Adaptive Rank Selection Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Layer':<30} {'Importance':<12} {'Rank':<6}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, importance, rank in zip(layer_names, importance_scores, adaptive_ranks):\n",
    "        print(f\"{name:<30} {importance:<12.3f} {rank:<6}\")\n",
    "    \n",
    "    print(\"\\nüìä Parameter Comparison:\")\n",
    "    print(f\"   Fixed rank ({fixed_rank}): {fixed_params:,} parameters\")\n",
    "    print(f\"   Adaptive ranks: {adaptive_params:,} parameters\")\n",
    "    print(f\"   Savings: {((fixed_params - adaptive_params) / fixed_params * 100):.1f}%\")\n",
    "    \n",
    "    return layer_names, importance_scores, adaptive_ranks\n",
    "\n",
    "# Visualize adaptive rank selection\n",
    "layer_names, importance_scores, adaptive_ranks = analyze_layer_importance()\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Importance scores\n",
    "colors = plt.cm.viridis(importance_scores)\n",
    "bars1 = ax1.bar(range(len(layer_names)), importance_scores, color=colors)\n",
    "ax1.set_xlabel('Layer Index')\n",
    "ax1.set_ylabel('Importance Score')\n",
    "ax1.set_title('Layer Importance Scores')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=min(importance_scores), vmax=max(importance_scores)))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax1, shrink=0.8)\n",
    "cbar.set_label('Importance Score')\n",
    "\n",
    "# 2. Adaptive ranks\n",
    "rank_colors = ['red' if r == 4 else 'orange' if r == 8 else 'yellow' if r == 16 else 'lightgreen' if r == 32 else 'green' for r in adaptive_ranks]\n",
    "bars2 = ax2.bar(range(len(layer_names)), adaptive_ranks, color=rank_colors, alpha=0.7)\n",
    "ax2.set_xlabel('Layer Index')\n",
    "ax2.set_ylabel('LoRA Rank')\n",
    "ax2.set_title('Adaptive Rank Assignment')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add rank labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practical Exercises\n",
    "\n",
    "### Exercise 1: LoRA Rank Experiment\n",
    "Implement and compare different LoRA ranks on a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_rank_experiment():\n",
    "    \"\"\"\n",
    "    Compare different LoRA ranks on a synthetic task.\n",
    "    \"\"\"\n",
    "    print(\"üß™ LoRA Rank Experiment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create synthetic data\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(1000, 256)\n",
    "    y = (X.sum(dim=1) > 0).float()  # Simple binary classification\n",
    "    \n",
    "    ranks_to_test = [1, 4, 8, 16, 32, 64]\n",
    "    results = []\n",
    "    \n",
    "    for rank in ranks_to_test:\n",
    "        print(f\"\\nüîÑ Testing rank {rank}...\")\n",
    "        \n",
    "        # Create model with LoRA\n",
    "        original_layer = nn.Linear(256, 1)\n",
    "        lora_layer = LoRALayer(original_layer, rank=rank, alpha=rank)\n",
    "        \n",
    "        # Simple training loop\n",
    "        optimizer = torch.optim.Adam(lora_layer.parameters(), lr=0.01)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            logits = lora_layer(X).squeeze()\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # Evaluate\n",
    "        with torch.no_grad():\n",
    "            logits = lora_layer(X).squeeze()\n",
    "            predictions = (torch.sigmoid(logits) > 0.5).float()\n",
    "            accuracy = (predictions == y).float().mean().item()\n",
    "        \n",
    "        param_count = lora_layer.get_parameter_count()\n",
    "        \n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'final_loss': losses[-1],\n",
    "            'accuracy': accuracy,\n",
    "            'lora_params': param_count['lora'],\n",
    "            'reduction': param_count['reduction']\n",
    "        })\n",
    "        \n",
    "        print(f\"   Final loss: {losses[-1]:.4f}\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   Parameters: {param_count['lora']:,} ({param_count['reduction']:.1f}x reduction)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "experiment_results = lora_rank_experiment()\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ranks = [r['rank'] for r in experiment_results]\n",
    "accuracies = [r['accuracy'] for r in experiment_results]\n",
    "losses = [r['final_loss'] for r in experiment_results]\n",
    "param_counts = [r['lora_params'] for r in experiment_results]\n",
    "\n",
    "# 1. Accuracy vs Rank\n",
    "ax1.plot(ranks, accuracies, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('LoRA Rank')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy vs LoRA Rank')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.5, 1.0])\n",
    "\n",
    "# 2. Loss vs Rank\n",
    "ax2.plot(ranks, losses, 'o-', color='red', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('LoRA Rank')\n",
    "ax2.set_ylabel('Final Loss')\n",
    "ax2.set_title('Final Loss vs LoRA Rank')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Parameter count vs performance\n",
    "scatter = ax3.scatter(param_counts, accuracies, c=ranks, s=100, cmap='viridis', alpha=0.7)\n",
    "ax3.set_xlabel('LoRA Parameters')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Accuracy vs Parameter Count')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax3, label='LoRA Rank')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Experiment Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<6} {'Accuracy':<10} {'Loss':<10} {'Params':<10} {'Reduction':<10}\")\n",
    "print(\"=\" * 70)\n",
    "for result in experiment_results:\n",
    "    print(f\"{result['rank']:<6} {result['accuracy']:<10.4f} {result['final_loss']:<10.4f} \"\n",
    "          f\"{result['lora_params']:<10,} {result['reduction']:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Takeaways\n",
    "\n",
    "### LoRA Advantages:\n",
    "1. **Extreme Parameter Efficiency**: 10,000√ó reduction in trainable parameters\n",
    "2. **Memory Efficiency**: Significant reduction in GPU memory requirements\n",
    "3. **Storage Efficiency**: Store multiple task-specific adaptations easily\n",
    "4. **Training Speed**: Faster training due to fewer parameters\n",
    "5. **Modularity**: Easy to combine multiple LoRA modules\n",
    "\n",
    "### Best Practices:\n",
    "1. **Rank Selection**: Start with rank 16, adjust based on task complexity\n",
    "2. **Alpha Scaling**: Use Œ± = rank for balanced scaling\n",
    "3. **Layer Selection**: Apply to attention layers first, then FFN layers\n",
    "4. **Initialization**: Zero-initialize B, random-initialize A\n",
    "5. **Dropout**: Use dropout on input to LoRA layers\n",
    "\n",
    "### When to Use LoRA:\n",
    "- **Large Model Fine-tuning**: When full fine-tuning is computationally prohibitive\n",
    "- **Multiple Tasks**: When you need task-specific adaptations\n",
    "- **Limited Resources**: When GPU memory or storage is constrained\n",
    "- **Quick Iteration**: When you need to experiment with many variations\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Explore QLoRA**: Combine LoRA with quantization for extreme efficiency\n",
    "2. **Try AdaLoRA**: Adaptive rank selection during training\n",
    "3. **Experiment with DoRA**: Direction-aware LoRA for improved performance\n",
    "4. **Study LoRA+**: Recent improvements to the LoRA technique\n",
    "5. **Apply to Real Tasks**: Use LoRA on actual NLP tasks with HuggingFace PEFT\n",
    "\n",
    "**LoRA has revolutionized how we approach large model adaptation, making it possible to fine-tune massive models efficiently and democratically. It's an essential technique for any modern NLP practitioner!** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}