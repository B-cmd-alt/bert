#!/usr/bin/env python3
"""
Full-Scale BERT Tokenizer Training Pipeline

Train a 50K vocabulary BERT tokenizer using complete Wikipedia, Books, and News datasets.
Optimized for 32GB RAM system with proper memory management and progress monitoring.
"""

import os
import sys
import time
import psutil
import logging
import gc
from datetime import datetime, timedelta
from pathlib import Path
import argparse
from typing import List, Tuple
import json

# Add the learning-examples directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'learning-examples'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'bert-wordpiece-tokenizer'))

from wordpiece_simple import SimpleWordPieceTokenizer

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_scale_training_log.txt'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FullScaleBERTTrainer:
    """Train large-scale BERT tokenizer with complete real-world datasets."""
    
    def __init__(self, output_dir: str = "large_bert_models"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.data_dir = self.output_dir / "full_data"
        self.data_dir.mkdir(exist_ok=True)
        
        # System specs and memory management (optimized for lower memory)
        self.total_ram_gb = 16  # Reduced assumption
        self.max_training_memory_gb = 8   # Reduced to 8GB max
        self.chunk_size_mb = 25  # Reduced chunk size for memory efficiency
        
        # Progress tracking
        self.start_time = time.time()
        
    def log_system_resources(self, stage: str = ""):
        """Log current system resource usage."""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        elapsed = time.time() - self.start_time
        elapsed_str = str(timedelta(seconds=int(elapsed)))
        
        logger.info(f"=== {stage} System Resources ===")
        logger.info(f"Elapsed Time: {elapsed_str}")
        logger.info(f"Memory: {memory.used/(1024**3):.1f}GB used / {memory.total/(1024**3):.1f}GB total ({memory.percent:.1f}%)")
        logger.info(f"Available: {memory.available/(1024**3):.1f}GB")
        logger.info(f"CPU: {cpu_percent:.1f}%")
        
        if memory.percent > 80:
            logger.warning("High memory usage - consider reducing batch size")
            gc.collect()  # Force garbage collection
    
    def download_full_wikipedia(self) -> str:
        """Download complete Wikipedia dataset using multiple strategies."""
        wiki_file = self.data_dir / "wikipedia_full.txt"
        
        if wiki_file.exists():
            file_size = os.path.getsize(wiki_file) / (1024 * 1024)  # MB
            if file_size > 100:  # If substantial file exists
                logger.info(f"Wikipedia dataset exists: {wiki_file} ({file_size:.1f} MB)")
                return str(wiki_file)
        
        logger.info("Downloading complete Wikipedia dataset...")
        
        try:
            import datasets
            
            # Try multiple Wikipedia dataset strategies
            strategies = [
                # Strategy 1: Use wikimedia/wikipedia
                lambda: datasets.load_dataset("wikimedia/wikipedia", "20231101.en", split="train", streaming=True),
                
                # Strategy 2: Use wikipedia dataset with different date
                lambda: datasets.load_dataset("wikipedia", "20220301.en", split="train", streaming=True, trust_remote_code=True),
                
                # Strategy 3: Use legacy wikipedia
                lambda: datasets.load_dataset("legacy-datasets/wikipedia", "20220301.en", split="train", streaming=True),
                
                # Strategy 4: Use different wikipedia config
                lambda: datasets.load_dataset("wikipedia", "20220301.simple", split="train", streaming=True, trust_remote_code=True)
            ]
            
            dataset = None
            for i, strategy in enumerate(strategies, 1):
                try:
                    logger.info(f"Trying Wikipedia strategy {i}...")
                    dataset = strategy()
                    logger.info(f"Successfully loaded Wikipedia with strategy {i}")
                    break
                except Exception as e:
                    logger.warning(f"Strategy {i} failed: {e}")
                    continue
            
            if dataset is None:
                logger.warning("All Wikipedia download strategies failed, using fallback...")
                return self._create_wikipedia_fallback(wiki_file)
            
            # Process Wikipedia data in chunks
            logger.info("Processing Wikipedia articles (this will take time)...")
            
            with open(wiki_file, "w", encoding="utf-8") as f:
                article_count = 0
                lines_written = 0
                target_articles = 500000  # Process up to 500k articles
                
                for article in dataset:
                    if article_count >= target_articles:
                        break
                    
                    text = article.get("text", "")
                    if len(text) > 200:  # Skip very short articles
                        # Clean and split text into sentences
                        sentences = text.replace("\n\n", "\n").split("\n")
                        for sentence in sentences:
                            sentence = sentence.strip()
                            if len(sentence) > 30:  # Meaningful sentences only
                                f.write(sentence + "\n")
                                lines_written += 1
                    
                    article_count += 1
                    
                    # Progress updates and memory management
                    if article_count % 10000 == 0:
                        file_size = os.path.getsize(wiki_file) / (1024 * 1024)
                        logger.info(f"Processed {article_count:,} articles, {lines_written:,} lines, {file_size:.1f} MB")
                        
                        # Memory management
                        if psutil.virtual_memory().percent > 85:
                            logger.info("High memory usage, forcing garbage collection...")
                            gc.collect()
            
            final_size = os.path.getsize(wiki_file) / (1024 * 1024)
            logger.info(f"Wikipedia dataset complete: {article_count:,} articles, {final_size:.1f} MB")
            return str(wiki_file)
            
        except Exception as e:
            logger.error(f"Wikipedia download failed: {e}")
            return self._create_wikipedia_fallback(wiki_file)
    
    def _create_wikipedia_fallback(self, wiki_file: Path) -> str:
        """Create substantial Wikipedia-style fallback content."""
        logger.info("Creating comprehensive Wikipedia fallback content...")
        
        # Comprehensive Wikipedia-style content across many domains
        wiki_topics = {
            "Science": [
                "Physics is the natural science that studies matter, its motion and behavior through space and time.",
                "Chemistry is the scientific discipline involved with elements and compounds composed of atoms, molecules and ions.",
                "Biology is the natural science that studies life and living organisms including their physical structure.",
                "Mathematics is the abstract science of number, quantity, and space studied in its own right.",
                "Astronomy is a natural science that studies celestial objects and phenomena in the universe."
            ],
            "Technology": [
                "Computer science is the study of algorithmic processes and computational systems and their design.",
                "Artificial intelligence is intelligence demonstrated by machines in contrast to natural intelligence.",
                "Machine learning is a method of data analysis that automates analytical model building using algorithms.",
                "Software engineering is the systematic approach to the design development and maintenance of software.",
                "Information technology is the use of computers to store retrieve transmit and manipulate data."
            ],
            "History": [
                "World history is the study of major civilizations over approximately five thousand years of human history.",
                "Ancient history is the aggregate of past events from the beginning of recorded human history.",
                "Medieval history is the history of Europe during the Middle Ages from the 5th to the 15th century.",
                "Modern history is the history of the world beginning after the Middle Ages generally around 1500.",
                "Contemporary history is the span of historic events from approximately 1945 to the present time."
            ],
            "Geography": [
                "Physical geography is the branch of geography dealing with natural features and processes.",
                "Human geography is the branch of geography that deals with the study of people and their communities.",
                "Economic geography is the subfield of geography that examines the spatial distribution of economic activities.",
                "Political geography is concerned with the study of both the spatially uneven outcomes of political processes.",
                "Cultural geography is a subfield within geography that studies cultural products and norms and their variations."
            ],
            "Literature": [
                "Literature is a body of written works including poetry drama fiction nonfiction and journalism.",
                "Poetry is a form of literature that uses aesthetic and rhythmic qualities of language to evoke meanings.",
                "Drama is a mode of fictional representation through dialogue and performance intended for theatrical performance.",
                "Fiction is the classification for any story or similar work derived from imagination rather than from history.",
                "Nonfiction is any document or media content that intends in good faith to present only truth and accuracy."
            ]
        }
        
        with open(wiki_file, "w", encoding="utf-8") as f:
            # Create substantial content by expanding each topic
            for topic, sentences in wiki_topics.items():
                for sentence in sentences:
                    # Write multiple variations of each sentence
                    for i in range(100):  # 100 variations per sentence
                        variations = [
                            sentence,
                            f"According to research, {sentence.lower()}",
                            f"Studies have shown that {sentence.lower()}",
                            f"It is widely recognized that {sentence.lower()}",
                            f"Academic consensus indicates that {sentence.lower()}",
                            f"Scientific evidence demonstrates that {sentence.lower()}"
                        ]
                        f.write(variations[i % len(variations)] + "\n")
        
        file_size = os.path.getsize(wiki_file) / (1024 * 1024)
        logger.info(f"Created Wikipedia fallback: {wiki_file} ({file_size:.1f} MB)")
        return str(wiki_file)
    
    def download_full_books(self) -> str:
        """Download complete Project Gutenberg books dataset."""
        books_file = self.data_dir / "books_full.txt"
        
        if books_file.exists():
            file_size = os.path.getsize(books_file) / (1024 * 1024)
            if file_size > 50:
                logger.info(f"Books dataset exists: {books_file} ({file_size:.1f} MB)")
                return str(books_file)
        
        logger.info("Downloading complete Project Gutenberg books...")
        
        try:
            import datasets
            
            # Try different Project Gutenberg configurations
            configs = ["en", "de", "fr", "es", "it"]  # Multiple languages for diversity
            
            with open(books_file, "w", encoding="utf-8") as f:
                total_books = 0
                lines_written = 0
                
                for config in configs:
                    try:
                        logger.info(f"Loading Project Gutenberg books in {config}...")
                        dataset = datasets.load_dataset("manu/project_gutenberg", config, split="train")
                        
                        book_count = 0
                        target_books = 5000 if config == "en" else 1000  # More English books
                        
                        for book in dataset:
                            if book_count >= target_books:
                                break
                            
                            text = book.get("text", "")
                            if len(text) > 1000:  # Skip very short texts
                                # Process book text into sentences
                                paragraphs = text.split("\n\n")
                                for paragraph in paragraphs:
                                    sentences = paragraph.replace("\n", " ").split(". ")
                                    for sentence in sentences:
                                        sentence = sentence.strip()
                                        if len(sentence) > 20:
                                            f.write(sentence + "\n")
                                            lines_written += 1
                            
                            book_count += 1
                            total_books += 1
                            
                            if book_count % 500 == 0:
                                file_size = os.path.getsize(books_file) / (1024 * 1024)
                                logger.info(f"Processed {book_count} {config} books, {file_size:.1f} MB")
                    
                    except Exception as e:
                        logger.warning(f"Failed to load {config} books: {e}")
                        continue
                
                final_size = os.path.getsize(books_file) / (1024 * 1024)
                logger.info(f"Books dataset complete: {total_books:,} books, {final_size:.1f} MB")
                return str(books_file)
                
        except Exception as e:
            logger.error(f"Books download failed: {e}")
            return self._create_books_fallback(books_file)
    
    def _create_books_fallback(self, books_file: Path) -> str:
        """Create substantial literary fallback content."""
        logger.info("Creating comprehensive literary fallback content...")
        
        # Famous literature excerpts and styles
        literary_content = [
            # Classic literature
            "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness.",
            "Call me Ishmael. Some years ago never mind how long precisely having little or no money in my purse.",
            "All happy families are alike each unhappy family is unhappy in its own way according to Tolstoy.",
            "To be or not to be that is the question whether tis nobler in the mind to suffer the slings and arrows.",
            "It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife.",
            
            # Different literary styles
            "The study of literature encompasses poetry prose drama and various forms of creative writing throughout history.",
            "Narrative techniques in fiction include point of view character development plot structure and thematic elements.",
            "Literary criticism analyzes interprets and evaluates literary works using various theoretical frameworks and methodologies.",
            "Comparative literature examines literary works across different cultures languages and historical periods to identify patterns.",
            "Creative writing involves the composition of original literary works including novels short stories poetry and screenplays.",
            
            # Philosophical content
            "Philosophy is the study of general and fundamental questions about existence knowledge values reason mind and language.",
            "Ethics is the branch of philosophy that involves systematizing defending and recommending concepts of right and wrong conduct.",
            "Metaphysics is a branch of philosophy that examines the fundamental nature of reality including the relationship between mind and matter.",
            "Epistemology is the study of knowledge and justified belief concerned with the nature of knowledge and how it relates to truth.",
            "Logic is the systematic study of the principles of valid inference and correct reasoning in philosophy and mathematics."
        ]
        
        with open(books_file, "w", encoding="utf-8") as f:
            # Create substantial literary content
            for sentence in literary_content:
                for i in range(500):  # 500 variations per sentence
                    variations = [
                        sentence,
                        f"In literary analysis, {sentence.lower()}",
                        f"Classic literature demonstrates that {sentence.lower()}",
                        f"According to literary scholars, {sentence.lower()}",
                        f"The great works of literature show us that {sentence.lower()}",
                        f"From a narrative perspective, {sentence.lower()}"
                    ]
                    f.write(variations[i % len(variations)] + "\n")
        
        file_size = os.path.getsize(books_file) / (1024 * 1024)
        logger.info(f"Created books fallback: {books_file} ({file_size:.1f} MB)")
        return str(books_file)
    
    def download_full_news(self) -> str:
        """Download complete news datasets from multiple sources."""
        news_file = self.data_dir / "news_full.txt"
        
        if news_file.exists():
            file_size = os.path.getsize(news_file) / (1024 * 1024)
            if file_size > 10:
                logger.info(f"News dataset exists: {news_file} ({file_size:.1f} MB)")
                return str(news_file)
        
        logger.info("Downloading complete news datasets...")
        
        try:
            import datasets
            
            # Multiple news datasets for diversity
            news_datasets = [
                ("SetFit/bbc-news", "train"),
                ("SetFit/bbc-news", "test"),
                ("Fraser/news-category-dataset", "train"),
                ("cc_news", None)  # Common Crawl news
            ]
            
            with open(news_file, "w", encoding="utf-8") as f:
                total_articles = 0
                lines_written = 0
                
                for dataset_name, split in news_datasets:
                    try:
                        logger.info(f"Loading {dataset_name}...")
                        
                        if split:
                            dataset = datasets.load_dataset(dataset_name, split=split)
                        else:
                            # For streaming datasets
                            dataset = datasets.load_dataset(dataset_name, split="train", streaming=True)
                            dataset = list(dataset.take(10000))  # Limit streaming datasets
                        
                        article_count = 0
                        for article in dataset:
                            text = article.get("text", "") or article.get("description", "") or str(article)
                            
                            if len(text) > 50:
                                # Process news text into sentences
                                sentences = text.replace("\n", " ").split(". ")
                                for sentence in sentences:
                                    sentence = sentence.strip()
                                    if len(sentence) > 15:
                                        f.write(sentence + "\n")
                                        lines_written += 1
                            
                            article_count += 1
                            total_articles += 1
                            
                            if article_count % 1000 == 0:
                                file_size = os.path.getsize(news_file) / (1024 * 1024)
                                logger.info(f"Processed {article_count} articles from {dataset_name}, {file_size:.1f} MB")
                    
                    except Exception as e:
                        logger.warning(f"Failed to load {dataset_name}: {e}")
                        continue
                
                final_size = os.path.getsize(news_file) / (1024 * 1024)
                logger.info(f"News dataset complete: {total_articles:,} articles, {final_size:.1f} MB")
                return str(news_file)
                
        except Exception as e:
            logger.error(f"News download failed: {e}")
            return self._create_news_fallback(news_file)
    
    def _create_news_fallback(self, news_file: Path) -> str:
        """Create substantial journalism fallback content."""
        logger.info("Creating comprehensive news fallback content...")
        
        # Diverse news content across different categories
        news_categories = {
            "Technology": [
                "Technology companies continue to invest heavily in artificial intelligence and machine learning research and development.",
                "Cybersecurity experts warn about increasing threats from sophisticated malware and ransomware attacks targeting businesses.",
                "Social media platforms implement new policies to combat misinformation and protect user privacy and data security.",
                "Semiconductor manufacturers face supply chain challenges affecting global electronics production and consumer prices.",
                "Cloud computing services expand rapidly as businesses migrate to digital infrastructure and remote work solutions."
            ],
            "Business": [
                "Financial markets respond to economic indicators showing mixed signals about inflation and employment rates.",
                "International trade agreements influence global supply chains and manufacturing strategies across industries.",
                "Startup companies raise significant venture capital funding for innovative products and services in emerging markets.",
                "Corporate earnings reports exceed analyst expectations despite ongoing economic uncertainty and market volatility.",
                "Merger and acquisition activity increases as companies seek strategic partnerships and market consolidation opportunities."
            ],
            "Science": [
                "Scientific researchers publish breakthrough studies on climate change impacts and potential mitigation strategies.",
                "Medical advances in gene therapy and personalized medicine offer new treatment options for rare diseases.",
                "Space exploration missions reveal new discoveries about planetary formation and the potential for extraterrestrial life.",
                "Environmental scientists develop innovative solutions for renewable energy generation and sustainable resource management.",
                "Neuroscience research provides insights into brain function and potential treatments for neurological disorders."
            ],
            "Politics": [
                "Government officials announce new policy initiatives aimed at addressing healthcare costs and accessibility issues.",
                "International diplomatic negotiations focus on trade relationships and security cooperation between nations.",
                "Election campaigns emphasize economic recovery plans and social justice reform proposals for voters.",
                "Legislative debates center on infrastructure investment and regulatory frameworks for emerging technologies.",
                "Political analysts examine polling data and demographic trends affecting electoral outcomes and representation."
            ],
            "Health": [
                "Public health authorities recommend updated vaccination schedules and preventive care measures for communities.",
                "Healthcare systems implement telemedicine solutions to improve patient access and reduce treatment costs.",
                "Medical research institutions collaborate on clinical trials for innovative therapies and diagnostic techniques.",
                "Health insurance policies adapt to changing healthcare needs and regulatory requirements in different states.",
                "Wellness programs in workplaces focus on mental health support and stress reduction strategies for employees."
            ]
        }
        
        with open(news_file, "w", encoding="utf-8") as f:
            for category, articles in news_categories.items():
                for article in articles:
                    for i in range(200):  # 200 variations per article
                        variations = [
                            article,
                            f"Breaking news: {article.lower()}",
                            f"Latest reports indicate that {article.lower()}",
                            f"According to industry sources, {article.lower()}",
                            f"Recent developments show that {article.lower()}",
                            f"News analysis reveals that {article.lower()}"
                        ]
                        f.write(variations[i % len(variations)] + "\n")
        
        file_size = os.path.getsize(news_file) / (1024 * 1024)
        logger.info(f"Created news fallback: {news_file} ({file_size:.1f} MB)")
        return str(news_file)
    
    def combine_full_datasets(self, files: List[str], output_file: str) -> str:
        """Combine multiple large dataset files efficiently."""
        logger.info(f"Combining {len(files)} large datasets into {output_file}")
        
        total_lines = 0
        total_size_mb = 0
        
        with open(output_file, 'w', encoding='utf-8') as outf:
            for file_path in files:
                if os.path.exists(file_path):
                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
                    logger.info(f"Processing {file_path} ({file_size_mb:.1f} MB)...")
                    
                    lines_from_file = 0
                    with open(file_path, 'r', encoding='utf-8') as inf:
                        for line in inf:
                            line = line.strip()
                            if len(line) > 10:  # Skip very short lines
                                outf.write(line + '\n')
                                lines_from_file += 1
                                total_lines += 1
                    
                    logger.info(f"  Added {lines_from_file:,} lines from {file_path}")
                    total_size_mb += file_size_mb
                    
                    # Memory management
                    if psutil.virtual_memory().percent > 80:
                        gc.collect()
                else:
                    logger.warning(f"File not found: {file_path}")
        
        combined_size_mb = os.path.getsize(output_file) / (1024 * 1024)
        logger.info(f"Combined dataset: {total_lines:,} lines, {combined_size_mb:.1f} MB")
        
        return output_file
    
    def train_large_bert_tokenizer(self, data_file: str, vocab_size: int = 50000) -> Tuple[str, dict]:
        """Train large BERT-compatible WordPiece tokenizer with memory management."""
        logger.info(f"Training large BERT tokenizer with vocab_size={vocab_size:,}")
        
        self.log_system_resources("Pre-training")
        
        # Initialize tokenizer
        tokenizer = SimpleWordPieceTokenizer(vocab_size=vocab_size)
        
        # Start training with progress monitoring
        start_time = time.time()
        logger.info("Starting large-scale WordPiece training...")
        logger.info("This may take 2-4 hours depending on data size...")
        
        try:
            # Train with periodic progress updates
            tokenizer.train(data_file)
            
            training_time = time.time() - start_time
            logger.info(f"Training completed in {timedelta(seconds=int(training_time))}")
            
            self.log_system_resources("Post-training")
            
            # Save model
            model_file = self.output_dir / f"bert_large_tokenizer_vocab{vocab_size//1000}k.pkl"
            vocab_file = self.output_dir / f"bert_large_vocab_{vocab_size//1000}k.txt"
            
            tokenizer.save_model(str(model_file))
            tokenizer.save_vocab(str(vocab_file))
            
            # Calculate model statistics
            model_size_mb = os.path.getsize(model_file) / (1024 * 1024)
            vocab_size_mb = os.path.getsize(vocab_file) / (1024 * 1024)
            
            results = {
                "final_vocab_size": len(tokenizer.vocab),
                "training_time_seconds": training_time,
                "training_time_formatted": str(timedelta(seconds=int(training_time))),
                "model_file": str(model_file),
                "vocab_file": str(vocab_file),
                "model_size_mb": model_size_mb,
                "vocab_size_mb": vocab_size_mb
            }
            
            return str(model_file), results
            
        except Exception as e:
            logger.error(f"Large-scale training failed: {e}")
            self.log_system_resources("Training failed")
            raise

def main():
    parser = argparse.ArgumentParser(description="Full-Scale BERT Tokenizer Training")
    parser.add_argument("--vocab-size", type=int, default=50000,
                       help="Target vocabulary size (default: 50,000)")
    parser.add_argument("--output-dir", type=str, default="large_bert_models",
                       help="Output directory for models")
    parser.add_argument("--force-download", action="store_true",
                       help="Force re-download of all datasets")
    
    args = parser.parse_args()
    
    trainer = FullScaleBERTTrainer(args.output_dir)
    
    logger.info("=== Full-Scale BERT Tokenizer Training Started ===")
    logger.info(f"Target vocabulary size: {args.vocab_size:,}")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Available RAM: {trainer.total_ram_gb}GB")
    logger.info(f"Max training memory: {trainer.max_training_memory_gb}GB")
    
    trainer.log_system_resources("Initial")
    
    try:
        # Step 1: Download complete datasets
        logger.info("Step 1: Downloading complete real-world datasets...")
        logger.info("This may take 30-60 minutes for initial download...")
        
        wiki_file = trainer.download_full_wikipedia()
        trainer.log_system_resources("After Wikipedia")
        
        books_file = trainer.download_full_books()
        trainer.log_system_resources("After Books")
        
        news_file = trainer.download_full_news()
        trainer.log_system_resources("After News")
        
        # Step 2: Combine datasets
        logger.info("Step 2: Combining complete datasets...")
        combined_file = str(trainer.output_dir / "combined_full_training_data.txt")
        trainer.combine_full_datasets([wiki_file, books_file, news_file], combined_file)
        
        trainer.log_system_resources("After Combining")
        
        # Step 3: Train large tokenizer
        logger.info("Step 3: Training large-scale BERT tokenizer...")
        model_file, results = trainer.train_large_bert_tokenizer(combined_file, args.vocab_size)
        
        # Step 4: Results and validation
        logger.info("=== Large-Scale Training Results ===")
        for key, value in results.items():
            logger.info(f"{key}: {value}")
        
        # Validate BERT compatibility
        final_vocab_size = results["final_vocab_size"]
        training_time = results["training_time_seconds"]
        
        logger.info("\n=== BERT Compatibility Assessment ===")
        if final_vocab_size >= 30000:
            logger.info("‚úÖ Excellent vocabulary size for BERT-large compatibility")
        elif final_vocab_size >= 15000:
            logger.info("‚úÖ Good vocabulary size for BERT-base compatibility")
        else:
            logger.info("‚ö†Ô∏è Vocabulary size may be insufficient for optimal BERT performance")
        
        if training_time > 3600:  # More than 1 hour
            logger.info("‚úÖ Substantial training time indicates comprehensive learning")
        else:
            logger.info("‚ö†Ô∏è Relatively quick training - consider more data for better coverage")
        
        logger.info(f"\nüéâ Full-scale BERT tokenizer training completed successfully!")
        logger.info(f"Large model ready for transformer training: {model_file}")
        logger.info(f"Vocabulary size: {final_vocab_size:,} tokens")
        logger.info(f"Model size: {results['model_size_mb']:.1f} MB")
        
        # Cleanup large intermediate files to save space
        try:
            os.remove(combined_file)
            logger.info("Cleaned up large temporary training file")
        except:
            pass
        
        trainer.log_system_resources("Final")
        
        return 0
        
    except Exception as e:
        logger.error(f"Full-scale training pipeline failed: {e}")
        trainer.log_system_resources("Error state")
        return 1

if __name__ == "__main__":
    sys.exit(main())