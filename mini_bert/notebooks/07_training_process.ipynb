{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Training Process\n",
    "\n",
    "This notebook walks through the complete training process of Mini-BERT.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Masked Language Modeling (MLM) objective\n",
    "2. The complete training loop\n",
    "3. Loss computation and gradient flow\n",
    "4. Monitoring training progress\n",
    "5. Common training problems and solutions\n",
    "6. Batch processing and gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model import MiniBERT\n",
    "from tokenizer import WordPieceTokenizer\n",
    "from mlm import mask_tokens, mlm_cross_entropy\n",
    "from optimizer import AdamOptimizer\n",
    "from gradients import MiniBERTGradients\n",
    "\n",
    "np.random.seed(42)\n",
    "# Set style for better visualizations - handle version compatibility\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-darkgrid') \n    except OSError:\n        plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding MLM Objective\n",
    "\n",
    "BERT learns through Masked Language Modeling - predicting masked words in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = WordPieceTokenizer()\ntokenizer.load_model('../tokenizer_8k.pkl')\n",
    "\n",
    "# Example training data\n",
    "training_texts = [\n",
    "    \"The cat sat on the mat because it was comfortable.\",\n",
    "    \"Machine learning models require large amounts of data.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"BERT uses transformer architecture for language understanding.\"\n",
    "]\n",
    "\n",
    "print(\"Training Examples:\")\n",
    "for i, text in enumerate(training_texts):\n",
    "    print(f\"{i+1}. {text}\")\n",
    "\n",
    "# Demonstrate MLM masking\n",
    "def demonstrate_mlm_masking(text, mask_prob=0.15):\n",
    "    \"\"\"Show how MLM masking works\"\"\"\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = tokenizer.decode(token_ids).split()\n",
    "    \n",
    "    # Apply masking\n",
    "    masked_ids, target_ids, mask_positions = mask_tokens(\n",
    "        np.array([token_ids]),\n",
    "        mask_token_id=tokenizer.vocab['[MASK]'],\n",
    "        vocab_size=8192,\n",
    "        mask_prob=mask_prob\n",
    "    )\n",
    "    \n",
    "    # Decode masked version\n",
    "    masked_text = tokenizer.decode(masked_ids[0])\n",
    "    print(f\"Masked:   {masked_text}\")\n",
    "    \n",
    "    # Show targets\n",
    "    if len(mask_positions) > 0 and len(target_ids) > 0:\n",
    "        print(f\"Targets:  {[tokenizer.decode([tid]) for tid in target_ids[0]]}\")\n",
    "        print(f\"Positions: {mask_positions[0]}\")\n",
    "    \n",
    "    return masked_ids, target_ids, mask_positions\n",
    "\n",
    "# Demonstrate on first example\n",
    "masked_ids, target_ids, mask_positions = demonstrate_mlm_masking(training_texts[0])\n",
    "\n",
    "print(\"\\nMLM Strategy:\")\n",
    "print(\"\u00e2\u20ac\u00a2 80% of masked tokens \u00e2\u2020\u2019 [MASK]\")\n",
    "print(\"\u00e2\u20ac\u00a2 10% of masked tokens \u00e2\u2020\u2019 random word\")\n",
    "print(\"\u00e2\u20ac\u00a2 10% of masked tokens \u00e2\u2020\u2019 unchanged\")\n",
    "print(\"\u00e2\u20ac\u00a2 Model learns to predict the original token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Single Training Step\n",
    "\n",
    "Let's walk through one complete training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_training_step(model, optimizer, grad_computer, \n",
    "                        input_ids, target_ids, mask_positions):\n",
    "    \"\"\"\n",
    "    Perform one training step and return detailed information.\n",
    "    \"\"\"\n",
    "    step_info = {}\n",
    "    \n",
    "    # 1. Forward pass\n",
    "    print(\"Step 1: Forward Pass\")\n",
    "    logits, cache = model.forward(input_ids)\n",
    "    step_info['logits_shape'] = logits.shape\n",
    "    print(f\"  Input shape: {input_ids.shape}\")\n",
    "    print(f\"  Output logits shape: {logits.shape}\")\n",
    "    \n",
    "    # 2. Compute loss and gradients\n",
    "    print(\"\\nStep 2: Loss Computation\")\n",
    "    if len(mask_positions) > 0:\n",
    "        loss, grad_logits = grad_computer.compute_mlm_loss_and_gradients(\n",
    "            logits, target_ids, mask_positions\n",
    "        )\n",
    "        step_info['loss'] = loss\n",
    "        step_info['num_masked_tokens'] = len(mask_positions[0])\n",
    "        print(f\"  Loss: {loss:.4f}\")\n",
    "        print(f\"  Masked positions: {len(mask_positions[0])}\")\n",
    "    else:\n",
    "        print(\"  No masked tokens - skipping loss computation\")\n",
    "        return step_info\n",
    "    \n",
    "    # 3. Backward pass\n",
    "    print(\"\\nStep 3: Backward Pass\")\n",
    "    grad_computer.zero_gradients()\n",
    "    grad_computer.backward_from_logits(grad_logits, cache)\n",
    "    \n",
    "    # Analyze gradients\n",
    "    all_grads = []\n",
    "    for name, grad in grad_computer.gradients.items():\n",
    "        if grad is not None and np.any(grad):\n",
    "            all_grads.append(grad.flatten())\n",
    "    \n",
    "    if all_grads:\n",
    "        all_grads = np.concatenate(all_grads)\n",
    "        step_info['grad_norm'] = np.linalg.norm(all_grads)\n",
    "        step_info['grad_mean'] = np.mean(all_grads)\n",
    "        step_info['grad_std'] = np.std(all_grads)\n",
    "        print(f\"  Gradient norm: {step_info['grad_norm']:.6f}\")\n",
    "        print(f\"  Gradient mean: {step_info['grad_mean']:.6f}\")\n",
    "    \n",
    "    # 4. Optimizer step\n",
    "    print(\"\\nStep 4: Parameter Update\")\n",
    "    optimizer.step(grad_computer.gradients)\n",
    "    print(f\"  Parameters updated with Adam\")\n",
    "    print(f\"  Learning rate: {optimizer.learning_rate}\")\n",
    "    \n",
    "    return step_info\n",
    "\n",
    "# Initialize components\n",
    "model = MiniBERT()\n",
    "optimizer = AdamOptimizer(learning_rate=0.001)\n",
    "grad_computer = MiniBERTGradients(model)\n",
    "\n",
    "# Run one training step\n",
    "print(\"=\" * 60)\n",
    "print(\"SINGLE TRAINING STEP DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(mask_positions) > 0:\n",
    "    step_info = single_training_step(model, optimizer, grad_computer,\n",
    "                                   masked_ids, target_ids, mask_positions)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"STEP SUMMARY:\")\n",
    "    for key, value in step_info.items():\n",
    "        print(f\"  {key}: {value}\")\nelse:\n",
    "    print(\"No masked tokens in this example. Try again or increase mask probability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Mini Training Loop\n",
    "\n",
    "Let's train on multiple examples and monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_training_loop(texts, num_epochs=3, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Run a mini training loop on the provided texts.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    model = MiniBERT()\n",
    "    optimizer = AdamOptimizer(learning_rate=0.0001)  # Smaller LR for stability\n",
    "    grad_computer = MiniBERTGradients(model)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'losses': [],\n",
    "        'grad_norms': [],\n",
    "        'steps': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(f\"Starting mini training loop...\")\n",
    "    print(f\"Texts: {len(texts)}, Epochs: {num_epochs}\")\n",
    "    print(\"\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for text_idx, text in enumerate(texts):\n",
    "            # Tokenize and mask\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            masked_ids, target_ids, mask_positions = mask_tokens(\n",
    "                np.array([token_ids]),\n",
    "                mask_token_id=tokenizer.vocab['[MASK]'],\n",
    "                vocab_size=8192,\n",
    "                mask_prob=mask_prob\n",
    "            )\n",
    "            \n",
    "            # Skip if no masks\n",
    "            if len(mask_positions) == 0 or len(target_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, cache = model.forward(masked_ids)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, grad_logits = grad_computer.compute_mlm_loss_and_gradients(\n",
    "                logits, target_ids, mask_positions\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_computer.zero_gradients()\n",
    "            grad_computer.backward_from_logits(grad_logits, cache)\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            all_grads = []\n",
    "            for grad in grad_computer.gradients.values():\n",
    "                if grad is not None and np.any(grad):\n",
    "                    all_grads.append(grad.flatten())\n",
    "            \n",
    "            if all_grads:\n",
    "                grad_norm = np.linalg.norm(np.concatenate(all_grads))\n",
    "            else:\n",
    "                grad_norm = 0.0\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step(grad_computer.gradients)\n",
    "            \n",
    "            # Record history\n",
    "            history['losses'].append(loss)\n",
    "            history['grad_norms'].append(grad_norm)\n",
    "            history['steps'].append(step)\n",
    "            history['epochs'].append(epoch)\n",
    "            \n",
    "            epoch_losses.append(loss)\n",
    "            step += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"  Text {text_idx+1}: Loss = {loss:.4f}, Grad norm = {grad_norm:.6f}\")\n",
    "        \n",
    "        # Epoch summary\n",
    "        if epoch_losses:\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"  Average epoch loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Run mini training\n",
    "trained_model, training_history = mini_training_loop(training_texts, num_epochs=2)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "if training_history['losses']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curve\n",
    "    axes[0, 0].plot(training_history['steps'], training_history['losses'], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Training Step')\n",
    "    axes[0, 0].set_ylabel('MLM Loss')\n",
    "    axes[0, 0].set_title('Training Loss Over Time')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norms\n",
    "    axes[0, 1].semilogy(training_history['steps'], training_history['grad_norms'], 'r-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Training Step')\n",
    "    axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "    axes[0, 1].set_title('Gradient Norms')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss distribution by epoch\n",
    "    unique_epochs = sorted(set(training_history['epochs']))\n",
    "    epoch_colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, epoch in enumerate(unique_epochs):\n",
    "        epoch_losses = [loss for loss, ep in zip(training_history['losses'], \n",
    "                                                training_history['epochs']) if ep == epoch]\n",
    "        if epoch_losses:\n",
    "            axes[1, 0].hist(epoch_losses, bins=10, alpha=0.7, \n",
    "                          color=epoch_colors[i % len(epoch_colors)], \n",
    "                          label=f'Epoch {epoch}')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Loss Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Loss Distribution by Epoch')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training statistics\n",
    "    stats_text = f\"\"\"\n",
    "    Training Statistics:\n",
    "    \n",
    "    Total Steps: {len(training_history['steps'])}\n",
    "    Final Loss: {training_history['losses'][-1]:.4f}\n",
    "    Best Loss: {min(training_history['losses']):.4f}\n",
    "    Avg Gradient Norm: {np.mean(training_history['grad_norms']):.6f}\n",
    "    \n",
    "    Loss Improvement:\n",
    "    First: {training_history['losses'][0]:.4f}\n",
    "    Last:  {training_history['losses'][-1]:.4f}\n",
    "    Change: {training_history['losses'][-1] - training_history['losses'][0]:.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[1, 1].set_title('Training Summary')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n    \n",
    "    # Print analysis\n",
    "    print(\"Training Analysis:\")\n",
    "    if training_history['losses'][-1] < training_history['losses'][0]:\n",
    "        print(\"\u00e2\u0153\u201c Loss decreased - model is learning!\")\n",
    "    else:\n",
    "        print(\"\u00e2\u0161\u00a0 Loss increased - may need tuning\")\n",
    "    \n",
    "    if np.mean(training_history['grad_norms']) > 1.0:\n",
    "        print(\"\u00e2\u0161\u00a0 Large gradients - consider gradient clipping\")\n",
    "    else:\n",
    "        print(\"\u00e2\u0153\u201c Gradient norms look healthy\")\nelse:\n",
    "    print(\"No training data recorded - check masking probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Batch Processing\n",
    "\n",
    "Real training uses batches of examples for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(texts, tokenizer, max_length=32):\n",
    "    \"\"\"\n",
    "    Create a batch of tokenized and padded sequences.\n",
    "    \"\"\"\n",
    "    batch_input_ids = []\n",
    "    batch_attention_mask = []\n",
    "    \n",
    "    pad_token_id = tokenizer.vocab.get('[PAD]', 0)\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        # Pad to max_length\n",
    "        while len(token_ids) < max_length:\n",
    "            token_ids.append(pad_token_id)\n",
    "            attention_mask.append(0)\n",
    "        \n",
    "        batch_input_ids.append(token_ids)\n",
    "        batch_attention_mask.append(attention_mask)\n",
    "    \n",
    "    return np.array(batch_input_ids), np.array(batch_attention_mask)\n",
    "\n",
    "# Create a batch\n",
    "batch_texts = training_texts\n",
    "batch_input_ids, batch_attention_mask = create_batch(batch_texts, tokenizer)\n",
    "\n",
    "print(f\"Batch shape: {batch_input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {batch_attention_mask.shape}\")\n",
    "\n",
    "# Visualize batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Input IDs\n",
    "im1 = ax1.imshow(batch_input_ids, cmap='viridis', aspect='auto')\n",
    "ax1.set_title('Batch Input IDs')\n",
    "ax1.set_xlabel('Sequence Position')\n",
    "ax1.set_ylabel('Batch Index')\n",
    "ax1.set_yticks(range(len(batch_texts)))\n",
    "ax1.set_yticklabels([f'Text {i+1}' for i in range(len(batch_texts))])\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Attention mask\n",
    "im2 = ax2.imshow(batch_attention_mask, cmap='RdYlBu', aspect='auto')\n",
    "ax2.set_title('Attention Mask (1=real, 0=padding)')\n",
    "ax2.set_xlabel('Sequence Position')\n",
    "ax2.set_ylabel('Batch Index')\n",
    "ax2.set_yticks(range(len(batch_texts)))\n",
    "ax2.set_yticklabels([f'Text {i+1}' for i in range(len(batch_texts))])\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBatch Processing Benefits:\")\n",
    "print(\"\u00e2\u20ac\u00a2 GPU/vectorization efficiency\")\n",
    "print(\"\u00e2\u20ac\u00a2 More stable gradients\")\n",
    "print(\"\u00e2\u20ac\u00a2 Better gradient estimates\")\n",
    "print(\"\u00e2\u20ac\u00a2 Faster training overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Monitoring\n",
    "\n",
    "Key metrics to watch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_training_metrics(model, tokenizer, texts, sample_size=100):\n",
    "    \"\"\"\n",
    "    Compute various training metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Sample some evaluation examples\n",
    "    eval_losses = []\n",
    "    total_tokens = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize and mask\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        masked_ids, target_ids, mask_positions = mask_tokens(\n",
    "            np.array([token_ids]),\n",
    "            mask_token_id=tokenizer.vocab['[MASK]'],\n",
    "            vocab_size=8192,\n",
    "            mask_prob=0.15\n",
    "        )\n",
    "        \n",
    "        if len(mask_positions) == 0 or len(target_ids) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = model.forward(masked_ids)\n",
    "        \n",
    "        # Compute predictions at masked positions\n",
    "        for i, pos in enumerate(mask_positions[0]):\n",
    "            if i < len(target_ids[0]):\n",
    "                target_id = target_ids[0][i]\n",
    "                predicted_id = np.argmax(logits[0, pos])\n",
    "                \n",
    "                if predicted_id == target_id:\n",
    "                    correct_predictions += 1\n",
    "                \n",
    "                total_tokens += 1\n",
    "                \n",
    "                # Compute cross-entropy loss for this token\n",
    "                token_probs = np.exp(logits[0, pos] - np.max(logits[0, pos]))\n",
    "                token_probs = token_probs / np.sum(token_probs)\n",
    "                token_loss = -np.log(token_probs[target_id] + 1e-10)\n",
    "                eval_losses.append(token_loss)\n",
    "    \n",
    "    # Compute metrics\n",
    "    if eval_losses:\n",
    "        metrics['avg_loss'] = np.mean(eval_losses)\n",
    "        metrics['perplexity'] = np.exp(metrics['avg_loss'])\n",
    "    else:\n",
    "        metrics['avg_loss'] = float('inf')\n",
    "        metrics['perplexity'] = float('inf')\n",
    "    \n",
    "    if total_tokens > 0:\n",
    "        metrics['accuracy'] = correct_predictions / total_tokens\n",
    "    else:\n",
    "        metrics['accuracy'] = 0.0\n",
    "    \n",
    "    metrics['total_masked_tokens'] = total_tokens\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute metrics for untrained vs trained model\n",
    "print(\"Computing training metrics...\")\n",
    "\n",
    "# Untrained model\n",
    "untrained_model = MiniBERT()\n",
    "untrained_metrics = compute_training_metrics(untrained_model, tokenizer, training_texts)\n",
    "\n",
    "# Trained model (if we have one)\n",
    "if 'trained_model' in locals():\n",
    "    trained_metrics = compute_training_metrics(trained_model, tokenizer, training_texts)\n",
    "else:\n",
    "    trained_metrics = untrained_metrics\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING METRICS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<20} {'Untrained':<15} {'Trained':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['avg_loss', 'perplexity', 'accuracy']:\n",
    "    untrained_val = untrained_metrics.get(metric, 0)\n",
    "    trained_val = trained_metrics.get(metric, 0)\n",
    "    \n",
    "    if metric == 'accuracy':\n",
    "        improvement = trained_val - untrained_val\n",
    "        improvement_str = f\"+{improvement:.3f}\" if improvement >= 0 else f\"{improvement:.3f}\"\n",
    "    else:\n",
    "        if untrained_val > 0:\n",
    "            improvement = (untrained_val - trained_val) / untrained_val * 100\n",
    "            improvement_str = f\"{improvement:.1f}%\"\n",
    "        else:\n",
    "            improvement_str = \"N/A\"\n",
    "    \n",
    "    print(f\"{metric:<20} {untrained_val:<15.3f} {trained_val:<15.3f} {improvement_str:<15}\")\n",
    "\n",
    "print(f\"\\nMasked tokens evaluated: {trained_metrics.get('total_masked_tokens', 0)}\")\n",
    "\n",
    "# Visualize metrics\n",
    "if trained_metrics.get('total_masked_tokens', 0) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = [untrained_metrics['accuracy'], trained_metrics['accuracy']]\n",
    "    labels = ['Untrained', 'Trained']\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    \n",
    "    bars = ax1.bar(labels, accuracies, color=colors)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('MLM Accuracy')\n",
    "    ax1.set_ylim(0, max(accuracies) * 1.2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Perplexity comparison\n",
    "    perplexities = [untrained_metrics['perplexity'], trained_metrics['perplexity']]\n",
    "    bars = ax2.bar(labels, perplexities, color=colors)\n",
    "    ax2.set_ylabel('Perplexity')\n",
    "    ax2.set_title('Perplexity (lower is better)')\n",
    "    ax2.set_ylim(0, max(perplexities) * 1.2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, perp in zip(bars, perplexities):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(perplexities) * 0.02,\n",
    "                f'{perp:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n\nprint(\"\\nTraining Success Indicators:\")\nprint(\"\u00e2\u0153\u201c Loss decreases over time\")\nprint(\"\u00e2\u0153\u201c Accuracy increases\")\nprint(\"\u00e2\u0153\u201c Perplexity decreases\")\nprint(\"\u00e2\u0153\u201c Gradients remain stable\")\nprint(\"\u00e2\u0153\u201c No NaN or inf values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Training Concepts\n",
    "\n",
    "### **1. MLM Objective**\n",
    "- Mask 15% of tokens randomly\n",
    "- Predict original tokens from context\n",
    "- Forces model to learn bidirectional representations\n",
    "\n",
    "### **2. Training Loop**\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    # Forward pass\n",
    "    logits = model(masked_inputs)\n",
    "    \n",
    "    # Loss computation\n",
    "    loss = cross_entropy(logits[mask_positions], targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = backward(loss)\n",
    "    \n",
    "    # Parameter update\n",
    "    optimizer.step(gradients)\n",
    "```\n",
    "\n",
    "### **3. Key Monitoring Metrics**\n",
    "- **Loss**: Should decrease over time\n",
    "- **Accuracy**: Percentage of correctly predicted masks\n",
    "- **Perplexity**: exp(loss), measures uncertainty\n",
    "- **Gradient norms**: Should be stable, not exploding/vanishing\n",
    "\n",
    "### **4. Training Best Practices**\n",
    "- Use appropriate learning rates (1e-4 to 1e-3)\n",
    "- Monitor gradient norms\n",
    "- Use learning rate scheduling\n",
    "- Implement gradient clipping if needed\n",
    "- Validate on held-out data\n",
    "\n",
    "### **5. Common Issues**\n",
    "- **Loss not decreasing**: LR too high/low, poor initialization\n",
    "- **Gradient explosion**: Clip gradients, reduce LR\n",
    "- **Gradient vanishing**: Check residual connections, LR\n",
    "- **NaN values**: Usually gradient explosion, reduce LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Learning Rate Sensitivity**: Try different learning rates (1e-2, 1e-3, 1e-4, 1e-5). How does training change?\n",
    "\n",
    "2. **Masking Probability**: Experiment with different mask probabilities (10%, 15%, 25%). What's optimal?\n",
    "\n",
    "3. **Batch Size Effects**: Compare training with different batch sizes. How does it affect convergence?\n",
    "\n",
    "4. **Training Diagnostics**: Implement additional metrics like token-level accuracy for different word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}