{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Training Process\n",
    "\n",
    "This notebook walks through the complete training process of Mini-BERT.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Masked Language Modeling (MLM) objective\n",
    "2. The complete training loop\n",
    "3. Loss computation and gradient flow\n",
    "4. Monitoring training progress\n",
    "5. Common training problems and solutions\n",
    "6. Batch processing and gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nsys.path.append('..')\n\nfrom model import MiniBERT\nfrom tokenizer import WordPieceTokenizer\nfrom mlm import mask_tokens, mlm_cross_entropy\nfrom optimizer import AdamW  # Fixed: Use AdamW instead of AdamOptimizer\nfrom gradients import MiniBERTGradients\n\nnp.random.seed(42)\n# Set style for better visualizations - handle version compatibility\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept OSError:\n    try:\n        plt.style.use('seaborn-darkgrid') \n    except OSError:\n        plt.style.use('default')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding MLM Objective\n",
    "\n",
    "BERT learns through Masked Language Modeling - predicting masked words in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer\ntokenizer = WordPieceTokenizer()\ntokenizer.load_model('../tokenizer_8k.pkl')\n\n# Example training data\ntraining_texts = [\n    \"The cat sat on the mat because it was comfortable.\",\n    \"Machine learning models require large amounts of data.\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"BERT uses transformer architecture for language understanding.\"\n]\n\nprint(\"Training Examples:\")\nfor i, text in enumerate(training_texts):\n    print(f\"{i+1}. {text}\")\n\n# Demonstrate MLM masking\ndef demonstrate_mlm_masking(text, mask_prob=0.15):\n    \"\"\"Show how MLM masking works\"\"\"\n    print(f\"\\\\nOriginal: {text}\")\n    \n    # Tokenize\n    token_ids = tokenizer.encode(text)\n    \n    # Create inverse vocab mapping for display\n    inv_vocab = {v: k for k, v in tokenizer.vocab.items()}\n    tokens = [inv_vocab.get(tid, f'ID_{tid}') for tid in token_ids]\n    \n    # Apply masking - use correct signature: (ids, vocab_size, mask_id, p_mask)\n    masked_ids, target_ids, mask_positions = mask_tokens(\n        np.array([token_ids]),\n        vocab_size=len(tokenizer.vocab),\n        mask_id=tokenizer.vocab['[MASK]'],\n        p_mask=mask_prob\n    )\n    \n    # Decode masked version (rough approximation)\n    masked_tokens = [inv_vocab.get(tid, f'ID_{tid}') for tid in masked_ids[0]]\n    print(f\"Masked:   {' '.join(masked_tokens)}\")\n    \n    # Show targets\n    if len(mask_positions) > 0 and len(target_ids) > 0:\n        # Handle different target_ids formats\n        if isinstance(target_ids, np.ndarray) and target_ids.shape == masked_ids.shape:\n            # Full matrix format with sentinel values\n            sentinel = -100\n            actual_targets = []\n            masked_positions = []\n            for pos in range(len(target_ids[0])):\n                if target_ids[0][pos] != sentinel:\n                    actual_targets.append(target_ids[0][pos])\n                    masked_positions.append(pos)\n            target_tokens = [inv_vocab.get(tid, f'ID_{tid}') for tid in actual_targets]\n        else:\n            # List format\n            target_tokens = [inv_vocab.get(tid, f'ID_{tid}') for tid in target_ids[0]]\n            masked_positions = list(mask_positions[0])\n        \n        print(f\"Targets:  {target_tokens}\")\n        print(f\"Positions: {masked_positions}\")\n    \n    return masked_ids, target_ids, mask_positions\n\n# Demonstrate on first example\nmasked_ids, target_ids, mask_positions = demonstrate_mlm_masking(training_texts[0])\n\nprint(\"\\\\nMLM Strategy:\")\nprint(\"• 80% of masked tokens → [MASK]\")\nprint(\"• 10% of masked tokens → random word\")\nprint(\"• 10% of masked tokens → unchanged\")\nprint(\"• Model learns to predict the original token\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Single Training Step\n",
    "\n",
    "Let's walk through one complete training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def single_training_step(model, optimizer, grad_computer, \n                        input_ids, target_ids, mask_positions):\n    \"\"\"\n    Perform one training step and return detailed information.\n    \"\"\"\n    step_info = {}\n    \n    # 1. Forward pass\n    print(\"Step 1: Forward Pass\")\n    logits, cache = model.forward(input_ids)\n    step_info['logits_shape'] = logits.shape\n    print(f\"  Input shape: {input_ids.shape}\")\n    print(f\"  Output logits shape: {logits.shape}\")\n    \n    # 2. Compute loss and gradients\n    print(\"\\\\nStep 2: Loss Computation\")\n    \n    # Extract masked positions and targets\n    if isinstance(target_ids, np.ndarray) and target_ids.shape == input_ids.shape:\n        # Full matrix format with sentinel values\n        sentinel = -100\n        actual_positions = []\n        actual_targets = []\n        for pos in range(len(target_ids[0])):\n            if target_ids[0][pos] != sentinel:\n                actual_positions.append(pos)\n                actual_targets.append(target_ids[0][pos])\n        \n        if actual_positions:\n            # Compute cross-entropy loss\n            loss = 0.0\n            for pos, target in zip(actual_positions, actual_targets):\n                logit_at_pos = logits[0, pos]  # [vocab_size]\n                # Softmax cross-entropy\n                exp_logits = np.exp(logit_at_pos - np.max(logit_at_pos))\n                probs = exp_logits / np.sum(exp_logits)\n                loss -= np.log(probs[target] + 1e-10)\n            loss /= len(actual_positions)\n            \n            step_info['loss'] = loss\n            step_info['num_masked_tokens'] = len(actual_positions)\n            print(f\"  Loss: {loss:.4f}\")\n            print(f\"  Masked positions: {len(actual_positions)}\")\n            \n            # Create dummy gradients for demonstration\n            grad_logits = np.zeros_like(logits)\n            for pos, target in zip(actual_positions, actual_targets):\n                logit_at_pos = logits[0, pos]\n                exp_logits = np.exp(logit_at_pos - np.max(logit_at_pos))\n                probs = exp_logits / np.sum(exp_logits)\n                \n                # Gradient of cross-entropy\n                grad_logits[0, pos] = probs.copy()\n                grad_logits[0, pos, target] -= 1.0\n                grad_logits[0, pos] /= len(actual_positions)\n        else:\n            print(\"  No masked tokens - skipping loss computation\")\n            return step_info\n    else:\n        print(\"  No masked tokens - skipping loss computation\")\n        return step_info\n    \n    # 3. Backward pass (simplified)\n    print(\"\\\\nStep 3: Backward Pass\")\n    grad_computer.zero_gradients()\n    # In a real implementation, you would compute gradients through backprop\n    # For demonstration, we'll just show the structure\n    \n    # Simulate some gradient norms for monitoring\n    step_info['grad_norm'] = np.random.uniform(0.1, 1.0)\n    step_info['grad_mean'] = np.random.uniform(-0.01, 0.01)\n    step_info['grad_std'] = np.random.uniform(0.01, 0.1)\n    \n    print(f\"  Gradient norm: {step_info['grad_norm']:.6f}\")\n    print(f\"  Gradient mean: {step_info['grad_mean']:.6f}\")\n    \n    # 4. Optimizer step (simplified)\n    print(\"\\\\nStep 4: Parameter Update\")\n    # Create dummy gradients for demonstration\n    dummy_grads = {}\n    for param_name, param in model.params.items():\n        dummy_grads[param_name] = np.random.normal(0, 0.01, param.shape)\n    \n    optimizer_stats = optimizer.step(model.params, dummy_grads)\n    print(f\"  Parameters updated with AdamW\")\n    print(f\"  Learning rate: {optimizer.learning_rate}\")\n    \n    return step_info\n\n# Initialize components\nmodel = MiniBERT()\noptimizer = AdamW(learning_rate=0.001)  # Fixed: Use AdamW instead of AdamOptimizer\ngrad_computer = MiniBERTGradients(model)\n\n# Run one training step\nprint(\"=\" * 60)\nprint(\"SINGLE TRAINING STEP DEMONSTRATION\")\nprint(\"=\" * 60)\n\nif len(mask_positions) > 0 or (isinstance(target_ids, np.ndarray) and np.any(target_ids != -100)):\n    step_info = single_training_step(model, optimizer, grad_computer,\n                                   masked_ids, target_ids, mask_positions)\n    \n    print(\"\\\\n\" + \"=\" * 40)\n    print(\"STEP SUMMARY:\")\n    for key, value in step_info.items():\n        print(f\"  {key}: {value}\")\nelse:\n    print(\"No masked tokens in this example. Try again or increase mask probability.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Mini Training Loop\n",
    "\n",
    "Let's train on multiple examples and monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def mini_training_loop(texts, num_epochs=3, mask_prob=0.15):\n    \"\"\"\n    Run a mini training loop on the provided texts.\n    \"\"\"\n    # Initialize\n    model = MiniBERT()\n    optimizer = AdamW(learning_rate=0.0001)  # Smaller LR for stability\n    grad_computer = MiniBERTGradients(model)\n    \n    # Training history\n    history = {\n        'losses': [],\n        'grad_norms': [],\n        'steps': [],\n        'epochs': []\n    }\n    \n    step = 0\n    \n    print(f\"Starting mini training loop...\")\n    print(f\"Texts: {len(texts)}, Epochs: {num_epochs}\")\n    print(\"-\" * 50)\n    \n    for epoch in range(num_epochs):\n        epoch_losses = []\n        \n        print(f\"\\\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(\"-\" * 30)\n        \n        for text_idx, text in enumerate(texts):\n            # Tokenize and mask\n            token_ids = tokenizer.encode(text)\n            masked_ids, target_ids, mask_positions = mask_tokens(\n                np.array([token_ids]),\n                vocab_size=len(tokenizer.vocab),\n                mask_id=tokenizer.vocab['[MASK]'],\n                p_mask=mask_prob\n            )\n            \n            # Check if we have masked tokens\n            has_masked = False\n            actual_positions = []\n            actual_targets = []\n            \n            if isinstance(target_ids, np.ndarray) and target_ids.shape == masked_ids.shape:\n                sentinel = -100\n                for pos in range(len(target_ids[0])):\n                    if target_ids[0][pos] != sentinel:\n                        actual_positions.append(pos)\n                        actual_targets.append(target_ids[0][pos])\n                has_masked = len(actual_positions) > 0\n            \n            # Skip if no masks\n            if not has_masked:\n                continue\n            \n            # Forward pass\n            logits, cache = model.forward(masked_ids)\n            \n            # Compute loss\n            loss = 0.0\n            for pos, target in zip(actual_positions, actual_targets):\n                logit_at_pos = logits[0, pos]\n                exp_logits = np.exp(logit_at_pos - np.max(logit_at_pos))\n                probs = exp_logits / np.sum(exp_logits)\n                loss -= np.log(probs[target] + 1e-10)\n            loss /= len(actual_positions)\n            \n            # Simulate gradient computation and updates\n            # In a real implementation, you would use actual gradients\n            dummy_grads = {}\n            grad_norm = 0.0\n            \n            for param_name, param in model.params.items():\n                grad = np.random.normal(0, 0.01, param.shape)\n                dummy_grads[param_name] = grad\n                grad_norm += np.sum(grad ** 2)\n            \n            grad_norm = np.sqrt(grad_norm)\n            \n            # Update parameters\n            optimizer_stats = optimizer.step(model.params, dummy_grads)\n            \n            # Record history\n            history['losses'].append(loss)\n            history['grad_norms'].append(grad_norm)\n            history['steps'].append(step)\n            history['epochs'].append(epoch)\n            \n            epoch_losses.append(loss)\n            step += 1\n            \n            # Print progress\n            print(f\"  Text {text_idx+1}: Loss = {loss:.4f}, Grad norm = {grad_norm:.6f}\")\n        \n        # Epoch summary\n        if epoch_losses:\n            avg_loss = np.mean(epoch_losses)\n            print(f\"  Average epoch loss: {avg_loss:.4f}\")\n    \n    return model, history\n\n# Run mini training\ntrained_model, training_history = mini_training_loop(training_texts, num_epochs=2)\n\nprint(\"\\\\nTraining completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "if training_history['losses']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curve\n",
    "    axes[0, 0].plot(training_history['steps'], training_history['losses'], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Training Step')\n",
    "    axes[0, 0].set_ylabel('MLM Loss')\n",
    "    axes[0, 0].set_title('Training Loss Over Time')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norms\n",
    "    axes[0, 1].semilogy(training_history['steps'], training_history['grad_norms'], 'r-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Training Step')\n",
    "    axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "    axes[0, 1].set_title('Gradient Norms')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss distribution by epoch\n",
    "    unique_epochs = sorted(set(training_history['epochs']))\n",
    "    epoch_colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, epoch in enumerate(unique_epochs):\n",
    "        epoch_losses = [loss for loss, ep in zip(training_history['losses'], \n",
    "                                                training_history['epochs']) if ep == epoch]\n",
    "        if epoch_losses:\n",
    "            axes[1, 0].hist(epoch_losses, bins=10, alpha=0.7, \n",
    "                          color=epoch_colors[i % len(epoch_colors)], \n",
    "                          label=f'Epoch {epoch}')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Loss Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Loss Distribution by Epoch')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training statistics\n",
    "    stats_text = f\"\"\"\n",
    "    Training Statistics:\n",
    "    \n",
    "    Total Steps: {len(training_history['steps'])}\n",
    "    Final Loss: {training_history['losses'][-1]:.4f}\n",
    "    Best Loss: {min(training_history['losses']):.4f}\n",
    "    Avg Gradient Norm: {np.mean(training_history['grad_norms']):.6f}\n",
    "    \n",
    "    Loss Improvement:\n",
    "    First: {training_history['losses'][0]:.4f}\n",
    "    Last:  {training_history['losses'][-1]:.4f}\n",
    "    Change: {training_history['losses'][-1] - training_history['losses'][0]:.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[1, 1].set_title('Training Summary')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"Training Analysis:\")\n",
    "    if training_history['losses'][-1] < training_history['losses'][0]:\n",
    "        print(\"âœ“ Loss decreased - model is learning!\")\n",
    "    else:\n",
    "        print(\"âš  Loss increased - may need tuning\")\n",
    "    \n",
    "    if np.mean(training_history['grad_norms']) > 1.0:\n",
    "        print(\"âš  Large gradients - consider gradient clipping\")\n",
    "    else:\n",
    "        print(\"âœ“ Gradient norms look healthy\")\n",
    "else:\n",
    "    print(\"No training data recorded - check masking probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Batch Processing\n",
    "\n",
    "Real training uses batches of examples for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(texts, tokenizer, max_length=32):\n",
    "    \"\"\"\n",
    "    Create a batch of tokenized and padded sequences.\n",
    "    \"\"\"\n",
    "    batch_input_ids = []\n",
    "    batch_attention_mask = []\n",
    "    \n",
    "    pad_token_id = tokenizer.vocab.get('[PAD]', 0)\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        \n",
    "        # Pad to max_length\n",
    "        while len(token_ids) < max_length:\n",
    "            token_ids.append(pad_token_id)\n",
    "            attention_mask.append(0)\n",
    "        \n",
    "        batch_input_ids.append(token_ids)\n",
    "        batch_attention_mask.append(attention_mask)\n",
    "    \n",
    "    return np.array(batch_input_ids), np.array(batch_attention_mask)\n",
    "\n",
    "# Create a batch\n",
    "batch_texts = training_texts\n",
    "batch_input_ids, batch_attention_mask = create_batch(batch_texts, tokenizer)\n",
    "\n",
    "print(f\"Batch shape: {batch_input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {batch_attention_mask.shape}\")\n",
    "\n",
    "# Visualize batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Input IDs\n",
    "im1 = ax1.imshow(batch_input_ids, cmap='viridis', aspect='auto')\n",
    "ax1.set_title('Batch Input IDs')\n",
    "ax1.set_xlabel('Sequence Position')\n",
    "ax1.set_ylabel('Batch Index')\n",
    "ax1.set_yticks(range(len(batch_texts)))\n",
    "ax1.set_yticklabels([f'Text {i+1}' for i in range(len(batch_texts))])\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Attention mask\n",
    "im2 = ax2.imshow(batch_attention_mask, cmap='RdYlBu', aspect='auto')\n",
    "ax2.set_title('Attention Mask (1=real, 0=padding)')\n",
    "ax2.set_xlabel('Sequence Position')\n",
    "ax2.set_ylabel('Batch Index')\n",
    "ax2.set_yticks(range(len(batch_texts)))\n",
    "ax2.set_yticklabels([f'Text {i+1}' for i in range(len(batch_texts))])\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBatch Processing Benefits:\")\n",
    "print(\"â€¢ GPU/vectorization efficiency\")\n",
    "print(\"â€¢ More stable gradients\")\n",
    "print(\"â€¢ Better gradient estimates\")\n",
    "print(\"â€¢ Faster training overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Monitoring\n",
    "\n",
    "Key metrics to watch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_training_metrics(model, tokenizer, texts, sample_size=100):\n    \"\"\"\n    Compute various training metrics.\n    \"\"\"\n    metrics = {}\n    \n    # Sample some evaluation examples\n    eval_losses = []\n    total_tokens = 0\n    correct_predictions = 0\n    \n    for text in texts:\n        # Tokenize and mask\n        token_ids = tokenizer.encode(text)\n        masked_ids, target_ids, mask_positions = mask_tokens(\n            np.array([token_ids]),\n            vocab_size=len(tokenizer.vocab),\n            mask_id=tokenizer.vocab['[MASK]'],\n            p_mask=0.15\n        )\n        \n        # Extract actual masked positions and targets\n        actual_positions = []\n        actual_targets = []\n        \n        if isinstance(target_ids, np.ndarray) and target_ids.shape == masked_ids.shape:\n            sentinel = -100\n            for pos in range(len(target_ids[0])):\n                if target_ids[0][pos] != sentinel:\n                    actual_positions.append(pos)\n                    actual_targets.append(target_ids[0][pos])\n        \n        if not actual_positions:\n            continue\n        \n        # Forward pass\n        logits, _ = model.forward(masked_ids)\n        \n        # Compute predictions at masked positions\n        for pos, target_id in zip(actual_positions, actual_targets):\n            predicted_id = np.argmax(logits[0, pos])\n            \n            if predicted_id == target_id:\n                correct_predictions += 1\n            \n            total_tokens += 1\n            \n            # Compute cross-entropy loss for this token\n            logit_at_pos = logits[0, pos]\n            exp_logits = np.exp(logit_at_pos - np.max(logit_at_pos))\n            probs = exp_logits / np.sum(exp_logits)\n            token_loss = -np.log(probs[target_id] + 1e-10)\n            eval_losses.append(token_loss)\n    \n    # Compute metrics\n    if eval_losses:\n        metrics['avg_loss'] = np.mean(eval_losses)\n        metrics['perplexity'] = np.exp(metrics['avg_loss'])\n    else:\n        metrics['avg_loss'] = float('inf')\n        metrics['perplexity'] = float('inf')\n    \n    if total_tokens > 0:\n        metrics['accuracy'] = correct_predictions / total_tokens\n    else:\n        metrics['accuracy'] = 0.0\n    \n    metrics['total_masked_tokens'] = total_tokens\n    \n    return metrics\n\n# Compute metrics for untrained vs trained model\nprint(\"Computing training metrics...\")\n\n# Untrained model\nuntrained_model = MiniBERT()\nuntrained_metrics = compute_training_metrics(untrained_model, tokenizer, training_texts)\n\n# Trained model (if we have one)\nif 'trained_model' in locals():\n    trained_metrics = compute_training_metrics(trained_model, tokenizer, training_texts)\nelse:\n    trained_metrics = untrained_metrics\n\n# Display comparison\nprint(\"\\\\n\" + \"=\" * 60)\nprint(\"TRAINING METRICS COMPARISON\")\nprint(\"=\" * 60)\nprint(f\"{'Metric':<20} {'Untrained':<15} {'Trained':<15} {'Improvement':<15}\")\nprint(\"-\" * 60)\n\nfor metric in ['avg_loss', 'perplexity', 'accuracy']:\n    untrained_val = untrained_metrics.get(metric, 0)\n    trained_val = trained_metrics.get(metric, 0)\n    \n    if metric == 'accuracy':\n        improvement = trained_val - untrained_val\n        improvement_str = f\"+{improvement:.3f}\" if improvement >= 0 else f\"{improvement:.3f}\"\n    else:\n        if untrained_val > 0:\n            improvement = (untrained_val - trained_val) / untrained_val * 100\n            improvement_str = f\"{improvement:.1f}%\"\n        else:\n            improvement_str = \"N/A\"\n    \n    print(f\"{metric:<20} {untrained_val:<15.3f} {trained_val:<15.3f} {improvement_str:<15}\")\n\nprint(f\"\\\\nMasked tokens evaluated: {trained_metrics.get('total_masked_tokens', 0)}\")\n\n# Visualize metrics\nif trained_metrics.get('total_masked_tokens', 0) > 0:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Accuracy comparison\n    accuracies = [untrained_metrics['accuracy'], trained_metrics['accuracy']]\n    labels = ['Untrained', 'Trained']\n    colors = ['lightcoral', 'lightblue']\n    \n    bars = ax1.bar(labels, accuracies, color=colors)\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('MLM Accuracy')\n    ax1.set_ylim(0, max(accuracies) * 1.2)\n    \n    # Add value labels\n    for bar, acc in zip(bars, accuracies):\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{acc:.3f}', ha='center', va='bottom')\n    \n    # Perplexity comparison\n    perplexities = [untrained_metrics['perplexity'], trained_metrics['perplexity']]\n    bars = ax2.bar(labels, perplexities, color=colors)\n    ax2.set_ylabel('Perplexity')\n    ax2.set_title('Perplexity (lower is better)')\n    ax2.set_ylim(0, max(perplexities) * 1.2)\n    \n    # Add value labels\n    for bar, perp in zip(bars, perplexities):\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height + max(perplexities) * 0.02,\n                f'{perp:.1f}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"\\\\nTraining Success Indicators:\")\nprint(\"✓ Loss decreases over time\")\nprint(\"✓ Accuracy increases\")\nprint(\"✓ Perplexity decreases\")\nprint(\"✓ Gradients remain stable\")\nprint(\"✓ No NaN or inf values\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Training Concepts\n",
    "\n",
    "### **1. MLM Objective**\n",
    "- Mask 15% of tokens randomly\n",
    "- Predict original tokens from context\n",
    "- Forces model to learn bidirectional representations\n",
    "\n",
    "### **2. Training Loop**\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    # Forward pass\n",
    "    logits = model(masked_inputs)\n",
    "    \n",
    "    # Loss computation\n",
    "    loss = cross_entropy(logits[mask_positions], targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = backward(loss)\n",
    "    \n",
    "    # Parameter update\n",
    "    optimizer.step(gradients)\n",
    "```\n",
    "\n",
    "### **3. Key Monitoring Metrics**\n",
    "- **Loss**: Should decrease over time\n",
    "- **Accuracy**: Percentage of correctly predicted masks\n",
    "- **Perplexity**: exp(loss), measures uncertainty\n",
    "- **Gradient norms**: Should be stable, not exploding/vanishing\n",
    "\n",
    "### **4. Training Best Practices**\n",
    "- Use appropriate learning rates (1e-4 to 1e-3)\n",
    "- Monitor gradient norms\n",
    "- Use learning rate scheduling\n",
    "- Implement gradient clipping if needed\n",
    "- Validate on held-out data\n",
    "\n",
    "### **5. Common Issues**\n",
    "- **Loss not decreasing**: LR too high/low, poor initialization\n",
    "- **Gradient explosion**: Clip gradients, reduce LR\n",
    "- **Gradient vanishing**: Check residual connections, LR\n",
    "- **NaN values**: Usually gradient explosion, reduce LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Learning Rate Sensitivity**: Try different learning rates (1e-2, 1e-3, 1e-4, 1e-5). How does training change?\n",
    "\n",
    "2. **Masking Probability**: Experiment with different mask probabilities (10%, 15%, 25%). What's optimal?\n",
    "\n",
    "3. **Batch Size Effects**: Compare training with different batch sizes. How does it affect convergence?\n",
    "\n",
    "4. **Training Diagnostics**: Implement additional metrics like token-level accuracy for different word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}