{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately\n",
    "\n",
    "**Rank**: #2 - Revolutionary Impact\n",
    "\n",
    "## Background & Motivation\n",
    "\n",
    "BERT's Masked Language Modeling (MLM) has a fundamental inefficiency: only 15% of tokens are masked, so the model only learns from a small fraction of the input at each step. This means BERT needs enormous amounts of compute to reach good performance.\n",
    "\n",
    "**The Problem with MLM:**\n",
    "- Only 15% of tokens contribute to the loss\n",
    "- 85% of computation is \"wasted\" on unmasked tokens\n",
    "- Need massive datasets and compute for good results\n",
    "- Small models significantly underperform large ones\n",
    "\n",
    "**ELECTRA's Innovation:**\n",
    "- Learn from **ALL** tokens, not just 15%\n",
    "- Replace MLM with \"Replaced Token Detection\" (RTD)\n",
    "- Use a generator-discriminator setup (like GANs)\n",
    "- 4x more sample efficient than BERT\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Generator-Discriminator Architecture**: How ELECTRA uses two models\n",
    "2. **Replaced Token Detection**: The core task that replaces MLM\n",
    "3. **Sample Efficiency**: Why ELECTRA learns faster\n",
    "4. **Implementation**: Building ELECTRA from scratch\n",
    "5. **Results**: Why small ELECTRA models rival large BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import random\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set style for better visualizations\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid') \n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "        \n",
    "print(\"ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately\")\n",
    "print(\"Paper: Clark et al., 2020 - Google Research & Stanford\")\n",
    "print(\"Impact: 4x more efficient than BERT, small models match large BERT performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Original Paper Context\n",
    "\n",
    "### Paper Details\n",
    "- **Title**: \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\"\n",
    "- **Authors**: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning\n",
    "- **Institutions**: Google Research, Stanford University\n",
    "- **Published**: March 2020 (ICLR 2020)\n",
    "- **arXiv**: https://arxiv.org/abs/2003.10555\n",
    "\n",
    "### Revolutionary Results\n",
    "- **ELECTRA-Small** (14M parameters) matches **BERT-Base** (110M parameters)\n",
    "- **ELECTRA-Base** outperforms **RoBERTa-Base** with 1/4 the compute\n",
    "- **ELECTRA-Large** achieves new SOTA on GLUE benchmark\n",
    "- Especially effective for **smaller models** and **limited compute**\n",
    "\n",
    "### Impact on the Field\n",
    "**Direct Applications:**\n",
    "- **Google's production models**: Used ELECTRA for efficiency\n",
    "- **Mobile NLP**: Small ELECTRA models enable on-device processing\n",
    "- **Low-resource settings**: Enables BERT-quality with limited compute\n",
    "\n",
    "**Influenced Research:**\n",
    "- **DeBERTa**: Adopted RTD-style objectives\n",
    "- **Efficient Transformers**: Inspired sample-efficient pre-training\n",
    "- **GAN-based NLP**: Showed generator-discriminator works for language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Core Problem with MLM\n",
    "\n",
    "Let's visualize why BERT's MLM is inefficient and how ELECTRA solves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlm_inefficiency():\n",
    "    \"\"\"\n",
    "    Show the fundamental inefficiency of Masked Language Modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example sentence\n",
    "    sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
    "    n_tokens = len(sentence)\n",
    "    \n",
    "    # MLM: Only 15% of tokens are masked\n",
    "    mask_prob = 0.15\n",
    "    num_masked = int(n_tokens * mask_prob)\n",
    "    \n",
    "    # Randomly select tokens to mask\n",
    "    masked_positions = np.random.choice(n_tokens, num_masked, replace=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Original sentence\n",
    "    colors_original = ['lightblue'] * n_tokens\n",
    "    bars1 = axes[0].bar(range(n_tokens), [1] * n_tokens, color=colors_original)\n",
    "    axes[0].set_title('Original Sentence', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylim(0, 1.5)\n",
    "    axes[0].set_xticks(range(n_tokens))\n",
    "    axes[0].set_xticklabels(sentence, rotation=45)\n",
    "    axes[0].set_ylabel('Token Status')\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, word in enumerate(sentence):\n",
    "        axes[0].text(i, 0.5, word, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. BERT MLM - only some tokens contribute to learning\n",
    "    colors_mlm = ['red' if i in masked_positions else 'lightgray' for i in range(n_tokens)]\n",
    "    bars2 = axes[1].bar(range(n_tokens), [1] * n_tokens, color=colors_mlm)\n",
    "    axes[1].set_title(f'BERT MLM: Only {num_masked}/{n_tokens} tokens ({mask_prob*100:.0f}%) contribute to learning', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylim(0, 1.5)\n",
    "    axes[1].set_xticks(range(n_tokens))\n",
    "    axes[1].set_xticklabels(['[MASK]' if i in masked_positions else word \n",
    "                           for i, word in enumerate(sentence)], rotation=45)\n",
    "    axes[1].set_ylabel('Learning Signal')\n",
    "    \n",
    "    # Add legend\n",
    "    axes[1].bar([], [], color='red', label='Contributes to Loss')\n",
    "    axes[1].bar([], [], color='lightgray', label='No Learning Signal')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 3. ELECTRA RTD - all tokens contribute to learning\n",
    "    colors_electra = ['green'] * n_tokens  # All tokens contribute\n",
    "    bars3 = axes[2].bar(range(n_tokens), [1] * n_tokens, color=colors_electra)\n",
    "    axes[2].set_title(f'ELECTRA RTD: All {n_tokens}/{n_tokens} tokens (100%) contribute to learning!', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylim(0, 1.5)\n",
    "    axes[2].set_xticks(range(n_tokens))\n",
    "    \n",
    "    # Generate some replaced tokens for visualization\n",
    "    replacements = sentence.copy()\n",
    "    replaced_positions = np.random.choice(n_tokens, num_masked, replace=False)\n",
    "    replacement_words = ['cat', 'slow', 'red'][:len(replaced_positions)]\n",
    "    \n",
    "    for i, pos in enumerate(replaced_positions):\n",
    "        if i < len(replacement_words):\n",
    "            replacements[pos] = replacement_words[i]\n",
    "    \n",
    "    axes[2].set_xticklabels(replacements, rotation=45)\n",
    "    axes[2].set_ylabel('Learning Signal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate efficiency\n",
    "    mlm_efficiency = mask_prob\n",
    "    electra_efficiency = 1.0\n",
    "    \n",
    "    print(f\"\\nSAMPLE EFFICIENCY COMPARISON:\")\n",
    "    print(f\"BERT MLM: {mlm_efficiency:.1%} of tokens provide learning signal\")\n",
    "    print(f\"ELECTRA RTD: {electra_efficiency:.1%} of tokens provide learning signal\")\n",
    "    print(f\"\\nELECTRA is {electra_efficiency/mlm_efficiency:.1f}x more sample efficient!\")\n",
    "    \n",
    "    return masked_positions, replaced_positions\n",
    "\nmasked_pos, replaced_pos = visualize_mlm_inefficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ELECTRA's Generator-Discriminator Architecture\n",
    "\n",
    "ELECTRA uses two models working together, similar to GANs but adapted for language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleELECTRA:\n",
    "    \"\"\"\n",
    "    Simplified ELECTRA implementation to demonstrate core concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=8192, hidden_size=192, generator_size=64):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.generator_size = generator_size\n",
    "        \n",
    "        # Initialize generator (smaller model, like BERT-Small)\n",
    "        self.generator_embeddings = np.random.randn(vocab_size, generator_size) * 0.02\n",
    "        self.generator_mlm_head = np.random.randn(generator_size, vocab_size) * 0.02\n",
    "        \n",
    "        # Initialize discriminator (larger model, like BERT-Base)\n",
    "        self.discriminator_embeddings = np.random.randn(vocab_size, hidden_size) * 0.02\n",
    "        self.discriminator_rtd_head = np.random.randn(hidden_size, 1) * 0.02\n",
    "        \n",
    "        print(f\"Generator parameters: {self.generator_embeddings.size + self.generator_mlm_head.size:,}\")\n",
    "        print(f\"Discriminator parameters: {self.discriminator_embeddings.size + self.discriminator_rtd_head.size:,}\")\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Compute sigmoid\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def generator_step(self, input_ids, masked_positions):\n",
    "        \"\"\"\n",
    "        Generator: Predict masked tokens (like BERT MLM)\n",
    "        \"\"\"\n",
    "        # Simple embedding lookup + linear layer\n",
    "        embeddings = self.generator_embeddings[input_ids]  # [seq_len, generator_size]\n",
    "        \n",
    "        # MLM predictions\n",
    "        logits = embeddings @ self.generator_mlm_head.T  # [seq_len, vocab_size]\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        # Sample predictions for masked positions\n",
    "        generated_tokens = input_ids.copy()\n",
    "        \n",
    "        for pos in masked_positions:\n",
    "            # Sample from the probability distribution\n",
    "            generated_token = np.random.choice(self.vocab_size, p=probs[pos])\n",
    "            generated_tokens[pos] = generated_token\n",
    "        \n",
    "        return generated_tokens, probs, logits\n",
    "    \n",
    "    def discriminator_step(self, corrupted_tokens, original_tokens):\n",
    "        \"\"\"\n",
    "        Discriminator: Detect which tokens are replaced\n",
    "        \"\"\"\n",
    "        # Embedding lookup\n",
    "        embeddings = self.discriminator_embeddings[corrupted_tokens]  # [seq_len, hidden_size]\n",
    "        \n",
    "        # Binary classification for each position\n",
    "        logits = embeddings @ self.discriminator_rtd_head  # [seq_len, 1]\n",
    "        logits = logits.squeeze(-1)  # [seq_len]\n",
    "        \n",
    "        # Sigmoid to get probabilities\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "        # True labels: 1 if token was replaced, 0 if original\n",
    "        labels = (corrupted_tokens != original_tokens).astype(float)\n",
    "        \n",
    "        return probs, labels, logits\n",
    "    \n",
    "    def train_step(self, input_ids, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Complete ELECTRA training step\n",
    "        \"\"\"\n",
    "        original_tokens = input_ids.copy()\n",
    "        \n",
    "        # Step 1: Create masked input for generator\n",
    "        masked_input = input_ids.copy()\n",
    "        mask_token_id = self.vocab_size - 1  # [MASK] token\n",
    "        \n",
    "        # Randomly select positions to mask\n",
    "        num_mask = max(1, int(len(input_ids) * mask_prob))\n",
    "        masked_positions = np.random.choice(len(input_ids), num_mask, replace=False)\n",
    "        \n",
    "        # Mask tokens\n",
    "        for pos in masked_positions:\n",
    "            masked_input[pos] = mask_token_id\n",
    "        \n",
    "        # Step 2: Generator predicts masked tokens\n",
    "        generated_tokens, gen_probs, gen_logits = self.generator_step(masked_input, masked_positions)\n",
    "        \n",
    "        # Step 3: Create corrupted sequence for discriminator\n",
    "        corrupted_tokens = original_tokens.copy()\n",
    "        for pos in masked_positions:\n",
    "            corrupted_tokens[pos] = generated_tokens[pos]\n",
    "        \n",
    "        # Step 4: Discriminator detects replaced tokens\n",
    "        disc_probs, disc_labels, disc_logits = self.discriminator_step(corrupted_tokens, original_tokens)\n",
    "        \n",
    "        # Calculate losses\n",
    "        # Generator loss: MLM cross-entropy on masked positions\n",
    "        gen_loss = 0\n",
    "        for pos in masked_positions:\n",
    "            target = original_tokens[pos]\n",
    "            gen_loss += -np.log(gen_probs[pos, target] + 1e-10)\n",
    "        gen_loss /= len(masked_positions)\n",
    "        \n",
    "        # Discriminator loss: Binary cross-entropy on all positions\n",
    "        disc_loss = 0\n",
    "        for i in range(len(disc_labels)):\n",
    "            p = disc_probs[i]\n",
    "            label = disc_labels[i]\n",
    "            disc_loss += -(label * np.log(p + 1e-10) + (1-label) * np.log(1-p + 1e-10))\n",
    "        disc_loss /= len(disc_labels)\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': gen_loss,\n",
    "            'discriminator_loss': disc_loss,\n",
    "            'original_tokens': original_tokens,\n",
    "            'corrupted_tokens': corrupted_tokens,\n",
    "            'discriminator_predictions': disc_probs,\n",
    "            'discriminator_labels': disc_labels,\n",
    "            'masked_positions': masked_positions\n",
    "        }\n",
    "\n# Demonstrate ELECTRA\nelectra = SimpleELECTRA()\n\n# Example input\nvocab = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', '[MASK]', '[PAD]']\nsentence_ids = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # \"the quick brown fox jumps over lazy dog\"\n\nprint(\"\\nELECTRA TRAINING DEMONSTRATION:\")\nprint(f\"Input: {[vocab[i] for i in sentence_ids]}\")\n\n# Run training step\nresult = electra.train_step(sentence_ids)\n\nprint(f\"\\nMasked positions: {result['masked_positions']}\")\nprint(f\"Original: {[vocab[i] for i in result['original_tokens']]}\")\nprint(f\"Corrupted: {[vocab[i % len(vocab)] for i in result['corrupted_tokens']]}\")\nprint(f\"\\nGenerator loss: {result['generator_loss']:.4f}\")\nprint(f\"Discriminator loss: {result['discriminator_loss']:.4f}\")\n\n# Show discriminator predictions\nprint(\"\\nDiscriminator predictions (probability token is replaced):\")\nfor i, (orig, corr, pred, label) in enumerate(zip(\n    result['original_tokens'], \n    result['corrupted_tokens'],\n    result['discriminator_predictions'],\n    result['discriminator_labels']\n)):\n    status = \"REPLACED\" if label == 1 else \"ORIGINAL\"\n    print(f\"Position {i}: {vocab[orig % len(vocab)]} -> {vocab[corr % len(vocab)]}: {pred:.3f} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Why ELECTRA is More Sample Efficient\n",
    "\n",
    "Let's analyze mathematically why ELECTRA learns faster than BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_efficiency():\n",
    "    \"\"\"\n",
    "    Analyze why ELECTRA is more sample efficient than BERT\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    sequence_length = 128\n",
    "    mask_probability = 0.15\n",
    "    num_examples = 1000\n",
    "    \n",
    "    # Calculate learning signals per example\n",
    "    bert_signals_per_example = sequence_length * mask_probability\n",
    "    electra_signals_per_example = sequence_length  # All tokens\n",
    "    \n",
    "    # Total learning signals\n",
    "    bert_total_signals = bert_signals_per_example * num_examples\n",
    "    electra_total_signals = electra_signals_per_example * num_examples\n",
    "    \n",
    "    print(\"SAMPLE EFFICIENCY ANALYSIS:\")\n",
    "    print(f\"\\nSequence length: {sequence_length} tokens\")\n",
    "    print(f\"Number of examples: {num_examples:,}\")\n",
    "    print(f\"\\nBERT MLM:\")\n",
    "    print(f\"  Mask probability: {mask_probability:.1%}\")\n",
    "    print(f\"  Learning signals per example: {bert_signals_per_example:.1f}\")\n",
    "    print(f\"  Total learning signals: {bert_total_signals:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nELECTRA RTD:\")\n",
    "    print(f\"  All tokens contribute: 100%\")\n",
    "    print(f\"  Learning signals per example: {electra_signals_per_example:.1f}\")\n",
    "    print(f\"  Total learning signals: {electra_total_signals:,.0f}\")\n",
    "    \n",
    "    efficiency_ratio = electra_total_signals / bert_total_signals\n",
    "    print(f\"\\nEfficiency ratio: {efficiency_ratio:.1f}x\")\n",
    "    \n",
    "    # Simulate learning curves\n",
    "    steps = np.linspace(0, num_examples, 100)\n",
    "    \n",
    "    # BERT learning curve (slower due to fewer signals)\n",
    "    bert_performance = 1 - np.exp(-steps * bert_signals_per_example / 10000)\n",
    "    \n",
    "    # ELECTRA learning curve (faster due to more signals)\n",
    "    electra_performance = 1 - np.exp(-steps * electra_signals_per_example / 10000)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sample efficiency comparison\n",
    "    methods = ['BERT MLM', 'ELECTRA RTD']\n",
    "    signals = [bert_signals_per_example, electra_signals_per_example]\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    \n",
    "    bars = axes[0].bar(methods, signals, color=colors, alpha=0.8)\n",
    "    axes[0].set_ylabel('Learning Signals per Example')\n",
    "    axes[0].set_title('Sample Efficiency Comparison')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, signals):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Learning curves\n",
    "    axes[1].plot(steps, bert_performance, 'r-', linewidth=3, label='BERT MLM', alpha=0.8)\n",
    "    axes[1].plot(steps, electra_performance, 'b-', linewidth=3, label='ELECTRA RTD', alpha=0.8)\n",
    "    axes[1].set_xlabel('Training Examples')\n",
    "    axes[1].set_ylabel('Performance')\n",
    "    axes[1].set_title('Simulated Learning Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    axes[1].annotate('ELECTRA reaches high\\nperformance faster', \n",
    "                    xy=(300, 0.8), xytext=(500, 0.6),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "                    fontsize=12, color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return efficiency_ratio\n",
    "\nefficiency_gain = analyze_sample_efficiency()\n\n# Additional analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"WHY THIS MATTERS:\")\nprint(f\"\\n1. Training Speed: ELECTRA needs {1/efficiency_gain:.1f}x less data\")\nprint(f\"2. Compute Cost: {efficiency_gain:.1f}x reduction in training time\")\nprint(f\"3. Model Size: Small ELECTRA can match large BERT\")\nprint(f\"4. Accessibility: Enables BERT-quality on modest hardware\")\nprint(f\"\\n5. Mathematical Intuition:\")\nprint(f\"   - BERT: Loss only on {15}% of tokens\")\nprint(f\"   - ELECTRA: Loss on {100}% of tokens\")\nprint(f\"   - Result: {100/15:.1f}x more learning signal per example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Replaced Token Detection Task\n",
    "\n",
    "Let's dive deeper into how RTD works and why it's effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rtd_task():\n",
    "    \"\"\"\n",
    "    Demonstrate the Replaced Token Detection task in detail\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example sentences with different types of replacements\n",
    "    examples = [\n",
    "        {\n",
    "            'original': 'The quick brown fox jumps over the lazy dog'.split(),\n",
    "            'corrupted': 'The fast brown fox jumps over the lazy dog'.split(),\n",
    "            'explanation': 'Semantic replacement: quick → fast'\n",
    "        },\n",
    "        {\n",
    "            'original': 'Machine learning algorithms require large datasets'.split(),\n",
    "            'corrupted': 'Machine learning algorithms require purple datasets'.split(),\n",
    "            'explanation': 'Nonsensical replacement: large → purple'\n",
    "        },\n",
    "        {\n",
    "            'original': 'The cat sat on the comfortable mat'.split(),\n",
    "            'corrupted': 'The cat sat on the comfortable cat'.split(),\n",
    "            'explanation': 'Repetition replacement: mat → cat'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"REPLACED TOKEN DETECTION (RTD) EXAMPLES:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(14, 4*len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ex_idx, example in enumerate(examples):\n",
    "        original = example['original']\n",
    "        corrupted = example['corrupted']\n",
    "        explanation = example['explanation']\n",
    "        \n",
    "        print(f\"\\nExample {ex_idx + 1}: {explanation}\")\n",
    "        print(f\"Original:  {' '.join(original)}\")\n",
    "        print(f\"Corrupted: {' '.join(corrupted)}\")\n",
    "        \n",
    "        # Find differences\n",
    "        labels = []\n",
    "        for i, (orig, corr) in enumerate(zip(original, corrupted)):\n",
    "            if orig != corr:\n",
    "                labels.append(1)  # Replaced\n",
    "                print(f\"Position {i}: '{orig}' → '{corr}' [REPLACED]\")\n",
    "            else:\n",
    "                labels.append(0)  # Original\n",
    "        \n",
    "        # Visualize\n",
    "        colors = ['red' if label == 1 else 'lightblue' for label in labels]\n",
    "        bars = axes[ex_idx].bar(range(len(corrupted)), [1] * len(corrupted), color=colors)\n",
    "        axes[ex_idx].set_title(f'Example {ex_idx + 1}: {explanation}', fontweight='bold')\n",
    "        axes[ex_idx].set_ylim(0, 1.5)\n",
    "        axes[ex_idx].set_xticks(range(len(corrupted)))\n",
    "        axes[ex_idx].set_xticklabels(corrupted, rotation=45)\n",
    "        axes[ex_idx].set_ylabel('Token Status')\n",
    "        \n",
    "        # Add token labels on bars\n",
    "        for i, (word, label) in enumerate(zip(corrupted, labels)):\n",
    "            axes[ex_idx].text(i, 0.5, word, ha='center', va='center', \n",
    "                             fontweight='bold', color='white' if label == 1 else 'black')\n",
    "        \n",
    "        # Add legend for first plot\n",
    "        if ex_idx == 0:\n",
    "            axes[ex_idx].bar([], [], color='red', label='Replaced Token')\n",
    "            axes[ex_idx].bar([], [], color='lightblue', label='Original Token')\n",
    "            axes[ex_idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RTD Task Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RTD TASK CHARACTERISTICS:\")\n",
    "    print(\"\\n1. Binary Classification: Each token is either ORIGINAL or REPLACED\")\n",
    "    print(\"2. Contextual Understanding: Model must use context to detect anomalies\")\n",
    "    print(\"3. All Positions Matter: Every token contributes to the loss\")\n",
    "    print(\"4. Generator Quality: Better generator makes task harder (good!)\")\n",
    "    \n",
    "    print(\"\\nWHY RTD WORKS BETTER THAN MLM:\")\n",
    "    print(\"✓ Dense learning signal (100% vs 15%)\")\n",
    "    print(\"✓ Contextual reasoning required\")\n",
    "    print(\"✓ Adversarial training effect\")\n",
    "    print(\"✓ Encourages better representation learning\")\n",
    "    \n",
    "demonstrate_rtd_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Generator-Discriminator Training Dynamics\n",
    "\n",
    "Understanding how the generator and discriminator interact during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training_dynamics(num_steps=1000):\n",
    "    \"\"\"\n",
    "    Simulate the interaction between generator and discriminator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize training history\n",
    "    history = {\n",
    "        'generator_loss': [],\n",
    "        'discriminator_loss': [],\n",
    "        'generator_quality': [],\n",
    "        'discriminator_accuracy': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    # Initial states\n",
    "    gen_quality = 0.1  # How good generator is at fooling discriminator\n",
    "    disc_accuracy = 0.5  # Random at first\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Generator improvement (learns to generate better replacements)\n",
    "        gen_improvement = 0.001 * (1 - gen_quality)  # Diminishing returns\n",
    "        gen_quality += gen_improvement\n",
    "        \n",
    "        # Discriminator improvement (learns to detect better)\n",
    "        disc_improvement = 0.002 * (1 - disc_accuracy) * (1 + gen_quality)  # Harder task as gen improves\n",
    "        disc_accuracy += disc_improvement\n",
    "        \n",
    "        # Add some noise\n",
    "        gen_quality += np.random.normal(0, 0.01)\n",
    "        disc_accuracy += np.random.normal(0, 0.01)\n",
    "        \n",
    "        # Clamp values\n",
    "        gen_quality = np.clip(gen_quality, 0, 1)\n",
    "        disc_accuracy = np.clip(disc_accuracy, 0.5, 1)\n",
    "        \n",
    "        # Compute losses (simplified)\n",
    "        gen_loss = -np.log(gen_quality + 0.1)  # Lower when quality is higher\n",
    "        disc_loss = -(disc_accuracy * np.log(disc_accuracy) + \n",
    "                     (1-disc_accuracy) * np.log(1-disc_accuracy + 0.1))  # Cross-entropy style\n",
    "        \n",
    "        # Store history\n",
    "        history['generator_loss'].append(gen_loss)\n",
    "        history['discriminator_loss'].append(disc_loss)\n",
    "        history['generator_quality'].append(gen_quality)\n",
    "        history['discriminator_accuracy'].append(disc_accuracy)\n",
    "        history['steps'].append(step)\n",
    "    \n",
    "    return history\n",
    "\ndef plot_training_dynamics(history):\n",
    "    \"\"\"\n",
    "    Plot the training dynamics between generator and discriminator\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Generator and Discriminator Losses\n",
    "    axes[0, 0].plot(history['steps'], history['generator_loss'], 'r-', \n",
    "                   linewidth=2, label='Generator Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(history['steps'], history['discriminator_loss'], 'b-', \n",
    "                   linewidth=2, label='Discriminator Loss', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Training Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Generator vs Discriminator Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generator Quality\n",
    "    axes[0, 1].plot(history['steps'], history['generator_quality'], 'g-', \n",
    "                   linewidth=2, alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Training Step')\n",
    "    axes[0, 1].set_ylabel('Quality (0-1)')\n",
    "    axes[0, 1].set_title('Generator Quality Over Time')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Discriminator Accuracy\n",
    "    axes[1, 0].plot(history['steps'], history['discriminator_accuracy'], 'm-', \n",
    "                   linewidth=2, alpha=0.8)\n",
    "    axes[1, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random chance')\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('Discriminator Accuracy Over Time')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training phases\n",
    "    phases = {\n",
    "        'Early (0-200)': 'Generator weak, Discriminator easy task',\n",
    "        'Middle (200-600)': 'Generator improves, Discriminator adapts', \n",
    "        'Late (600-1000)': 'Both models mature, Adversarial balance'\n",
    "    }\n",
    "    \n",
    "    phase_text = \"Training Phases:\\n\\n\"\n",
    "    for phase, description in phases.items():\n",
    "        phase_text += f\"{phase}:\\n{description}\\n\\n\"\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, phase_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='top', \n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    axes[1, 1].set_title('Training Phase Analysis')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n# Run simulation\nprint(\"ELECTRA TRAINING DYNAMICS SIMULATION:\")\nhistory = simulate_training_dynamics()\nplot_training_dynamics(history)\n\n# Analysis\nprint(f\"\\nTRAINING ANALYSIS:\")\nprint(f\"Initial generator quality: {history['generator_quality'][0]:.3f}\")\nprint(f\"Final generator quality: {history['generator_quality'][-1]:.3f}\")\nprint(f\"Initial discriminator accuracy: {history['discriminator_accuracy'][0]:.3f}\")\nprint(f\"Final discriminator accuracy: {history['discriminator_accuracy'][-1]:.3f}\")\n\nprint(f\"\\nKEY INSIGHTS:\")\nprint(f\"1. Generator improves over time → better replacements\")\nprint(f\"2. Discriminator task gets harder → learns better representations\")\nprint(f\"3. Adversarial training → both models push each other\")\nprint(f\"4. Final discriminator is our pre-trained model for downstream tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Empirical Results and Performance Analysis\n",
    "\n",
    "Let's examine ELECTRA's impressive results and understand their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_electra_results():\n",
    "    \"\"\"\n",
    "    Analyze ELECTRA's performance compared to BERT variants\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configurations (parameters in millions)\n",
    "    models = {\n",
    "        'ELECTRA-Small': {'params': 14, 'glue': 79.9, 'training_cost': 1},\n",
    "        'BERT-Base': {'params': 110, 'glue': 79.6, 'training_cost': 4},\n",
    "        'ELECTRA-Base': {'params': 110, 'glue': 85.2, 'training_cost': 4},\n",
    "        'RoBERTa-Base': {'params': 125, 'glue': 84.3, 'training_cost': 16},\n",
    "        'ELECTRA-Large': {'params': 335, 'glue': 88.8, 'training_cost': 25}\n",
    "    }\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    model_names = list(models.keys())\n",
    "    params = [models[m]['params'] for m in model_names]\n",
    "    glue_scores = [models[m]['glue'] for m in model_names]\n",
    "    costs = [models[m]['training_cost'] for m in model_names]\n",
    "    \n",
    "    print(\"ELECTRA PERFORMANCE ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Model':<15} {'Params (M)':<12} {'GLUE Score':<12} {'Training Cost':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name in model_names:\n",
    "        model = models[name]\n",
    "        print(f\"{name:<15} {model['params']:<12} {model['glue']:<12} {model['training_cost']:<15}x\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Performance vs Model Size\n",
    "    colors = ['red' if 'ELECTRA' in name else 'blue' for name in model_names]\n",
    "    scatter = axes[0, 0].scatter(params, glue_scores, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[0, 0].annotate(name, (params[i], glue_scores[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Parameters (Millions)')\n",
    "    axes[0, 0].set_ylabel('GLUE Score')\n",
    "    axes[0, 0].set_title('Performance vs Model Size')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight key comparison\n",
    "    axes[0, 0].plot([14, 110], [79.9, 79.6], 'g--', linewidth=2, alpha=0.7)\n",
    "    axes[0, 0].annotate('ELECTRA-Small matches BERT-Base!', \n",
    "                       xy=(62, 79.75), xytext=(150, 77),\n",
    "                       arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                       fontsize=12, color='green', fontweight='bold')\n",
    "    \n",
    "    # 2. Efficiency Analysis (Performance per Parameter)\n",
    "    efficiency = [score/param for score, param in zip(glue_scores, params)]\n",
    "    bars = axes[0, 1].bar(range(len(model_names)), efficiency, color=colors, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Models')\n",
    "    axes[0, 1].set_ylabel('GLUE Score per Million Parameters')\n",
    "    axes[0, 1].set_title('Parameter Efficiency')\n",
    "    axes[0, 1].set_xticks(range(len(model_names)))\n",
    "    axes[0, 1].set_xticklabels(model_names, rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, eff in zip(bars, efficiency):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{eff:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Training Cost vs Performance\n",
    "    axes[1, 0].scatter(costs, glue_scores, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[1, 0].annotate(name, (costs[i], glue_scores[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Training Cost (Relative)')\n",
    "    axes[1, 0].set_ylabel('GLUE Score')\n",
    "    axes[1, 0].set_title('Performance vs Training Cost')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Key Insights Summary\n",
    "    insights_text = \"\"\"\n",
    "KEY ELECTRA ADVANTAGES:\n",
    "\n",
    "🎯 Efficiency Breakthrough:\n",
    "   • ELECTRA-Small (14M) = BERT-Base (110M)\n",
    "   • 8x fewer parameters, same performance\n",
    "\n",
    "🚀 Training Speed:\n",
    "   • 4x faster training than RoBERTa\n",
    "   • Better sample efficiency\n",
    "\n",
    "📈 Scaling Benefits:\n",
    "   • Small models benefit most\n",
    "   • Excellent for resource constraints\n",
    "\n",
    "🏆 SOTA Results:\n",
    "   • ELECTRA-Large: 88.8 GLUE\n",
    "   • Best published at time of release\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, insights_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    axes[1, 1].set_title('Key Insights')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    electra_small_efficiency = models['ELECTRA-Small']['glue'] / models['ELECTRA-Small']['params']\n",
    "    bert_base_efficiency = models['BERT-Base']['glue'] / models['BERT-Base']['params']\n",
    "    \n",
    "    print(f\"\\nEFFICIENCY COMPARISON:\")\n",
    "    print(f\"ELECTRA-Small efficiency: {electra_small_efficiency:.3f} GLUE/M params\")\n",
    "    print(f\"BERT-Base efficiency: {bert_base_efficiency:.3f} GLUE/M params\")\n",
    "    print(f\"ELECTRA is {electra_small_efficiency/bert_base_efficiency:.1f}x more parameter efficient!\")\n",
    "    \n",
    "    return models\n\nmodels_data = analyze_electra_results()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"REVOLUTIONARY IMPACT OF ELECTRA:\")\nprint(\"\\n1. DEMOCRATIZATION OF NLP:\")\nprint(\"   - Small models can achieve strong performance\")\nprint(\"   - Enables research with limited compute\")\nprint(\"   - Makes NLP accessible to smaller organizations\")\n\nprint(\"\\n2. PARADIGM SHIFT:\")\nprint(\"   - From generative (MLM) to discriminative (RTD) pre-training\")\nprint(\"   - Showed that task design matters more than model size\")\nprint(\"   - Inspired new pre-training objectives\")\n\nprint(\"\\n3. PRACTICAL DEPLOYMENT:\")\nprint(\"   - Mobile applications can run BERT-quality models\")\nprint(\"   - Real-time inference becomes feasible\")\nprint(\"   - Reduced cloud computing costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: ELECTRA's Revolutionary Impact\n",
    "\n",
    "### **Why ELECTRA Ranks #2**\n",
    "\n",
    "1. **Efficiency Revolution**: 4x more sample efficient than BERT\n",
    "2. **Accessibility**: Small models achieve large model performance\n",
    "3. **Paradigm Shift**: From generative to discriminative pre-training\n",
    "4. **Practical Impact**: Enabled deployment in resource-constrained environments\n",
    "\n",
    "### **Core Innovation Comparison**\n",
    "\n",
    "| Aspect | BERT MLM | ELECTRA RTD |\n",
    "|--------|----------|-------------|\n",
    "| **Learning Signal** | 15% of tokens | 100% of tokens |\n",
    "| **Task** | Generate masked tokens | Detect replaced tokens |\n",
    "| **Architecture** | Single model | Generator + Discriminator |\n",
    "| **Sample Efficiency** | 1x | 4x |\n",
    "| **Small Model Performance** | Poor | Excellent |\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "**BERT MLM Loss:**\n",
    "```\n",
    "L_MLM = (1/|M|) Σ_{i∈M} -log P(x_i | x_{\\i})\n",
    "where M = masked positions (15% of tokens)\n",
    "```\n",
    "\n",
    "**ELECTRA RTD Loss:**\n",
    "```\n",
    "L_RTD = (1/n) Σ_{i=1}^n -log P(replaced_i | x)\n",
    "where n = all positions (100% of tokens)\n",
    "```\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "1. **Dense Learning Signal**: Every token contributes to learning\n",
    "2. **Adversarial Training**: Generator-discriminator setup creates challenging examples\n",
    "3. **Contextual Understanding**: Model must understand context to detect replacements\n",
    "4. **Computational Efficiency**: More learning per compute unit\n",
    "\n",
    "### **Legacy and Influence**\n",
    "\n",
    "**Direct Applications:**\n",
    "- Google's production models\n",
    "- Mobile and edge AI applications\n",
    "- Low-resource NLP research\n",
    "\n",
    "**Research Influence:**\n",
    "- **Alternative Pre-training Objectives**: Inspired new tasks beyond MLM\n",
    "- **Efficient Training**: Focus on sample efficiency\n",
    "- **Small Model Research**: Showed small models can be powerful\n",
    "\n",
    "### **Practical Takeaways**\n",
    "\n",
    "**For Researchers:**\n",
    "- ✅ Consider RTD for efficient pre-training\n",
    "- ✅ Use generator-discriminator setups\n",
    "- ✅ Focus on learning signal density\n",
    "- ✅ Don't assume bigger is always better\n",
    "\n",
    "**For Practitioners:**\n",
    "- ✅ ELECTRA-Small for resource-constrained applications\n",
    "- ✅ ELECTRA-Base for best efficiency-performance tradeoff\n",
    "- ✅ Consider ELECTRA for mobile/edge deployment\n",
    "- ✅ Use for scenarios with limited training data\n",
    "\n",
    "**ELECTRA proved that smarter training objectives can dramatically improve efficiency while maintaining quality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **RTD vs MLM Comparison**: Implement both objectives on the same small dataset. Compare convergence speed and final performance.\n",
    "\n",
    "2. **Generator Quality Analysis**: Experiment with different generator sizes. How does generator quality affect discriminator learning?\n",
    "\n",
    "3. **Replacement Strategy**: Try different token replacement strategies (random, similar words, antonyms). Which works best?\n",
    "\n",
    "4. **Efficiency Measurement**: Implement a simple version of both BERT and ELECTRA. Measure actual training time and memory usage.\n",
    "\n",
    "5. **Small Model Optimization**: Design an even smaller ELECTRA model. What's the minimum size that still works effectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n",
    "# Try implementing the exercises above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}