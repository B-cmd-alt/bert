{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention: Fast and Memory-Efficient Exact Attention\n",
    "\n",
    "## ðŸŽ¯ Overview\n",
    "\n",
    "FlashAttention is a groundbreaking algorithm that revolutionized how we compute attention in transformers. It reduces memory complexity from O(NÂ²) to O(N) while maintaining mathematical exactness, enabling training on much longer sequences.\n",
    "\n",
    "**Key Innovation**: IO-aware computation that uses block-wise processing and recomputation to minimize memory access patterns.\n",
    "\n",
    "**Impact**: Universal adoption in major frameworks (PyTorch, JAX), enabling 8x longer sequences with the same memory.\n",
    "\n",
    "## ðŸ“š Background & Motivation\n",
    "\n",
    "### The Memory Wall Problem\n",
    "- Standard attention requires O(NÂ²) memory for the attention matrix\n",
    "- Memory access is often the bottleneck, not computation\n",
    "- GPU memory hierarchy: HBM (slow, large) vs SRAM (fast, small)\n",
    "- Naive attention repeatedly reads/writes to slow HBM memory\n",
    "\n",
    "### The FlashAttention Solution\n",
    "- Block-wise computation that fits in fast SRAM\n",
    "- Online softmax algorithm for numerical stability\n",
    "- Recomputation in backward pass to save memory\n",
    "- Exact attention computation (not an approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully!\")\n",
    "print(f\"ðŸ”¢ NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ’¾ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸš€ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® Mathematical Foundation\n",
    "\n",
    "### Standard Attention\n",
    "The standard attention computation involves:\n",
    "\n",
    "1. **Compute attention scores**: S = QK^T / âˆšd\n",
    "2. **Apply softmax**: P = softmax(S)\n",
    "3. **Apply to values**: O = PV\n",
    "\n",
    "**Memory complexity**: O(NÂ²) for storing S and P\n",
    "\n",
    "### FlashAttention Key Insights\n",
    "\n",
    "1. **Online Softmax**: Compute softmax incrementally without storing full matrix\n",
    "2. **Block-wise Processing**: Process attention in blocks that fit in SRAM\n",
    "3. **Recomputation**: Trade computation for memory in backward pass\n",
    "\n",
    "### Online Softmax Algorithm\n",
    "\n",
    "For computing softmax(x) incrementally:\n",
    "- **m_new = max(m_old, x_new)**\n",
    "- **d_new = d_old Ã— exp(m_old - m_new) + exp(x_new - m_new)**\n",
    "- **Update previous values with correction factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Standard attention implementation - O(NÂ²) memory.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, head_dim = Q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    \n",
    "    # Compute attention scores - O(NÂ²) memory\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Softmax - stores full attention matrix\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply to values\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "def online_softmax(x, dim=-1):\n",
    "    \"\"\"\n",
    "    Demonstrate online softmax computation.\n",
    "    \"\"\"\n",
    "    # Standard softmax for comparison\n",
    "    standard_softmax = F.softmax(x, dim=dim)\n",
    "    \n",
    "    # Online softmax simulation\n",
    "    if dim == -1:\n",
    "        dim = x.dim() - 1\n",
    "    \n",
    "    # Initialize\n",
    "    m = torch.full_like(x[..., :1], -float('inf'))\n",
    "    d = torch.zeros_like(x[..., :1])\n",
    "    output = torch.zeros_like(x)\n",
    "    \n",
    "    # Process each element\n",
    "    for i in range(x.size(dim)):\n",
    "        x_i = x[..., i:i+1]\n",
    "        \n",
    "        # Update max\n",
    "        m_new = torch.maximum(m, x_i)\n",
    "        \n",
    "        # Update denominator\n",
    "        d_new = d * torch.exp(m - m_new) + torch.exp(x_i - m_new)\n",
    "        \n",
    "        # Update previous outputs\n",
    "        if i > 0:\n",
    "            correction = torch.exp(m - m_new)\n",
    "            output[..., :i] = output[..., :i] * correction\n",
    "        \n",
    "        # Compute current output\n",
    "        output[..., i:i+1] = torch.exp(x_i - m_new)\n",
    "        \n",
    "        # Update states\n",
    "        m = m_new\n",
    "        d = d_new\n",
    "    \n",
    "    # Final normalization\n",
    "    output = output / d\n",
    "    \n",
    "    return output, standard_softmax\n",
    "\n",
    "# Test online softmax\n",
    "test_input = torch.randn(2, 8)\n",
    "online_result, standard_result = online_softmax(test_input)\n",
    "\n",
    "print(\"ðŸ§® Online Softmax Test:\")\n",
    "print(f\"   Max difference: {torch.max(torch.abs(online_result - standard_result)).item():.2e}\")\n",
    "print(f\"   Results match: {torch.allclose(online_result, standard_result, atol=1e-6)}\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Input\n",
    "im1 = ax1.imshow(test_input.numpy(), aspect='auto', cmap='coolwarm')\n",
    "ax1.set_title('Input')\n",
    "ax1.set_xlabel('Sequence Position')\n",
    "ax1.set_ylabel('Batch')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Standard softmax\n",
    "im2 = ax2.imshow(standard_result.numpy(), aspect='auto', cmap='viridis')\n",
    "ax2.set_title('Standard Softmax')\n",
    "ax2.set_xlabel('Sequence Position')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# Difference\n",
    "diff = torch.abs(online_result - standard_result).numpy()\n",
    "im3 = ax3.imshow(diff, aspect='auto', cmap='Reds')\n",
    "ax3.set_title('Absolute Difference')\n",
    "ax3.set_xlabel('Sequence Position')\n",
    "plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ FlashAttention Algorithm Implementation\n",
    "\n",
    "Let's implement a simplified version of FlashAttention to understand the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_forward(Q, K, V, block_size=64, mask=None):\n",
    "    \"\"\"\n",
    "    Simplified FlashAttention forward pass.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value tensors [batch, seq_len, head_dim]\n",
    "        block_size: Size of blocks for computation\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        max_memory: Peak memory usage during computation\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, head_dim = Q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    \n",
    "    # Initialize output and statistics\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(batch_size, seq_len, 1, device=Q.device)  # Row sums\n",
    "    m = torch.full((batch_size, seq_len, 1), -float('inf'), device=Q.device)  # Row maxes\n",
    "    \n",
    "    max_memory_used = 0\n",
    "    \n",
    "    # Process in blocks\n",
    "    for j in range(0, seq_len, block_size):\n",
    "        j_end = min(j + block_size, seq_len)\n",
    "        K_j = K[:, j:j_end, :]\n",
    "        V_j = V[:, j:j_end, :]\n",
    "        \n",
    "        for i in range(0, seq_len, block_size):\n",
    "            i_end = min(i + block_size, seq_len)\n",
    "            Q_i = Q[:, i:i_end, :]\n",
    "            \n",
    "            # Compute block attention scores\n",
    "            S_ij = torch.matmul(Q_i, K_j.transpose(-2, -1)) * scale\n",
    "            \n",
    "            # Apply mask if provided\n",
    "            if mask is not None:\n",
    "                mask_block = mask[:, i:i_end, j:j_end]\n",
    "                S_ij = S_ij.masked_fill(mask_block == 0, -1e9)\n",
    "            \n",
    "            # Online softmax update\n",
    "            m_prev = m[:, i:i_end, :].clone()\n",
    "            l_prev = l[:, i:i_end, :].clone()\n",
    "            \n",
    "            # Update row maxes\n",
    "            m_new = torch.maximum(m_prev, S_ij.max(dim=-1, keepdim=True)[0])\n",
    "            \n",
    "            # Compute attention weights for this block\n",
    "            P_ij = torch.exp(S_ij - m_new)\n",
    "            \n",
    "            # Update row sums\n",
    "            l_new = torch.exp(m_prev - m_new) * l_prev + P_ij.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Update output\n",
    "            correction = torch.exp(m_prev - m_new) * l_prev / l_new\n",
    "            O[:, i:i_end, :] = O[:, i:i_end, :] * correction + \\\n",
    "                              torch.matmul(P_ij, V_j) / l_new\n",
    "            \n",
    "            # Update statistics\n",
    "            m[:, i:i_end, :] = m_new\n",
    "            l[:, i:i_end, :] = l_new\n",
    "            \n",
    "            # Track memory usage (simplified)\n",
    "            current_memory = S_ij.numel() + P_ij.numel()\n",
    "            max_memory_used = max(max_memory_used, current_memory)\n",
    "    \n",
    "    return O, max_memory_used\n",
    "\n",
    "def compare_attention_methods(seq_lengths, head_dim=64, batch_size=2, block_size=64):\n",
    "    \"\"\"\n",
    "    Compare naive attention vs FlashAttention in terms of memory and speed.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'seq_lengths': [],\n",
    "        'naive_memory': [],\n",
    "        'flash_memory': [],\n",
    "        'naive_time': [],\n",
    "        'flash_time': [],\n",
    "        'memory_reduction': [],\n",
    "        'max_diff': []\n",
    "    }\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"\\nðŸ”„ Testing sequence length: {seq_len}\")\n",
    "        \n",
    "        # Generate test data\n",
    "        Q = torch.randn(batch_size, seq_len, head_dim)\n",
    "        K = torch.randn(batch_size, seq_len, head_dim)\n",
    "        V = torch.randn(batch_size, seq_len, head_dim)\n",
    "        \n",
    "        # Naive attention\n",
    "        start_time = time.time()\n",
    "        naive_output, naive_weights = naive_attention(Q, K, V)\n",
    "        naive_time = time.time() - start_time\n",
    "        naive_memory = seq_len * seq_len * batch_size  # Attention matrix size\n",
    "        \n",
    "        # FlashAttention\n",
    "        start_time = time.time()\n",
    "        flash_output, flash_memory = flash_attention_forward(Q, K, V, block_size)\n",
    "        flash_time = time.time() - start_time\n",
    "        \n",
    "        # Compare outputs\n",
    "        max_diff = torch.max(torch.abs(naive_output - flash_output)).item()\n",
    "        \n",
    "        # Store results\n",
    "        results['seq_lengths'].append(seq_len)\n",
    "        results['naive_memory'].append(naive_memory)\n",
    "        results['flash_memory'].append(flash_memory)\n",
    "        results['naive_time'].append(naive_time)\n",
    "        results['flash_time'].append(flash_time)\n",
    "        results['memory_reduction'].append(naive_memory / flash_memory)\n",
    "        results['max_diff'].append(max_diff)\n",
    "        \n",
    "        print(f\"   Naive memory: {naive_memory:,} elements\")\n",
    "        print(f\"   Flash memory: {flash_memory:,} elements\")\n",
    "        print(f\"   Memory reduction: {naive_memory / flash_memory:.1f}x\")\n",
    "        print(f\"   Max difference: {max_diff:.2e}\")\n",
    "        print(f\"   Outputs match: {max_diff < 1e-4}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with different sequence lengths\n",
    "test_seq_lengths = [128, 256, 512, 1024]\n",
    "comparison_results = compare_attention_methods(test_seq_lengths)\n",
    "\n",
    "print(\"\\nâœ… FlashAttention comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Memory and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "def plot_attention_comparison(results):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    seq_lengths = results['seq_lengths']\n",
    "    \n",
    "    # 1. Memory usage comparison\n",
    "    ax1.plot(seq_lengths, results['naive_memory'], 'o-', label='Naive Attention', linewidth=2)\n",
    "    ax1.plot(seq_lengths, results['flash_memory'], 's-', label='FlashAttention', linewidth=2)\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Memory Usage (elements)')\n",
    "    ax1.set_title('Memory Usage Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 2. Memory reduction factor\n",
    "    ax2.plot(seq_lengths, results['memory_reduction'], 'o-', color='green', linewidth=2)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Memory Reduction Factor')\n",
    "    ax2.set_title('Memory Reduction (Naive / Flash)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (x, y) in enumerate(zip(seq_lengths, results['memory_reduction'])):\n",
    "        ax2.annotate(f'{y:.1f}x', (x, y), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    # 3. Time comparison\n",
    "    ax3.plot(seq_lengths, results['naive_time'], 'o-', label='Naive Attention', linewidth=2)\n",
    "    ax3.plot(seq_lengths, results['flash_time'], 's-', label='FlashAttention', linewidth=2)\n",
    "    ax3.set_xlabel('Sequence Length')\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.set_title('Computation Time Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Accuracy (max difference)\n",
    "    ax4.semilogy(seq_lengths, results['max_diff'], 'o-', color='red', linewidth=2)\n",
    "    ax4.axhline(y=1e-4, color='orange', linestyle='--', label='Tolerance (1e-4)')\n",
    "    ax4.set_xlabel('Sequence Length')\n",
    "    ax4.set_ylabel('Max Absolute Difference')\n",
    "    ax4.set_title('Numerical Accuracy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_comparison(comparison_results)\n",
    "\n",
    "# Create theoretical analysis\n",
    "def theoretical_memory_analysis():\n",
    "    \"\"\"\n",
    "    Analyze theoretical memory complexity.\n",
    "    \"\"\"\n",
    "    seq_lengths = np.array([128, 256, 512, 1024, 2048, 4096, 8192])\n",
    "    block_size = 64\n",
    "    \n",
    "    # Naive attention: O(NÂ²)\n",
    "    naive_memory = seq_lengths ** 2\n",
    "    \n",
    "    # FlashAttention: O(N) with block size overhead\n",
    "    flash_memory = seq_lengths * block_size\n",
    "    \n",
    "    # Memory reduction factor\n",
    "    reduction_factor = naive_memory / flash_memory\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Memory complexity\n",
    "    ax1.loglog(seq_lengths, naive_memory, 'o-', label='Naive O(NÂ²)', linewidth=2)\n",
    "    ax1.loglog(seq_lengths, flash_memory, 's-', label='Flash O(N)', linewidth=2)\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Memory Usage (relative)')\n",
    "    ax1.set_title('Theoretical Memory Complexity')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reduction factor\n",
    "    ax2.semilogx(seq_lengths, reduction_factor, 'o-', color='green', linewidth=2)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Memory Reduction Factor')\n",
    "    ax2.set_title('Theoretical Memory Reduction')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(np.log(seq_lengths), np.log(reduction_factor), 1)\n",
    "    trend = np.exp(z[1]) * seq_lengths ** z[0]\n",
    "    ax2.plot(seq_lengths, trend, '--', color='red', alpha=0.7, \n",
    "             label=f'Trend: {np.exp(z[1]):.1f} Ã— N^{z[0]:.2f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return seq_lengths, naive_memory, flash_memory, reduction_factor\n",
    "\n",
    "seq_lens, naive_mem, flash_mem, reduction = theoretical_memory_analysis()\n",
    "\n",
    "print(\"\\nðŸ“Š Theoretical Memory Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Seq Len':<10} {'Naive Mem':<12} {'Flash Mem':<12} {'Reduction':<12}\")\n",
    "print(\"=\" * 60)\n",
    "for i, seq_len in enumerate(seq_lens):\n",
    "    if seq_len <= 2048:  # Only show manageable sizes\n",
    "        print(f\"{seq_len:<10} {naive_mem[i]:<12,.0f} {flash_mem[i]:<12,.0f} {reduction[i]:<12.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ FlashAttention-2 Improvements\n",
    "\n",
    "FlashAttention-2 introduced several optimizations:\n",
    "1. **Reduced non-matmul operations** \n",
    "2. **Better parallelization** across sequence length\n",
    "3. **Improved work partitioning** between thread blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttentionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    FlashAttention module with improved memory efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.block_size = block_size\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None, use_flash=True):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        if use_flash and seq_len > self.block_size:\n",
    "            # Use FlashAttention for long sequences\n",
    "            attn_output = self._flash_attention(Q, K, V, attention_mask)\n",
    "        else:\n",
    "            # Use standard attention for short sequences\n",
    "            attn_output = self._standard_attention(Q, K, V, attention_mask)\n",
    "        \n",
    "        # Reshape back\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, embed_dim\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _standard_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Standard attention implementation.\"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output\n",
    "    \n",
    "    def _flash_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"FlashAttention implementation.\"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = Q.shape\n",
    "        \n",
    "        # Flatten batch and heads for processing\n",
    "        Q_flat = Q.view(-1, seq_len, head_dim)\n",
    "        K_flat = K.view(-1, seq_len, head_dim)\n",
    "        V_flat = V.view(-1, seq_len, head_dim)\n",
    "        \n",
    "        # Apply FlashAttention\n",
    "        output_flat, _ = flash_attention_forward(\n",
    "            Q_flat, K_flat, V_flat, self.block_size, mask\n",
    "        )\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output_flat.view(batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_memory_stats(self, seq_len):\n",
    "        \"\"\"Estimate memory usage for different sequence lengths.\"\"\"\n",
    "        # Standard attention memory\n",
    "        standard_memory = self.num_heads * seq_len * seq_len\n",
    "        \n",
    "        # FlashAttention memory (block-wise)\n",
    "        flash_memory = self.num_heads * self.block_size * self.block_size\n",
    "        \n",
    "        return {\n",
    "            'standard': standard_memory,\n",
    "            'flash': flash_memory,\n",
    "            'reduction': standard_memory / flash_memory\n",
    "        }\n",
    "\n",
    "# Test FlashAttention module\n",
    "flash_attn = FlashAttentionModule(\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    block_size=64\n",
    ")\n",
    "\n",
    "# Test with different sequence lengths\n",
    "test_lengths = [128, 256, 512, 1024]\n",
    "\n",
    "print(\"ðŸ§ª FlashAttention Module Test:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for seq_len in test_lengths:\n",
    "    x = torch.randn(2, seq_len, 512)\n",
    "    \n",
    "    # Test both modes\n",
    "    start_time = time.time()\n",
    "    output_standard = flash_attn(x, use_flash=False)\n",
    "    time_standard = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output_flash = flash_attn(x, use_flash=True)\n",
    "    time_flash = time.time() - start_time\n",
    "    \n",
    "    # Compare outputs\n",
    "    max_diff = torch.max(torch.abs(output_standard - output_flash)).item()\n",
    "    \n",
    "    # Memory stats\n",
    "    memory_stats = flash_attn.get_memory_stats(seq_len)\n",
    "    \n",
    "    print(f\"\\nSequence length: {seq_len}\")\n",
    "    print(f\"  Time - Standard: {time_standard:.4f}s, Flash: {time_flash:.4f}s\")\n",
    "    print(f\"  Max difference: {max_diff:.2e}\")\n",
    "    print(f\"  Memory reduction: {memory_stats['reduction']:.1f}x\")\n",
    "    print(f\"  Outputs match: {max_diff < 1e-4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Scaling Analysis: Long Sequences\n",
    "\n",
    "FlashAttention's real power becomes apparent with very long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_long_sequence_scaling():\n",
    "    \"\"\"\n",
    "    Analyze how FlashAttention scales with very long sequences.\n",
    "    \"\"\"\n",
    "    # Sequence lengths to test (some very long)\n",
    "    seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "    block_size = 128\n",
    "    head_dim = 64\n",
    "    \n",
    "    results = {\n",
    "        'seq_lengths': [],\n",
    "        'naive_memory_gb': [],\n",
    "        'flash_memory_gb': [],\n",
    "        'memory_reduction': [],\n",
    "        'naive_feasible': [],\n",
    "        'flash_feasible': []\n",
    "    }\n",
    "    \n",
    "    # Assume we have 24GB GPU memory\n",
    "    gpu_memory_gb = 24\n",
    "    bytes_per_float = 4\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Naive attention memory (just for attention matrix)\n",
    "        naive_elements = seq_len * seq_len\n",
    "        naive_memory_gb = (naive_elements * bytes_per_float) / (1024**3)\n",
    "        \n",
    "        # FlashAttention memory (block-wise)\n",
    "        flash_elements = block_size * block_size\n",
    "        flash_memory_gb = (flash_elements * bytes_per_float) / (1024**3)\n",
    "        \n",
    "        # Check feasibility (simplified - just attention matrix)\n",
    "        naive_feasible = naive_memory_gb < gpu_memory_gb * 0.5  # Leave room for other tensors\n",
    "        flash_feasible = flash_memory_gb < gpu_memory_gb * 0.5\n",
    "        \n",
    "        results['seq_lengths'].append(seq_len)\n",
    "        results['naive_memory_gb'].append(naive_memory_gb)\n",
    "        results['flash_memory_gb'].append(flash_memory_gb)\n",
    "        results['memory_reduction'].append(naive_memory_gb / flash_memory_gb)\n",
    "        results['naive_feasible'].append(naive_feasible)\n",
    "        results['flash_feasible'].append(flash_feasible)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze scaling\n",
    "scaling_results = analyze_long_sequence_scaling()\n",
    "\n",
    "# Visualize scaling results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "seq_lengths = scaling_results['seq_lengths']\n",
    "\n",
    "# 1. Memory usage in GB\n",
    "ax1.semilogy(seq_lengths, scaling_results['naive_memory_gb'], 'o-', \n",
    "             label='Naive Attention', linewidth=2, markersize=8)\n",
    "ax1.semilogy(seq_lengths, scaling_results['flash_memory_gb'], 's-', \n",
    "             label='FlashAttention', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=12, color='red', linestyle='--', alpha=0.7, label='50% of 24GB GPU')\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('Memory Usage (GB)')\n",
    "ax1.set_title('Memory Usage vs Sequence Length')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Memory reduction factor\n",
    "ax2.semilogx(seq_lengths, scaling_results['memory_reduction'], 'o-', \n",
    "             color='green', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Memory Reduction Factor')\n",
    "ax2.set_title('Memory Reduction (Log Scale)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, (x, y) in enumerate(zip(seq_lengths, scaling_results['memory_reduction'])):\n",
    "    if i % 2 == 0:  # Annotate every other point\n",
    "        ax2.annotate(f'{y:.0f}x', (x, y), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# 3. Feasibility analysis\n",
    "feasible_data = []\n",
    "colors = []\n",
    "labels = []\n",
    "\n",
    "for i, seq_len in enumerate(seq_lengths):\n",
    "    if scaling_results['naive_feasible'][i] and scaling_results['flash_feasible'][i]:\n",
    "        feasible_data.append((seq_len, 'Both Feasible', 'green'))\n",
    "    elif scaling_results['flash_feasible'][i]:\n",
    "        feasible_data.append((seq_len, 'Flash Only', 'orange'))\n",
    "    else:\n",
    "        feasible_data.append((seq_len, 'Neither Feasible', 'red'))\n",
    "\n",
    "# Create feasibility plot\n",
    "for i, (seq_len, status, color) in enumerate(feasible_data):\n",
    "    ax3.bar(i, seq_len, color=color, alpha=0.7, label=status if status not in labels else \"\")\n",
    "    if status not in labels:\n",
    "        labels.append(status)\n",
    "\n",
    "ax3.set_xlabel('Sequence Index')\n",
    "ax3.set_ylabel('Sequence Length')\n",
    "ax3.set_title('Feasibility on 24GB GPU')\n",
    "ax3.set_xticks(range(len(seq_lengths)))\n",
    "ax3.set_xticklabels([f'{s//1024}K' for s in seq_lengths])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Maximum achievable sequence length\n",
    "# Theoretical analysis\n",
    "memory_budgets = [8, 16, 24, 40, 80]  # Different GPU memory sizes in GB\n",
    "max_seq_naive = []\n",
    "max_seq_flash = []\n",
    "\n",
    "for budget_gb in memory_budgets:\n",
    "    budget_elements = (budget_gb * 0.5 * 1024**3) / 4  # 50% of memory, 4 bytes per float\n",
    "    \n",
    "    # Naive: NÂ² elements\n",
    "    max_naive = int(np.sqrt(budget_elements))\n",
    "    \n",
    "    # Flash: block_sizeÂ² elements (can handle any sequence length)\n",
    "    max_flash = 100000  # Essentially unlimited for practical purposes\n",
    "    \n",
    "    max_seq_naive.append(max_naive)\n",
    "    max_seq_flash.append(min(max_flash, 100000))  # Cap for visualization\n",
    "\n",
    "ax4.plot(memory_budgets, max_seq_naive, 'o-', label='Naive Attention', linewidth=2)\n",
    "ax4.axhline(y=100000, color='green', linestyle='-', linewidth=2, label='FlashAttention (>100K)')\n",
    "ax4.set_xlabel('GPU Memory (GB)')\n",
    "ax4.set_ylabel('Max Sequence Length')\n",
    "ax4.set_title('Maximum Achievable Sequence Length')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(0, 50000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nðŸ“Š Long Sequence Scaling Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Seq Len':<10} {'Naive GB':<12} {'Flash GB':<12} {'Reduction':<12} {'Feasible':<15}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, seq_len in enumerate(scaling_results['seq_lengths']):\n",
    "    naive_gb = scaling_results['naive_memory_gb'][i]\n",
    "    flash_gb = scaling_results['flash_memory_gb'][i]\n",
    "    reduction = scaling_results['memory_reduction'][i]\n",
    "    \n",
    "    if scaling_results['naive_feasible'][i] and scaling_results['flash_feasible'][i]:\n",
    "        feasible = \"Both\"\n",
    "    elif scaling_results['flash_feasible'][i]:\n",
    "        feasible = \"Flash Only\"\n",
    "    else:\n",
    "        feasible = \"Neither\"\n",
    "    \n",
    "    print(f\"{seq_len:<10} {naive_gb:<12.3f} {flash_gb:<12.6f} {reduction:<12.0f}x {feasible:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practical Exercises\n",
    "\n",
    "### Exercise 1: Implement Block-wise Attention\n",
    "Implement a simplified version of block-wise attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_blockwise_attention():\n",
    "    \"\"\"\n",
    "    Exercise: Implement and test block-wise attention.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª Exercise: Block-wise Attention Implementation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def blockwise_attention_exercise(Q, K, V, block_size):\n",
    "        \"\"\"\n",
    "        YOUR TASK: Complete this block-wise attention implementation.\n",
    "        \n",
    "        Hints:\n",
    "        1. Process Q in blocks of size block_size\n",
    "        2. For each Q block, process all K blocks\n",
    "        3. Use online softmax to maintain numerical stability\n",
    "        4. Accumulate results properly\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, head_dim = Q.shape\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        \n",
    "        # Initialize output\n",
    "        O = torch.zeros_like(Q)\n",
    "        \n",
    "        # TODO: Implement block-wise processing\n",
    "        # For now, we'll provide a working solution\n",
    "        \n",
    "        for i in range(0, seq_len, block_size):\n",
    "            i_end = min(i + block_size, seq_len)\n",
    "            Q_block = Q[:, i:i_end, :]\n",
    "            \n",
    "            # Initialize block statistics\n",
    "            block_max = torch.full((batch_size, i_end - i, 1), -float('inf'), device=Q.device)\n",
    "            block_sum = torch.zeros((batch_size, i_end - i, 1), device=Q.device)\n",
    "            block_output = torch.zeros_like(Q_block)\n",
    "            \n",
    "            for j in range(0, seq_len, block_size):\n",
    "                j_end = min(j + block_size, seq_len)\n",
    "                K_block = K[:, j:j_end, :]\n",
    "                V_block = V[:, j:j_end, :]\n",
    "                \n",
    "                # Compute attention scores for this block pair\n",
    "                scores = torch.matmul(Q_block, K_block.transpose(-2, -1)) * scale\n",
    "                \n",
    "                # Update statistics (simplified online softmax)\n",
    "                block_max_new = torch.maximum(block_max, scores.max(dim=-1, keepdim=True)[0])\n",
    "                \n",
    "                # Compute softmax for this block\n",
    "                exp_scores = torch.exp(scores - block_max_new)\n",
    "                \n",
    "                # Update running sum\n",
    "                correction = torch.exp(block_max - block_max_new)\n",
    "                block_sum = block_sum * correction + exp_scores.sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Update output\n",
    "                block_output = block_output * correction + torch.matmul(exp_scores, V_block)\n",
    "                block_max = block_max_new\n",
    "            \n",
    "            # Normalize the block output\n",
    "            O[:, i:i_end, :] = block_output / block_sum\n",
    "        \n",
    "        return O\n",
    "    \n",
    "    # Test the implementation\n",
    "    seq_len = 256\n",
    "    head_dim = 64\n",
    "    batch_size = 2\n",
    "    block_size = 32\n",
    "    \n",
    "    Q = torch.randn(batch_size, seq_len, head_dim)\n",
    "    K = torch.randn(batch_size, seq_len, head_dim)\n",
    "    V = torch.randn(batch_size, seq_len, head_dim)\n",
    "    \n",
    "    # Compare with standard attention\n",
    "    standard_output, _ = naive_attention(Q, K, V)\n",
    "    blockwise_output = blockwise_attention_exercise(Q, K, V, block_size)\n",
    "    \n",
    "    # Measure accuracy\n",
    "    max_diff = torch.max(torch.abs(standard_output - blockwise_output)).item()\n",
    "    mean_diff = torch.mean(torch.abs(standard_output - blockwise_output)).item()\n",
    "    \n",
    "    print(f\"âœ… Block-wise attention test:\")\n",
    "    print(f\"   Sequence length: {seq_len}\")\n",
    "    print(f\"   Block size: {block_size}\")\n",
    "    print(f\"   Max difference: {max_diff:.2e}\")\n",
    "    print(f\"   Mean difference: {mean_diff:.2e}\")\n",
    "    print(f\"   Results match: {max_diff < 1e-3}\")\n",
    "    \n",
    "    # Test different block sizes\n",
    "    block_sizes = [16, 32, 64, 128]\n",
    "    differences = []\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Block size analysis:\")\n",
    "    for bs in block_sizes:\n",
    "        if bs <= seq_len:\n",
    "            output = blockwise_attention_exercise(Q, K, V, bs)\n",
    "            diff = torch.max(torch.abs(standard_output - output)).item()\n",
    "            differences.append(diff)\n",
    "            print(f\"   Block size {bs}: max diff = {diff:.2e}\")\n",
    "    \n",
    "    return block_sizes[:len(differences)], differences\n",
    "\n",
    "# Run the exercise\n",
    "block_sizes, differences = exercise_blockwise_attention()\n",
    "\n",
    "# Visualize block size effects\n",
    "if len(differences) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(block_sizes, differences, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Block Size')\n",
    "    plt.ylabel('Max Absolute Difference')\n",
    "    plt.title('Numerical Accuracy vs Block Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=1e-6, color='red', linestyle='--', alpha=0.7, label='Target Precision')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Key Takeaways\n",
    "\n",
    "### FlashAttention Advantages:\n",
    "1. **Memory Efficiency**: Reduces memory complexity from O(NÂ²) to O(N)\n",
    "2. **Exact Computation**: Mathematically equivalent to standard attention\n",
    "3. **Speed Improvements**: 2-4x faster on modern GPUs\n",
    "4. **Long Sequences**: Enables training on sequences 8x longer\n",
    "5. **Hardware Awareness**: Optimized for GPU memory hierarchy\n",
    "\n",
    "### Core Innovations:\n",
    "1. **Block-wise Computation**: Process attention in blocks that fit in SRAM\n",
    "2. **Online Softmax**: Compute softmax incrementally without storing full matrix\n",
    "3. **Recomputation**: Trade computation for memory in backward pass\n",
    "4. **IO Optimization**: Minimize slow HBM memory access\n",
    "\n",
    "### When to Use FlashAttention:\n",
    "- **Long Sequences**: When sequence length > 1024\n",
    "- **Memory Constraints**: When standard attention causes OOM\n",
    "- **Large Models**: When every bit of memory efficiency matters\n",
    "- **Production Deployment**: For improved inference speed\n",
    "\n",
    "### Implementation Considerations:\n",
    "1. **Block Size**: Balance between memory and computation efficiency\n",
    "2. **Hardware**: Different optimal block sizes for different GPUs\n",
    "3. **Precision**: FP16 vs FP32 considerations\n",
    "4. **Backward Pass**: Recomputation strategy affects memory vs speed trade-off\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Study FlashAttention-2**: Latest improvements and optimizations\n",
    "2. **Explore Ring Attention**: Distributed attention for extremely long sequences\n",
    "3. **Try PagedAttention**: Attention with paging for serving\n",
    "4. **Implement in Practice**: Use with actual transformer models\n",
    "5. **Hardware Optimization**: Learn about kernel optimization and CUDA programming\n",
    "\n",
    "**FlashAttention has fundamentally changed how we think about attention computation, making long-context models practical and efficient. It's an essential technique for modern transformer architectures!** ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}